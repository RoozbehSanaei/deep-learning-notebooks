{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP64QFN4PzPL/D6PMZdiG5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/deep-learning-notebooks/blob/master/bigan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4uPInV_Ls9O",
        "colab_type": "text"
      },
      "source": [
        "BiGAN includes an encoder E which maps data x to latent representations z in addition to the generator G from the standard GAN framework. The BiGAN discriminator D discriminates not only in data space (x versus G(z)), but jointly in data and latent space (tuples (x, E(x)) versus (G(z), z)), where the latent component is either an encoder output E(x) or a generator input z."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QpwYCQDLlCp",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://raphaellederman.github.io/assets/images/bigan.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmSwXIcBT1Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "!mkdir images\n",
        "!mkdir saved_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys58LdUKUlmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BIGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The part of the bigan that trains the discriminator and encoder\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator(z)\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_])\n",
        "        valid = self.discriminator([z_, img])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bigan_generator = Model([z, img], [fake, valid])\n",
        "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcwO2DirUoS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_encoder(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=self.img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(self.latent_dim))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=self.img_shape)\n",
        "    z = model(img)\n",
        "\n",
        "    return Model(img, z)\n",
        "\n",
        "BIGAN.build_encoder = build_encoder;\n",
        "del build_encoder;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv8ni5ASUyQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(512, input_dim=self.latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "    model.add(Reshape(self.img_shape))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    z = Input(shape=(self.latent_dim,))\n",
        "    gen_img = model(z)\n",
        "\n",
        "    return Model(z, gen_img)\n",
        "\n",
        "BIGAN.build_generator = build_generator;\n",
        "del build_generator;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zAf4USHU0kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_discriminator(self):\n",
        "\n",
        "    z = Input(shape=(self.latent_dim, ))\n",
        "    img = Input(shape=self.img_shape)\n",
        "    d_in = concatenate([z, Flatten()(img)])\n",
        "\n",
        "    model = Dense(1024)(d_in)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    model = Dropout(0.5)(model)\n",
        "    model = Dense(1024)(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    model = Dropout(0.5)(model)\n",
        "    model = Dense(1024)(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    model = Dropout(0.5)(model)\n",
        "    validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    return Model([z, img], validity)\n",
        "\n",
        "BIGAN.build_discriminator = build_discriminator;\n",
        "del build_discriminator;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QUFnUiZU2l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "    # Load the dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Rescale -1 to 1\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        # Sample noise and generate img\n",
        "        z = np.random.normal(size=(batch_size, self.latent_dim))\n",
        "        imgs_ = self.generator.predict(z)\n",
        "\n",
        "        # Select a random batch of images and encode\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "        z_ = self.encoder.predict(imgs)\n",
        "\n",
        "        # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "        d_loss_real = self.discriminator.train_on_batch([z_, imgs], valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (z -> img is valid and img -> z is is invalid)\n",
        "        g_loss = self.bigan_generator.train_on_batch([z, imgs], [valid, fake])\n",
        "\n",
        "        # Plot the progress\n",
        "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % sample_interval == 0:\n",
        "            self.sample_interval(epoch)\n",
        "\n",
        "BIGAN.train = train;\n",
        "del train;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idUeo0O6U3_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sample_interval(self, epoch):\n",
        "    r, c = 5, 5\n",
        "    z = np.random.normal(size=(25, self.latent_dim))\n",
        "    gen_imgs = self.generator.predict(z)\n",
        "\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "\n",
        "BIGAN.sample_interval = sample_interval;\n",
        "del sample_interval;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa324z7tUuxw",
        "colab_type": "code",
        "outputId": "6ed8397c-01e4-41fb-a095-02532b430301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bigan = BIGAN()\n",
        "bigan.train(epochs=40000, batch_size=32, sample_interval=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 512)               51712     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 784)               402192    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 720,656\n",
            "Trainable params: 718,608\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 100)               51300     \n",
            "=================================================================\n",
            "Total params: 719,972\n",
            "Trainable params: 717,924\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 1.118813, acc: 23.44%] [G loss: 4.338573]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.652759, acc: 64.06%] [G loss: 4.464272]\n",
            "2 [D loss: 0.293520, acc: 89.06%] [G loss: 4.533921]\n",
            "3 [D loss: 0.161115, acc: 96.88%] [G loss: 6.363196]\n",
            "4 [D loss: 0.176801, acc: 93.75%] [G loss: 8.119849]\n",
            "5 [D loss: 0.093477, acc: 100.00%] [G loss: 7.990685]\n",
            "6 [D loss: 0.034517, acc: 100.00%] [G loss: 8.396814]\n",
            "7 [D loss: 0.063887, acc: 100.00%] [G loss: 9.218338]\n",
            "8 [D loss: 0.027905, acc: 100.00%] [G loss: 9.929588]\n",
            "9 [D loss: 0.024044, acc: 100.00%] [G loss: 10.597345]\n",
            "10 [D loss: 0.029141, acc: 100.00%] [G loss: 10.974556]\n",
            "11 [D loss: 0.015313, acc: 100.00%] [G loss: 10.940910]\n",
            "12 [D loss: 0.012610, acc: 100.00%] [G loss: 11.378998]\n",
            "13 [D loss: 0.031253, acc: 100.00%] [G loss: 11.521930]\n",
            "14 [D loss: 0.012511, acc: 100.00%] [G loss: 13.259706]\n",
            "15 [D loss: 0.010120, acc: 100.00%] [G loss: 13.599478]\n",
            "16 [D loss: 0.009109, acc: 100.00%] [G loss: 12.756534]\n",
            "17 [D loss: 0.009493, acc: 100.00%] [G loss: 12.143055]\n",
            "18 [D loss: 0.004750, acc: 100.00%] [G loss: 12.646590]\n",
            "19 [D loss: 0.010797, acc: 100.00%] [G loss: 13.208315]\n",
            "20 [D loss: 0.011077, acc: 100.00%] [G loss: 12.919256]\n",
            "21 [D loss: 0.016221, acc: 100.00%] [G loss: 14.732557]\n",
            "22 [D loss: 0.012281, acc: 100.00%] [G loss: 15.048037]\n",
            "23 [D loss: 0.006949, acc: 100.00%] [G loss: 14.911057]\n",
            "24 [D loss: 0.013633, acc: 100.00%] [G loss: 15.687848]\n",
            "25 [D loss: 0.012638, acc: 100.00%] [G loss: 15.925814]\n",
            "26 [D loss: 0.004721, acc: 100.00%] [G loss: 15.762573]\n",
            "27 [D loss: 0.006133, acc: 100.00%] [G loss: 15.741231]\n",
            "28 [D loss: 0.006879, acc: 100.00%] [G loss: 15.729135]\n",
            "29 [D loss: 0.026449, acc: 98.44%] [G loss: 18.112305]\n",
            "30 [D loss: 0.009844, acc: 100.00%] [G loss: 18.824459]\n",
            "31 [D loss: 0.008468, acc: 100.00%] [G loss: 19.518696]\n",
            "32 [D loss: 0.006977, acc: 100.00%] [G loss: 18.253160]\n",
            "33 [D loss: 0.004794, acc: 100.00%] [G loss: 17.982372]\n",
            "34 [D loss: 0.003657, acc: 100.00%] [G loss: 16.538628]\n",
            "35 [D loss: 0.007353, acc: 100.00%] [G loss: 18.828665]\n",
            "36 [D loss: 0.004400, acc: 100.00%] [G loss: 19.182842]\n",
            "37 [D loss: 0.008483, acc: 100.00%] [G loss: 18.655184]\n",
            "38 [D loss: 0.003885, acc: 100.00%] [G loss: 17.991644]\n",
            "39 [D loss: 0.005668, acc: 100.00%] [G loss: 18.082596]\n",
            "40 [D loss: 0.004766, acc: 100.00%] [G loss: 18.503693]\n",
            "41 [D loss: 0.084790, acc: 96.88%] [G loss: 19.870043]\n",
            "42 [D loss: 0.020185, acc: 100.00%] [G loss: 20.433697]\n",
            "43 [D loss: 0.018636, acc: 100.00%] [G loss: 20.240095]\n",
            "44 [D loss: 0.018536, acc: 100.00%] [G loss: 20.765711]\n",
            "45 [D loss: 0.006273, acc: 100.00%] [G loss: 20.886717]\n",
            "46 [D loss: 0.011360, acc: 100.00%] [G loss: 20.621880]\n",
            "47 [D loss: 0.005436, acc: 100.00%] [G loss: 19.718559]\n",
            "48 [D loss: 0.008434, acc: 100.00%] [G loss: 20.948488]\n",
            "49 [D loss: 0.506030, acc: 89.06%] [G loss: 19.478752]\n",
            "50 [D loss: 0.201250, acc: 92.19%] [G loss: 20.679291]\n",
            "51 [D loss: 0.027461, acc: 100.00%] [G loss: 21.389362]\n",
            "52 [D loss: 0.029496, acc: 98.44%] [G loss: 21.307203]\n",
            "53 [D loss: 0.469924, acc: 92.19%] [G loss: 20.639381]\n",
            "54 [D loss: 0.074234, acc: 98.44%] [G loss: 20.902172]\n",
            "55 [D loss: 0.076810, acc: 95.31%] [G loss: 23.009424]\n",
            "56 [D loss: 0.026942, acc: 98.44%] [G loss: 23.808540]\n",
            "57 [D loss: 0.037993, acc: 98.44%] [G loss: 24.152977]\n",
            "58 [D loss: 0.015635, acc: 100.00%] [G loss: 24.963364]\n",
            "59 [D loss: 0.007642, acc: 100.00%] [G loss: 24.783140]\n",
            "60 [D loss: 0.006756, acc: 100.00%] [G loss: 25.290342]\n",
            "61 [D loss: 0.023888, acc: 98.44%] [G loss: 24.779839]\n",
            "62 [D loss: 0.046594, acc: 98.44%] [G loss: 25.236385]\n",
            "63 [D loss: 0.179371, acc: 93.75%] [G loss: 25.143158]\n",
            "64 [D loss: 0.484699, acc: 92.19%] [G loss: 24.909899]\n",
            "65 [D loss: 0.397728, acc: 89.06%] [G loss: 24.258993]\n",
            "66 [D loss: 0.041941, acc: 98.44%] [G loss: 23.008011]\n",
            "67 [D loss: 0.579891, acc: 85.94%] [G loss: 24.137199]\n",
            "68 [D loss: 0.229997, acc: 89.06%] [G loss: 22.085320]\n",
            "69 [D loss: 0.351063, acc: 92.19%] [G loss: 22.049829]\n",
            "70 [D loss: 0.622449, acc: 87.50%] [G loss: 25.098328]\n",
            "71 [D loss: 0.381859, acc: 87.50%] [G loss: 24.216867]\n",
            "72 [D loss: 0.329475, acc: 92.19%] [G loss: 22.641560]\n",
            "73 [D loss: 0.638737, acc: 85.94%] [G loss: 25.018513]\n",
            "74 [D loss: 0.152756, acc: 95.31%] [G loss: 21.164612]\n",
            "75 [D loss: 0.700857, acc: 84.38%] [G loss: 24.883110]\n",
            "76 [D loss: 0.342982, acc: 89.06%] [G loss: 22.394936]\n",
            "77 [D loss: 1.769352, acc: 73.44%] [G loss: 21.192797]\n",
            "78 [D loss: 0.408026, acc: 85.94%] [G loss: 16.974083]\n",
            "79 [D loss: 3.004861, acc: 60.94%] [G loss: 24.311790]\n",
            "80 [D loss: 1.677400, acc: 79.69%] [G loss: 22.498095]\n",
            "81 [D loss: 0.868321, acc: 76.56%] [G loss: 22.569534]\n",
            "82 [D loss: 0.149744, acc: 93.75%] [G loss: 16.774315]\n",
            "83 [D loss: 0.687128, acc: 89.06%] [G loss: 21.751080]\n",
            "84 [D loss: 0.218634, acc: 90.62%] [G loss: 15.258055]\n",
            "85 [D loss: 0.368871, acc: 89.06%] [G loss: 17.123531]\n",
            "86 [D loss: 0.628801, acc: 85.94%] [G loss: 23.257341]\n",
            "87 [D loss: 0.576320, acc: 89.06%] [G loss: 20.880442]\n",
            "88 [D loss: 0.222594, acc: 90.62%] [G loss: 13.984256]\n",
            "89 [D loss: 0.521729, acc: 81.25%] [G loss: 16.812355]\n",
            "90 [D loss: 0.482054, acc: 81.25%] [G loss: 13.548332]\n",
            "91 [D loss: 1.413464, acc: 64.06%] [G loss: 22.792305]\n",
            "92 [D loss: 0.876944, acc: 79.69%] [G loss: 19.816610]\n",
            "93 [D loss: 0.504650, acc: 81.25%] [G loss: 12.567539]\n",
            "94 [D loss: 1.308895, acc: 71.88%] [G loss: 17.954384]\n",
            "95 [D loss: 0.268319, acc: 87.50%] [G loss: 16.649727]\n",
            "96 [D loss: 0.541466, acc: 82.81%] [G loss: 12.242369]\n",
            "97 [D loss: 0.951764, acc: 68.75%] [G loss: 14.628638]\n",
            "98 [D loss: 0.362248, acc: 87.50%] [G loss: 11.064772]\n",
            "99 [D loss: 1.155035, acc: 62.50%] [G loss: 18.761080]\n",
            "100 [D loss: 0.420778, acc: 84.38%] [G loss: 19.478550]\n",
            "101 [D loss: 0.402718, acc: 85.94%] [G loss: 12.784821]\n",
            "102 [D loss: 0.227944, acc: 90.62%] [G loss: 12.273130]\n",
            "103 [D loss: 0.330325, acc: 84.38%] [G loss: 12.165581]\n",
            "104 [D loss: 0.572966, acc: 79.69%] [G loss: 14.212723]\n",
            "105 [D loss: 0.377984, acc: 81.25%] [G loss: 11.582839]\n",
            "106 [D loss: 0.757778, acc: 70.31%] [G loss: 13.383757]\n",
            "107 [D loss: 0.438817, acc: 79.69%] [G loss: 11.010561]\n",
            "108 [D loss: 0.480675, acc: 76.56%] [G loss: 11.440987]\n",
            "109 [D loss: 0.535434, acc: 76.56%] [G loss: 10.789936]\n",
            "110 [D loss: 0.598742, acc: 76.56%] [G loss: 11.137157]\n",
            "111 [D loss: 0.347030, acc: 84.38%] [G loss: 10.133868]\n",
            "112 [D loss: 0.505058, acc: 73.44%] [G loss: 10.594900]\n",
            "113 [D loss: 0.462016, acc: 75.00%] [G loss: 9.945217]\n",
            "114 [D loss: 0.345488, acc: 82.81%] [G loss: 10.314996]\n",
            "115 [D loss: 0.366095, acc: 81.25%] [G loss: 11.612084]\n",
            "116 [D loss: 0.301427, acc: 85.94%] [G loss: 11.264914]\n",
            "117 [D loss: 0.375220, acc: 81.25%] [G loss: 10.794315]\n",
            "118 [D loss: 0.278990, acc: 87.50%] [G loss: 10.737993]\n",
            "119 [D loss: 0.480063, acc: 78.12%] [G loss: 12.197504]\n",
            "120 [D loss: 0.224355, acc: 90.62%] [G loss: 9.452797]\n",
            "121 [D loss: 0.286831, acc: 82.81%] [G loss: 10.569208]\n",
            "122 [D loss: 0.462677, acc: 79.69%] [G loss: 10.769809]\n",
            "123 [D loss: 0.245965, acc: 90.62%] [G loss: 9.173981]\n",
            "124 [D loss: 0.389356, acc: 82.81%] [G loss: 12.136728]\n",
            "125 [D loss: 0.318791, acc: 84.38%] [G loss: 9.882555]\n",
            "126 [D loss: 0.345711, acc: 84.38%] [G loss: 10.066269]\n",
            "127 [D loss: 0.540491, acc: 78.12%] [G loss: 10.637217]\n",
            "128 [D loss: 0.333438, acc: 84.38%] [G loss: 10.210449]\n",
            "129 [D loss: 0.347172, acc: 76.56%] [G loss: 8.842665]\n",
            "130 [D loss: 0.482101, acc: 68.75%] [G loss: 11.543158]\n",
            "131 [D loss: 0.214888, acc: 85.94%] [G loss: 10.874592]\n",
            "132 [D loss: 0.332538, acc: 81.25%] [G loss: 9.849400]\n",
            "133 [D loss: 0.387180, acc: 81.25%] [G loss: 9.393291]\n",
            "134 [D loss: 0.596888, acc: 73.44%] [G loss: 9.824812]\n",
            "135 [D loss: 0.431778, acc: 79.69%] [G loss: 10.043530]\n",
            "136 [D loss: 0.282252, acc: 85.94%] [G loss: 8.019013]\n",
            "137 [D loss: 0.627295, acc: 68.75%] [G loss: 11.546556]\n",
            "138 [D loss: 0.376813, acc: 78.12%] [G loss: 9.520988]\n",
            "139 [D loss: 0.404709, acc: 81.25%] [G loss: 10.640939]\n",
            "140 [D loss: 0.382465, acc: 81.25%] [G loss: 9.520445]\n",
            "141 [D loss: 0.354807, acc: 79.69%] [G loss: 8.850922]\n",
            "142 [D loss: 0.394167, acc: 76.56%] [G loss: 9.291948]\n",
            "143 [D loss: 0.329629, acc: 81.25%] [G loss: 8.265667]\n",
            "144 [D loss: 0.244259, acc: 89.06%] [G loss: 9.584032]\n",
            "145 [D loss: 0.289212, acc: 84.38%] [G loss: 9.556850]\n",
            "146 [D loss: 0.372396, acc: 79.69%] [G loss: 9.405660]\n",
            "147 [D loss: 0.444554, acc: 76.56%] [G loss: 9.929350]\n",
            "148 [D loss: 0.252276, acc: 89.06%] [G loss: 8.795613]\n",
            "149 [D loss: 0.410723, acc: 78.12%] [G loss: 9.337111]\n",
            "150 [D loss: 0.288134, acc: 90.62%] [G loss: 9.489677]\n",
            "151 [D loss: 0.332350, acc: 84.38%] [G loss: 11.159731]\n",
            "152 [D loss: 0.424462, acc: 82.81%] [G loss: 10.989597]\n",
            "153 [D loss: 0.361255, acc: 78.12%] [G loss: 8.792541]\n",
            "154 [D loss: 0.330060, acc: 87.50%] [G loss: 10.372508]\n",
            "155 [D loss: 0.365591, acc: 79.69%] [G loss: 9.156490]\n",
            "156 [D loss: 0.292493, acc: 85.94%] [G loss: 9.249645]\n",
            "157 [D loss: 0.419243, acc: 82.81%] [G loss: 9.960031]\n",
            "158 [D loss: 0.378646, acc: 79.69%] [G loss: 8.995464]\n",
            "159 [D loss: 0.298452, acc: 82.81%] [G loss: 9.403162]\n",
            "160 [D loss: 0.282937, acc: 85.94%] [G loss: 10.570869]\n",
            "161 [D loss: 0.365864, acc: 84.38%] [G loss: 8.541731]\n",
            "162 [D loss: 0.269348, acc: 89.06%] [G loss: 10.387082]\n",
            "163 [D loss: 0.254876, acc: 84.38%] [G loss: 10.067341]\n",
            "164 [D loss: 0.383072, acc: 84.38%] [G loss: 10.120396]\n",
            "165 [D loss: 0.454049, acc: 73.44%] [G loss: 7.370737]\n",
            "166 [D loss: 0.294751, acc: 82.81%] [G loss: 8.445610]\n",
            "167 [D loss: 0.246279, acc: 85.94%] [G loss: 9.998150]\n",
            "168 [D loss: 0.294851, acc: 85.94%] [G loss: 10.705976]\n",
            "169 [D loss: 0.349543, acc: 81.25%] [G loss: 9.858158]\n",
            "170 [D loss: 0.276766, acc: 85.94%] [G loss: 7.778138]\n",
            "171 [D loss: 0.515382, acc: 75.00%] [G loss: 10.868689]\n",
            "172 [D loss: 0.389585, acc: 82.81%] [G loss: 9.175357]\n",
            "173 [D loss: 0.446654, acc: 76.56%] [G loss: 10.026268]\n",
            "174 [D loss: 0.377643, acc: 76.56%] [G loss: 9.493450]\n",
            "175 [D loss: 0.412070, acc: 75.00%] [G loss: 8.574185]\n",
            "176 [D loss: 0.322014, acc: 85.94%] [G loss: 9.330141]\n",
            "177 [D loss: 0.613919, acc: 71.88%] [G loss: 9.723928]\n",
            "178 [D loss: 0.320898, acc: 84.38%] [G loss: 8.364471]\n",
            "179 [D loss: 0.378377, acc: 82.81%] [G loss: 9.239725]\n",
            "180 [D loss: 0.333088, acc: 82.81%] [G loss: 9.047302]\n",
            "181 [D loss: 0.519784, acc: 71.88%] [G loss: 10.033693]\n",
            "182 [D loss: 0.281452, acc: 84.38%] [G loss: 9.284969]\n",
            "183 [D loss: 0.425597, acc: 78.12%] [G loss: 9.620996]\n",
            "184 [D loss: 0.427721, acc: 79.69%] [G loss: 8.892779]\n",
            "185 [D loss: 0.258376, acc: 87.50%] [G loss: 8.356037]\n",
            "186 [D loss: 0.297872, acc: 84.38%] [G loss: 9.014736]\n",
            "187 [D loss: 0.305410, acc: 84.38%] [G loss: 9.718548]\n",
            "188 [D loss: 0.338739, acc: 81.25%] [G loss: 9.027115]\n",
            "189 [D loss: 0.227399, acc: 82.81%] [G loss: 8.057239]\n",
            "190 [D loss: 0.388909, acc: 79.69%] [G loss: 8.697662]\n",
            "191 [D loss: 0.274199, acc: 87.50%] [G loss: 8.394571]\n",
            "192 [D loss: 0.275266, acc: 84.38%] [G loss: 8.680803]\n",
            "193 [D loss: 0.356711, acc: 82.81%] [G loss: 9.555069]\n",
            "194 [D loss: 0.275666, acc: 85.94%] [G loss: 9.321311]\n",
            "195 [D loss: 0.268306, acc: 84.38%] [G loss: 9.733879]\n",
            "196 [D loss: 0.380755, acc: 79.69%] [G loss: 8.443364]\n",
            "197 [D loss: 0.237397, acc: 84.38%] [G loss: 8.439068]\n",
            "198 [D loss: 0.416606, acc: 79.69%] [G loss: 9.099953]\n",
            "199 [D loss: 0.361564, acc: 82.81%] [G loss: 8.538651]\n",
            "200 [D loss: 0.369540, acc: 78.12%] [G loss: 9.248844]\n",
            "201 [D loss: 0.317263, acc: 89.06%] [G loss: 8.965446]\n",
            "202 [D loss: 0.356279, acc: 79.69%] [G loss: 8.549870]\n",
            "203 [D loss: 0.313750, acc: 84.38%] [G loss: 8.630096]\n",
            "204 [D loss: 0.323162, acc: 85.94%] [G loss: 8.234101]\n",
            "205 [D loss: 0.349113, acc: 82.81%] [G loss: 8.241938]\n",
            "206 [D loss: 0.251745, acc: 85.94%] [G loss: 9.367046]\n",
            "207 [D loss: 0.425562, acc: 79.69%] [G loss: 8.346751]\n",
            "208 [D loss: 0.341418, acc: 79.69%] [G loss: 8.298111]\n",
            "209 [D loss: 0.374399, acc: 79.69%] [G loss: 8.723125]\n",
            "210 [D loss: 0.304549, acc: 82.81%] [G loss: 10.451516]\n",
            "211 [D loss: 0.336123, acc: 81.25%] [G loss: 8.722017]\n",
            "212 [D loss: 0.434870, acc: 81.25%] [G loss: 8.876909]\n",
            "213 [D loss: 0.427889, acc: 81.25%] [G loss: 7.531959]\n",
            "214 [D loss: 0.427710, acc: 78.12%] [G loss: 8.698638]\n",
            "215 [D loss: 0.386533, acc: 79.69%] [G loss: 8.240977]\n",
            "216 [D loss: 0.369419, acc: 78.12%] [G loss: 8.046680]\n",
            "217 [D loss: 0.281421, acc: 85.94%] [G loss: 8.649939]\n",
            "218 [D loss: 0.268650, acc: 92.19%] [G loss: 9.169872]\n",
            "219 [D loss: 0.262988, acc: 87.50%] [G loss: 7.724750]\n",
            "220 [D loss: 0.267577, acc: 84.38%] [G loss: 8.382457]\n",
            "221 [D loss: 0.295766, acc: 85.94%] [G loss: 7.723231]\n",
            "222 [D loss: 0.346801, acc: 85.94%] [G loss: 8.340712]\n",
            "223 [D loss: 0.276780, acc: 90.62%] [G loss: 8.133350]\n",
            "224 [D loss: 0.268277, acc: 87.50%] [G loss: 8.557168]\n",
            "225 [D loss: 0.303141, acc: 85.94%] [G loss: 8.892941]\n",
            "226 [D loss: 0.276754, acc: 82.81%] [G loss: 9.914581]\n",
            "227 [D loss: 0.160583, acc: 93.75%] [G loss: 7.673234]\n",
            "228 [D loss: 0.305008, acc: 82.81%] [G loss: 7.745446]\n",
            "229 [D loss: 0.388132, acc: 79.69%] [G loss: 7.712467]\n",
            "230 [D loss: 0.213910, acc: 90.62%] [G loss: 7.519907]\n",
            "231 [D loss: 0.353933, acc: 78.12%] [G loss: 8.595557]\n",
            "232 [D loss: 0.239668, acc: 85.94%] [G loss: 7.505243]\n",
            "233 [D loss: 0.374737, acc: 81.25%] [G loss: 8.000082]\n",
            "234 [D loss: 0.307368, acc: 81.25%] [G loss: 7.124799]\n",
            "235 [D loss: 0.300582, acc: 90.62%] [G loss: 8.665012]\n",
            "236 [D loss: 0.313204, acc: 85.94%] [G loss: 7.646703]\n",
            "237 [D loss: 0.287425, acc: 87.50%] [G loss: 7.174389]\n",
            "238 [D loss: 0.323554, acc: 81.25%] [G loss: 7.679813]\n",
            "239 [D loss: 0.332622, acc: 82.81%] [G loss: 7.698696]\n",
            "240 [D loss: 0.209982, acc: 89.06%] [G loss: 8.211290]\n",
            "241 [D loss: 0.266470, acc: 92.19%] [G loss: 7.554852]\n",
            "242 [D loss: 0.334979, acc: 81.25%] [G loss: 8.293705]\n",
            "243 [D loss: 0.240127, acc: 84.38%] [G loss: 8.047827]\n",
            "244 [D loss: 0.431297, acc: 82.81%] [G loss: 7.138032]\n",
            "245 [D loss: 0.446342, acc: 78.12%] [G loss: 7.172314]\n",
            "246 [D loss: 0.278807, acc: 89.06%] [G loss: 6.696188]\n",
            "247 [D loss: 0.257046, acc: 87.50%] [G loss: 8.078403]\n",
            "248 [D loss: 0.267860, acc: 85.94%] [G loss: 7.279321]\n",
            "249 [D loss: 0.213809, acc: 92.19%] [G loss: 6.271722]\n",
            "250 [D loss: 0.224979, acc: 90.62%] [G loss: 7.342162]\n",
            "251 [D loss: 0.278623, acc: 87.50%] [G loss: 7.518515]\n",
            "252 [D loss: 0.293702, acc: 82.81%] [G loss: 7.589567]\n",
            "253 [D loss: 0.307981, acc: 84.38%] [G loss: 7.473963]\n",
            "254 [D loss: 0.202982, acc: 89.06%] [G loss: 6.878032]\n",
            "255 [D loss: 0.278450, acc: 85.94%] [G loss: 7.631253]\n",
            "256 [D loss: 0.333084, acc: 84.38%] [G loss: 7.427917]\n",
            "257 [D loss: 0.333102, acc: 85.94%] [G loss: 7.911726]\n",
            "258 [D loss: 0.161691, acc: 92.19%] [G loss: 7.145357]\n",
            "259 [D loss: 0.339106, acc: 82.81%] [G loss: 7.692036]\n",
            "260 [D loss: 0.203240, acc: 92.19%] [G loss: 6.947336]\n",
            "261 [D loss: 0.205521, acc: 92.19%] [G loss: 6.263916]\n",
            "262 [D loss: 0.239669, acc: 90.62%] [G loss: 7.393493]\n",
            "263 [D loss: 0.217639, acc: 84.38%] [G loss: 6.896353]\n",
            "264 [D loss: 0.198156, acc: 93.75%] [G loss: 7.588636]\n",
            "265 [D loss: 0.230505, acc: 85.94%] [G loss: 7.341558]\n",
            "266 [D loss: 0.359148, acc: 85.94%] [G loss: 7.332105]\n",
            "267 [D loss: 0.227351, acc: 92.19%] [G loss: 6.953827]\n",
            "268 [D loss: 0.253925, acc: 89.06%] [G loss: 6.346851]\n",
            "269 [D loss: 0.280116, acc: 85.94%] [G loss: 5.818558]\n",
            "270 [D loss: 0.424462, acc: 89.06%] [G loss: 7.014539]\n",
            "271 [D loss: 0.229476, acc: 92.19%] [G loss: 6.505335]\n",
            "272 [D loss: 0.328765, acc: 84.38%] [G loss: 6.820580]\n",
            "273 [D loss: 0.455096, acc: 81.25%] [G loss: 7.525332]\n",
            "274 [D loss: 0.259685, acc: 85.94%] [G loss: 5.873770]\n",
            "275 [D loss: 0.275850, acc: 84.38%] [G loss: 7.339520]\n",
            "276 [D loss: 0.187266, acc: 92.19%] [G loss: 6.930248]\n",
            "277 [D loss: 0.240142, acc: 92.19%] [G loss: 7.213995]\n",
            "278 [D loss: 0.297760, acc: 85.94%] [G loss: 6.681536]\n",
            "279 [D loss: 0.216601, acc: 92.19%] [G loss: 6.895239]\n",
            "280 [D loss: 0.363496, acc: 79.69%] [G loss: 6.929220]\n",
            "281 [D loss: 0.243149, acc: 85.94%] [G loss: 6.665575]\n",
            "282 [D loss: 0.344168, acc: 78.12%] [G loss: 6.195914]\n",
            "283 [D loss: 0.250428, acc: 89.06%] [G loss: 5.952223]\n",
            "284 [D loss: 0.198754, acc: 95.31%] [G loss: 6.893322]\n",
            "285 [D loss: 0.273065, acc: 85.94%] [G loss: 5.883379]\n",
            "286 [D loss: 0.215570, acc: 84.38%] [G loss: 6.551960]\n",
            "287 [D loss: 0.235529, acc: 89.06%] [G loss: 5.479798]\n",
            "288 [D loss: 0.532375, acc: 78.12%] [G loss: 6.278386]\n",
            "289 [D loss: 0.266835, acc: 87.50%] [G loss: 5.673375]\n",
            "290 [D loss: 0.399933, acc: 79.69%] [G loss: 6.146869]\n",
            "291 [D loss: 0.323317, acc: 85.94%] [G loss: 6.953942]\n",
            "292 [D loss: 0.315100, acc: 84.38%] [G loss: 6.666298]\n",
            "293 [D loss: 0.140520, acc: 95.31%] [G loss: 6.135688]\n",
            "294 [D loss: 0.407033, acc: 78.12%] [G loss: 5.756971]\n",
            "295 [D loss: 0.255505, acc: 89.06%] [G loss: 5.661622]\n",
            "296 [D loss: 0.442781, acc: 76.56%] [G loss: 6.828818]\n",
            "297 [D loss: 0.187275, acc: 95.31%] [G loss: 6.105947]\n",
            "298 [D loss: 0.313397, acc: 82.81%] [G loss: 5.400651]\n",
            "299 [D loss: 0.393274, acc: 82.81%] [G loss: 6.000241]\n",
            "300 [D loss: 0.324540, acc: 79.69%] [G loss: 5.692622]\n",
            "301 [D loss: 0.209199, acc: 93.75%] [G loss: 6.153008]\n",
            "302 [D loss: 0.318322, acc: 84.38%] [G loss: 5.480012]\n",
            "303 [D loss: 0.194448, acc: 93.75%] [G loss: 4.818107]\n",
            "304 [D loss: 0.352704, acc: 85.94%] [G loss: 6.205761]\n",
            "305 [D loss: 0.238753, acc: 87.50%] [G loss: 5.722884]\n",
            "306 [D loss: 0.262829, acc: 87.50%] [G loss: 5.408126]\n",
            "307 [D loss: 0.345991, acc: 85.94%] [G loss: 5.683214]\n",
            "308 [D loss: 0.295346, acc: 89.06%] [G loss: 5.620942]\n",
            "309 [D loss: 0.206858, acc: 87.50%] [G loss: 5.161712]\n",
            "310 [D loss: 0.301135, acc: 84.38%] [G loss: 5.470902]\n",
            "311 [D loss: 0.340496, acc: 92.19%] [G loss: 6.762636]\n",
            "312 [D loss: 0.442692, acc: 78.12%] [G loss: 5.687143]\n",
            "313 [D loss: 0.396288, acc: 79.69%] [G loss: 5.357061]\n",
            "314 [D loss: 0.397192, acc: 81.25%] [G loss: 4.845835]\n",
            "315 [D loss: 0.377690, acc: 81.25%] [G loss: 5.218614]\n",
            "316 [D loss: 0.356968, acc: 79.69%] [G loss: 4.784879]\n",
            "317 [D loss: 0.439312, acc: 76.56%] [G loss: 5.544170]\n",
            "318 [D loss: 0.292514, acc: 87.50%] [G loss: 4.615751]\n",
            "319 [D loss: 0.491955, acc: 70.31%] [G loss: 5.822654]\n",
            "320 [D loss: 0.266278, acc: 89.06%] [G loss: 5.146714]\n",
            "321 [D loss: 0.251378, acc: 90.62%] [G loss: 4.485918]\n",
            "322 [D loss: 0.389196, acc: 78.12%] [G loss: 4.794029]\n",
            "323 [D loss: 0.312071, acc: 85.94%] [G loss: 5.648757]\n",
            "324 [D loss: 0.384379, acc: 78.12%] [G loss: 4.705059]\n",
            "325 [D loss: 0.330821, acc: 87.50%] [G loss: 4.563227]\n",
            "326 [D loss: 0.445630, acc: 78.12%] [G loss: 5.208682]\n",
            "327 [D loss: 0.266810, acc: 87.50%] [G loss: 4.340696]\n",
            "328 [D loss: 0.206863, acc: 90.62%] [G loss: 4.932090]\n",
            "329 [D loss: 0.430248, acc: 73.44%] [G loss: 4.794112]\n",
            "330 [D loss: 0.392606, acc: 82.81%] [G loss: 4.275385]\n",
            "331 [D loss: 0.313661, acc: 87.50%] [G loss: 5.129470]\n",
            "332 [D loss: 0.610974, acc: 62.50%] [G loss: 4.840074]\n",
            "333 [D loss: 0.358531, acc: 81.25%] [G loss: 4.917631]\n",
            "334 [D loss: 0.349348, acc: 84.38%] [G loss: 4.862086]\n",
            "335 [D loss: 0.424443, acc: 81.25%] [G loss: 4.424146]\n",
            "336 [D loss: 0.221392, acc: 95.31%] [G loss: 3.581139]\n",
            "337 [D loss: 0.631393, acc: 65.62%] [G loss: 5.240447]\n",
            "338 [D loss: 0.225092, acc: 90.62%] [G loss: 3.810165]\n",
            "339 [D loss: 0.396703, acc: 84.38%] [G loss: 4.461096]\n",
            "340 [D loss: 0.291554, acc: 89.06%] [G loss: 4.751321]\n",
            "341 [D loss: 0.298000, acc: 90.62%] [G loss: 4.241800]\n",
            "342 [D loss: 0.368891, acc: 84.38%] [G loss: 5.452102]\n",
            "343 [D loss: 0.195305, acc: 92.19%] [G loss: 4.653059]\n",
            "344 [D loss: 0.352605, acc: 84.38%] [G loss: 4.103017]\n",
            "345 [D loss: 0.329623, acc: 89.06%] [G loss: 4.983900]\n",
            "346 [D loss: 0.280717, acc: 87.50%] [G loss: 4.751849]\n",
            "347 [D loss: 0.696217, acc: 62.50%] [G loss: 4.962787]\n",
            "348 [D loss: 0.411066, acc: 82.81%] [G loss: 4.256636]\n",
            "349 [D loss: 0.277052, acc: 89.06%] [G loss: 4.270427]\n",
            "350 [D loss: 0.536904, acc: 71.88%] [G loss: 4.628149]\n",
            "351 [D loss: 0.382762, acc: 79.69%] [G loss: 4.248326]\n",
            "352 [D loss: 0.390901, acc: 79.69%] [G loss: 5.504124]\n",
            "353 [D loss: 0.448730, acc: 78.12%] [G loss: 4.606466]\n",
            "354 [D loss: 0.374941, acc: 82.81%] [G loss: 4.256765]\n",
            "355 [D loss: 0.391284, acc: 78.12%] [G loss: 4.422535]\n",
            "356 [D loss: 0.369716, acc: 85.94%] [G loss: 3.557994]\n",
            "357 [D loss: 0.485675, acc: 76.56%] [G loss: 5.519348]\n",
            "358 [D loss: 0.245040, acc: 95.31%] [G loss: 4.661147]\n",
            "359 [D loss: 0.401824, acc: 81.25%] [G loss: 4.321971]\n",
            "360 [D loss: 0.559096, acc: 68.75%] [G loss: 4.591323]\n",
            "361 [D loss: 0.224228, acc: 92.19%] [G loss: 4.499806]\n",
            "362 [D loss: 0.558732, acc: 67.19%] [G loss: 4.210859]\n",
            "363 [D loss: 0.285867, acc: 90.62%] [G loss: 4.392057]\n",
            "364 [D loss: 0.513675, acc: 67.19%] [G loss: 4.510512]\n",
            "365 [D loss: 0.361032, acc: 84.38%] [G loss: 3.503811]\n",
            "366 [D loss: 0.344515, acc: 89.06%] [G loss: 4.619485]\n",
            "367 [D loss: 0.356295, acc: 82.81%] [G loss: 4.629776]\n",
            "368 [D loss: 0.323336, acc: 90.62%] [G loss: 4.014842]\n",
            "369 [D loss: 0.448143, acc: 75.00%] [G loss: 3.543202]\n",
            "370 [D loss: 0.399144, acc: 81.25%] [G loss: 3.522963]\n",
            "371 [D loss: 0.246424, acc: 93.75%] [G loss: 4.434581]\n",
            "372 [D loss: 0.493910, acc: 76.56%] [G loss: 4.030402]\n",
            "373 [D loss: 0.286305, acc: 90.62%] [G loss: 3.614507]\n",
            "374 [D loss: 0.421057, acc: 79.69%] [G loss: 4.529065]\n",
            "375 [D loss: 0.524838, acc: 73.44%] [G loss: 4.482099]\n",
            "376 [D loss: 0.380891, acc: 82.81%] [G loss: 3.705812]\n",
            "377 [D loss: 0.488503, acc: 75.00%] [G loss: 4.068835]\n",
            "378 [D loss: 0.375851, acc: 87.50%] [G loss: 4.269349]\n",
            "379 [D loss: 0.456744, acc: 78.12%] [G loss: 4.357350]\n",
            "380 [D loss: 0.355819, acc: 82.81%] [G loss: 3.286299]\n",
            "381 [D loss: 0.442111, acc: 82.81%] [G loss: 3.988506]\n",
            "382 [D loss: 0.424272, acc: 79.69%] [G loss: 3.813191]\n",
            "383 [D loss: 0.244707, acc: 93.75%] [G loss: 3.666762]\n",
            "384 [D loss: 0.519659, acc: 70.31%] [G loss: 4.311805]\n",
            "385 [D loss: 0.285541, acc: 89.06%] [G loss: 4.257497]\n",
            "386 [D loss: 0.355564, acc: 82.81%] [G loss: 4.008746]\n",
            "387 [D loss: 0.504376, acc: 78.12%] [G loss: 3.866831]\n",
            "388 [D loss: 0.294752, acc: 92.19%] [G loss: 4.194392]\n",
            "389 [D loss: 0.382415, acc: 85.94%] [G loss: 4.181497]\n",
            "390 [D loss: 0.374088, acc: 81.25%] [G loss: 3.685089]\n",
            "391 [D loss: 0.589781, acc: 65.62%] [G loss: 3.726328]\n",
            "392 [D loss: 0.463814, acc: 79.69%] [G loss: 4.019102]\n",
            "393 [D loss: 0.356881, acc: 85.94%] [G loss: 3.963749]\n",
            "394 [D loss: 0.449241, acc: 82.81%] [G loss: 4.450780]\n",
            "395 [D loss: 0.372532, acc: 87.50%] [G loss: 3.736974]\n",
            "396 [D loss: 0.472282, acc: 76.56%] [G loss: 4.799354]\n",
            "397 [D loss: 0.303512, acc: 89.06%] [G loss: 4.011162]\n",
            "398 [D loss: 0.351396, acc: 82.81%] [G loss: 4.633988]\n",
            "399 [D loss: 0.311065, acc: 85.94%] [G loss: 3.795691]\n",
            "400 [D loss: 0.718243, acc: 53.12%] [G loss: 4.175724]\n",
            "401 [D loss: 0.303942, acc: 87.50%] [G loss: 4.014709]\n",
            "402 [D loss: 0.395714, acc: 85.94%] [G loss: 4.324571]\n",
            "403 [D loss: 0.403014, acc: 78.12%] [G loss: 4.308933]\n",
            "404 [D loss: 0.278464, acc: 95.31%] [G loss: 4.111486]\n",
            "405 [D loss: 0.415537, acc: 81.25%] [G loss: 4.404549]\n",
            "406 [D loss: 0.345491, acc: 85.94%] [G loss: 4.739882]\n",
            "407 [D loss: 0.499549, acc: 81.25%] [G loss: 4.389852]\n",
            "408 [D loss: 0.408142, acc: 87.50%] [G loss: 3.785168]\n",
            "409 [D loss: 0.370783, acc: 84.38%] [G loss: 4.505995]\n",
            "410 [D loss: 0.451706, acc: 76.56%] [G loss: 4.415167]\n",
            "411 [D loss: 0.516569, acc: 75.00%] [G loss: 4.128092]\n",
            "412 [D loss: 0.322802, acc: 82.81%] [G loss: 4.285487]\n",
            "413 [D loss: 0.309328, acc: 95.31%] [G loss: 4.402771]\n",
            "414 [D loss: 0.287552, acc: 87.50%] [G loss: 4.027544]\n",
            "415 [D loss: 0.478887, acc: 76.56%] [G loss: 3.900560]\n",
            "416 [D loss: 0.391931, acc: 84.38%] [G loss: 4.714600]\n",
            "417 [D loss: 0.305694, acc: 87.50%] [G loss: 3.940904]\n",
            "418 [D loss: 0.382211, acc: 81.25%] [G loss: 4.186625]\n",
            "419 [D loss: 0.355632, acc: 89.06%] [G loss: 3.873907]\n",
            "420 [D loss: 0.652225, acc: 62.50%] [G loss: 4.868308]\n",
            "421 [D loss: 0.405244, acc: 81.25%] [G loss: 4.389193]\n",
            "422 [D loss: 0.399532, acc: 84.38%] [G loss: 4.314150]\n",
            "423 [D loss: 0.289596, acc: 92.19%] [G loss: 4.654509]\n",
            "424 [D loss: 0.372086, acc: 79.69%] [G loss: 4.386326]\n",
            "425 [D loss: 0.380362, acc: 78.12%] [G loss: 3.648150]\n",
            "426 [D loss: 0.737901, acc: 57.81%] [G loss: 4.307854]\n",
            "427 [D loss: 0.204240, acc: 95.31%] [G loss: 3.919615]\n",
            "428 [D loss: 0.318439, acc: 87.50%] [G loss: 4.591650]\n",
            "429 [D loss: 0.337086, acc: 84.38%] [G loss: 4.683000]\n",
            "430 [D loss: 0.333828, acc: 81.25%] [G loss: 4.197709]\n",
            "431 [D loss: 0.511535, acc: 67.19%] [G loss: 4.000780]\n",
            "432 [D loss: 0.488422, acc: 78.12%] [G loss: 4.202229]\n",
            "433 [D loss: 0.281288, acc: 90.62%] [G loss: 4.590038]\n",
            "434 [D loss: 0.574227, acc: 68.75%] [G loss: 4.598677]\n",
            "435 [D loss: 0.218433, acc: 95.31%] [G loss: 4.162760]\n",
            "436 [D loss: 0.558410, acc: 68.75%] [G loss: 4.966537]\n",
            "437 [D loss: 0.391199, acc: 87.50%] [G loss: 4.317271]\n",
            "438 [D loss: 0.455524, acc: 81.25%] [G loss: 4.415173]\n",
            "439 [D loss: 0.329984, acc: 87.50%] [G loss: 4.011652]\n",
            "440 [D loss: 0.410380, acc: 84.38%] [G loss: 4.612678]\n",
            "441 [D loss: 0.405655, acc: 85.94%] [G loss: 3.826351]\n",
            "442 [D loss: 0.537949, acc: 70.31%] [G loss: 4.040759]\n",
            "443 [D loss: 0.323883, acc: 85.94%] [G loss: 4.087474]\n",
            "444 [D loss: 0.599486, acc: 67.19%] [G loss: 5.098281]\n",
            "445 [D loss: 0.353577, acc: 81.25%] [G loss: 4.226773]\n",
            "446 [D loss: 0.362782, acc: 85.94%] [G loss: 4.182341]\n",
            "447 [D loss: 0.413178, acc: 84.38%] [G loss: 4.168564]\n",
            "448 [D loss: 0.430084, acc: 79.69%] [G loss: 4.268519]\n",
            "449 [D loss: 0.487798, acc: 71.88%] [G loss: 3.660240]\n",
            "450 [D loss: 0.432777, acc: 73.44%] [G loss: 3.371467]\n",
            "451 [D loss: 0.371293, acc: 90.62%] [G loss: 3.910395]\n",
            "452 [D loss: 0.384597, acc: 84.38%] [G loss: 5.075181]\n",
            "453 [D loss: 0.402498, acc: 81.25%] [G loss: 3.925888]\n",
            "454 [D loss: 0.382467, acc: 85.94%] [G loss: 4.197353]\n",
            "455 [D loss: 0.345877, acc: 87.50%] [G loss: 4.051335]\n",
            "456 [D loss: 0.561153, acc: 73.44%] [G loss: 4.262473]\n",
            "457 [D loss: 0.252368, acc: 92.19%] [G loss: 3.662431]\n",
            "458 [D loss: 0.672722, acc: 57.81%] [G loss: 3.663017]\n",
            "459 [D loss: 0.356114, acc: 87.50%] [G loss: 3.743427]\n",
            "460 [D loss: 0.439457, acc: 81.25%] [G loss: 4.231015]\n",
            "461 [D loss: 0.414313, acc: 79.69%] [G loss: 3.778229]\n",
            "462 [D loss: 0.379996, acc: 84.38%] [G loss: 4.319091]\n",
            "463 [D loss: 0.399933, acc: 84.38%] [G loss: 4.136239]\n",
            "464 [D loss: 0.475855, acc: 81.25%] [G loss: 4.010959]\n",
            "465 [D loss: 0.377428, acc: 87.50%] [G loss: 4.371298]\n",
            "466 [D loss: 0.318758, acc: 90.62%] [G loss: 4.343920]\n",
            "467 [D loss: 0.410176, acc: 82.81%] [G loss: 4.619050]\n",
            "468 [D loss: 0.478014, acc: 73.44%] [G loss: 4.243309]\n",
            "469 [D loss: 0.366615, acc: 81.25%] [G loss: 3.922444]\n",
            "470 [D loss: 0.367407, acc: 89.06%] [G loss: 4.185836]\n",
            "471 [D loss: 0.364726, acc: 79.69%] [G loss: 4.283673]\n",
            "472 [D loss: 0.317669, acc: 89.06%] [G loss: 3.896808]\n",
            "473 [D loss: 0.469929, acc: 75.00%] [G loss: 3.687547]\n",
            "474 [D loss: 0.440404, acc: 81.25%] [G loss: 4.023328]\n",
            "475 [D loss: 0.317175, acc: 87.50%] [G loss: 3.981161]\n",
            "476 [D loss: 0.691631, acc: 62.50%] [G loss: 4.260782]\n",
            "477 [D loss: 0.342253, acc: 84.38%] [G loss: 3.854467]\n",
            "478 [D loss: 0.570961, acc: 75.00%] [G loss: 4.009405]\n",
            "479 [D loss: 0.448896, acc: 78.12%] [G loss: 4.191448]\n",
            "480 [D loss: 0.481939, acc: 79.69%] [G loss: 3.755957]\n",
            "481 [D loss: 0.486304, acc: 81.25%] [G loss: 4.414364]\n",
            "482 [D loss: 0.352391, acc: 85.94%] [G loss: 3.822443]\n",
            "483 [D loss: 0.433614, acc: 79.69%] [G loss: 3.656986]\n",
            "484 [D loss: 0.358624, acc: 87.50%] [G loss: 4.036359]\n",
            "485 [D loss: 0.373820, acc: 81.25%] [G loss: 3.413145]\n",
            "486 [D loss: 0.371431, acc: 84.38%] [G loss: 3.820308]\n",
            "487 [D loss: 0.394186, acc: 82.81%] [G loss: 4.150892]\n",
            "488 [D loss: 0.435699, acc: 78.12%] [G loss: 4.382349]\n",
            "489 [D loss: 0.358312, acc: 87.50%] [G loss: 4.334970]\n",
            "490 [D loss: 0.434664, acc: 81.25%] [G loss: 3.858643]\n",
            "491 [D loss: 0.261146, acc: 90.62%] [G loss: 4.251317]\n",
            "492 [D loss: 0.403117, acc: 82.81%] [G loss: 3.846780]\n",
            "493 [D loss: 0.349837, acc: 87.50%] [G loss: 4.121821]\n",
            "494 [D loss: 0.514917, acc: 73.44%] [G loss: 3.834327]\n",
            "495 [D loss: 0.314572, acc: 90.62%] [G loss: 4.052602]\n",
            "496 [D loss: 0.504770, acc: 71.88%] [G loss: 3.565838]\n",
            "497 [D loss: 0.302192, acc: 85.94%] [G loss: 3.694204]\n",
            "498 [D loss: 0.401932, acc: 81.25%] [G loss: 4.236167]\n",
            "499 [D loss: 0.421911, acc: 76.56%] [G loss: 3.891664]\n",
            "500 [D loss: 0.414491, acc: 87.50%] [G loss: 4.054443]\n",
            "501 [D loss: 0.463157, acc: 87.50%] [G loss: 3.869378]\n",
            "502 [D loss: 0.466140, acc: 78.12%] [G loss: 4.677005]\n",
            "503 [D loss: 0.345405, acc: 87.50%] [G loss: 3.710260]\n",
            "504 [D loss: 0.463088, acc: 78.12%] [G loss: 4.447486]\n",
            "505 [D loss: 0.469203, acc: 79.69%] [G loss: 4.583614]\n",
            "506 [D loss: 0.324334, acc: 84.38%] [G loss: 3.980954]\n",
            "507 [D loss: 0.404494, acc: 84.38%] [G loss: 4.724085]\n",
            "508 [D loss: 0.471102, acc: 79.69%] [G loss: 4.058104]\n",
            "509 [D loss: 0.357926, acc: 85.94%] [G loss: 3.841718]\n",
            "510 [D loss: 0.518677, acc: 65.62%] [G loss: 3.746201]\n",
            "511 [D loss: 0.413574, acc: 84.38%] [G loss: 3.982919]\n",
            "512 [D loss: 0.334921, acc: 90.62%] [G loss: 3.946430]\n",
            "513 [D loss: 0.644047, acc: 59.38%] [G loss: 3.940888]\n",
            "514 [D loss: 0.362792, acc: 87.50%] [G loss: 4.056717]\n",
            "515 [D loss: 0.631791, acc: 71.88%] [G loss: 4.229683]\n",
            "516 [D loss: 0.304709, acc: 92.19%] [G loss: 4.038330]\n",
            "517 [D loss: 0.522142, acc: 65.62%] [G loss: 3.883213]\n",
            "518 [D loss: 0.436256, acc: 75.00%] [G loss: 3.683782]\n",
            "519 [D loss: 0.367993, acc: 85.94%] [G loss: 3.844320]\n",
            "520 [D loss: 0.397264, acc: 82.81%] [G loss: 4.379230]\n",
            "521 [D loss: 0.428947, acc: 81.25%] [G loss: 3.989361]\n",
            "522 [D loss: 0.382797, acc: 82.81%] [G loss: 3.531731]\n",
            "523 [D loss: 0.507230, acc: 79.69%] [G loss: 4.164046]\n",
            "524 [D loss: 0.399230, acc: 87.50%] [G loss: 3.655155]\n",
            "525 [D loss: 0.330260, acc: 95.31%] [G loss: 4.200042]\n",
            "526 [D loss: 0.327517, acc: 92.19%] [G loss: 4.383235]\n",
            "527 [D loss: 0.414654, acc: 85.94%] [G loss: 4.199165]\n",
            "528 [D loss: 0.500827, acc: 78.12%] [G loss: 4.637880]\n",
            "529 [D loss: 0.372932, acc: 85.94%] [G loss: 3.880043]\n",
            "530 [D loss: 0.406510, acc: 84.38%] [G loss: 4.709511]\n",
            "531 [D loss: 0.407430, acc: 79.69%] [G loss: 4.242671]\n",
            "532 [D loss: 0.348673, acc: 87.50%] [G loss: 4.319456]\n",
            "533 [D loss: 0.363617, acc: 81.25%] [G loss: 4.598439]\n",
            "534 [D loss: 0.524120, acc: 70.31%] [G loss: 3.976318]\n",
            "535 [D loss: 0.440651, acc: 81.25%] [G loss: 4.050576]\n",
            "536 [D loss: 0.702110, acc: 60.94%] [G loss: 4.002719]\n",
            "537 [D loss: 0.487606, acc: 71.88%] [G loss: 4.425103]\n",
            "538 [D loss: 0.380044, acc: 89.06%] [G loss: 4.041029]\n",
            "539 [D loss: 0.553477, acc: 64.06%] [G loss: 4.298326]\n",
            "540 [D loss: 0.514462, acc: 71.88%] [G loss: 3.897350]\n",
            "541 [D loss: 0.562509, acc: 68.75%] [G loss: 3.722082]\n",
            "542 [D loss: 0.504015, acc: 78.12%] [G loss: 3.913144]\n",
            "543 [D loss: 0.452458, acc: 82.81%] [G loss: 3.715696]\n",
            "544 [D loss: 0.547425, acc: 79.69%] [G loss: 3.640937]\n",
            "545 [D loss: 0.480027, acc: 76.56%] [G loss: 3.423952]\n",
            "546 [D loss: 0.286621, acc: 90.62%] [G loss: 4.049896]\n",
            "547 [D loss: 0.497441, acc: 78.12%] [G loss: 4.157762]\n",
            "548 [D loss: 0.308763, acc: 87.50%] [G loss: 3.482502]\n",
            "549 [D loss: 0.440727, acc: 85.94%] [G loss: 3.785958]\n",
            "550 [D loss: 0.345543, acc: 82.81%] [G loss: 3.637817]\n",
            "551 [D loss: 0.426051, acc: 85.94%] [G loss: 3.957897]\n",
            "552 [D loss: 0.465441, acc: 78.12%] [G loss: 4.097880]\n",
            "553 [D loss: 0.457965, acc: 79.69%] [G loss: 3.329126]\n",
            "554 [D loss: 0.584206, acc: 73.44%] [G loss: 3.338471]\n",
            "555 [D loss: 0.536407, acc: 71.88%] [G loss: 4.197394]\n",
            "556 [D loss: 0.591474, acc: 70.31%] [G loss: 3.633636]\n",
            "557 [D loss: 0.414442, acc: 81.25%] [G loss: 3.558744]\n",
            "558 [D loss: 0.382308, acc: 84.38%] [G loss: 3.482013]\n",
            "559 [D loss: 0.521142, acc: 73.44%] [G loss: 3.607368]\n",
            "560 [D loss: 0.506716, acc: 73.44%] [G loss: 3.176907]\n",
            "561 [D loss: 0.398504, acc: 79.69%] [G loss: 3.485805]\n",
            "562 [D loss: 0.341556, acc: 84.38%] [G loss: 3.697489]\n",
            "563 [D loss: 0.456588, acc: 81.25%] [G loss: 3.365131]\n",
            "564 [D loss: 0.519856, acc: 70.31%] [G loss: 3.582812]\n",
            "565 [D loss: 0.382073, acc: 79.69%] [G loss: 3.553413]\n",
            "566 [D loss: 0.445099, acc: 81.25%] [G loss: 3.590214]\n",
            "567 [D loss: 0.572995, acc: 70.31%] [G loss: 3.207375]\n",
            "568 [D loss: 0.523491, acc: 78.12%] [G loss: 3.557242]\n",
            "569 [D loss: 0.393128, acc: 84.38%] [G loss: 3.722644]\n",
            "570 [D loss: 0.671970, acc: 65.62%] [G loss: 3.885799]\n",
            "571 [D loss: 0.397066, acc: 85.94%] [G loss: 3.522430]\n",
            "572 [D loss: 0.487967, acc: 81.25%] [G loss: 3.354031]\n",
            "573 [D loss: 0.391437, acc: 85.94%] [G loss: 3.565853]\n",
            "574 [D loss: 0.403058, acc: 89.06%] [G loss: 3.770445]\n",
            "575 [D loss: 0.386912, acc: 82.81%] [G loss: 3.737574]\n",
            "576 [D loss: 0.561037, acc: 70.31%] [G loss: 3.475384]\n",
            "577 [D loss: 0.426036, acc: 78.12%] [G loss: 3.508931]\n",
            "578 [D loss: 0.579605, acc: 68.75%] [G loss: 4.045995]\n",
            "579 [D loss: 0.494878, acc: 76.56%] [G loss: 3.406493]\n",
            "580 [D loss: 0.326246, acc: 92.19%] [G loss: 3.950248]\n",
            "581 [D loss: 0.595244, acc: 71.88%] [G loss: 4.136745]\n",
            "582 [D loss: 0.396825, acc: 84.38%] [G loss: 3.716676]\n",
            "583 [D loss: 0.477024, acc: 79.69%] [G loss: 3.705401]\n",
            "584 [D loss: 0.409722, acc: 82.81%] [G loss: 3.842524]\n",
            "585 [D loss: 0.614581, acc: 65.62%] [G loss: 3.850517]\n",
            "586 [D loss: 0.357654, acc: 87.50%] [G loss: 3.658759]\n",
            "587 [D loss: 0.451634, acc: 78.12%] [G loss: 3.804523]\n",
            "588 [D loss: 0.449884, acc: 78.12%] [G loss: 3.906615]\n",
            "589 [D loss: 0.568318, acc: 67.19%] [G loss: 4.012455]\n",
            "590 [D loss: 0.437990, acc: 76.56%] [G loss: 3.818765]\n",
            "591 [D loss: 0.512604, acc: 78.12%] [G loss: 4.432680]\n",
            "592 [D loss: 0.453031, acc: 79.69%] [G loss: 4.021079]\n",
            "593 [D loss: 0.549133, acc: 70.31%] [G loss: 3.460776]\n",
            "594 [D loss: 0.328735, acc: 93.75%] [G loss: 3.633113]\n",
            "595 [D loss: 0.554885, acc: 76.56%] [G loss: 4.204583]\n",
            "596 [D loss: 0.437346, acc: 76.56%] [G loss: 3.714379]\n",
            "597 [D loss: 0.522903, acc: 79.69%] [G loss: 3.783430]\n",
            "598 [D loss: 0.454505, acc: 81.25%] [G loss: 3.796822]\n",
            "599 [D loss: 0.368630, acc: 82.81%] [G loss: 3.766819]\n",
            "600 [D loss: 0.550147, acc: 73.44%] [G loss: 4.016598]\n",
            "601 [D loss: 0.491593, acc: 75.00%] [G loss: 3.641545]\n",
            "602 [D loss: 0.388663, acc: 82.81%] [G loss: 3.710973]\n",
            "603 [D loss: 0.642289, acc: 71.88%] [G loss: 4.060326]\n",
            "604 [D loss: 0.411669, acc: 79.69%] [G loss: 3.315325]\n",
            "605 [D loss: 0.436562, acc: 78.12%] [G loss: 4.221416]\n",
            "606 [D loss: 0.455260, acc: 87.50%] [G loss: 3.465083]\n",
            "607 [D loss: 0.513523, acc: 78.12%] [G loss: 3.780960]\n",
            "608 [D loss: 0.452612, acc: 81.25%] [G loss: 3.598672]\n",
            "609 [D loss: 0.321015, acc: 89.06%] [G loss: 4.095383]\n",
            "610 [D loss: 0.793806, acc: 50.00%] [G loss: 3.875204]\n",
            "611 [D loss: 0.376494, acc: 84.38%] [G loss: 3.761606]\n",
            "612 [D loss: 0.481331, acc: 78.12%] [G loss: 3.778722]\n",
            "613 [D loss: 0.566115, acc: 71.88%] [G loss: 3.731280]\n",
            "614 [D loss: 0.628540, acc: 70.31%] [G loss: 3.735393]\n",
            "615 [D loss: 0.571571, acc: 73.44%] [G loss: 3.555475]\n",
            "616 [D loss: 0.469316, acc: 81.25%] [G loss: 3.758049]\n",
            "617 [D loss: 0.334963, acc: 90.62%] [G loss: 4.086455]\n",
            "618 [D loss: 0.379545, acc: 82.81%] [G loss: 3.402960]\n",
            "619 [D loss: 0.566398, acc: 73.44%] [G loss: 3.455377]\n",
            "620 [D loss: 0.319326, acc: 90.62%] [G loss: 3.705011]\n",
            "621 [D loss: 0.541960, acc: 79.69%] [G loss: 3.695534]\n",
            "622 [D loss: 0.438394, acc: 79.69%] [G loss: 3.761380]\n",
            "623 [D loss: 0.488957, acc: 79.69%] [G loss: 3.658757]\n",
            "624 [D loss: 0.480021, acc: 78.12%] [G loss: 4.290747]\n",
            "625 [D loss: 0.711616, acc: 59.38%] [G loss: 3.958486]\n",
            "626 [D loss: 0.446833, acc: 79.69%] [G loss: 3.631673]\n",
            "627 [D loss: 0.544823, acc: 70.31%] [G loss: 3.759043]\n",
            "628 [D loss: 0.388542, acc: 85.94%] [G loss: 3.813064]\n",
            "629 [D loss: 0.573490, acc: 67.19%] [G loss: 3.659564]\n",
            "630 [D loss: 0.338333, acc: 87.50%] [G loss: 4.019903]\n",
            "631 [D loss: 0.474221, acc: 78.12%] [G loss: 3.487331]\n",
            "632 [D loss: 0.550795, acc: 73.44%] [G loss: 3.520583]\n",
            "633 [D loss: 0.448908, acc: 79.69%] [G loss: 3.677142]\n",
            "634 [D loss: 0.621332, acc: 75.00%] [G loss: 3.574840]\n",
            "635 [D loss: 0.402679, acc: 85.94%] [G loss: 3.462418]\n",
            "636 [D loss: 0.511710, acc: 75.00%] [G loss: 3.772121]\n",
            "637 [D loss: 0.489310, acc: 79.69%] [G loss: 3.369476]\n",
            "638 [D loss: 0.342301, acc: 89.06%] [G loss: 3.208563]\n",
            "639 [D loss: 0.583022, acc: 73.44%] [G loss: 3.278115]\n",
            "640 [D loss: 0.684026, acc: 60.94%] [G loss: 3.505634]\n",
            "641 [D loss: 0.531273, acc: 71.88%] [G loss: 3.830074]\n",
            "642 [D loss: 0.475853, acc: 79.69%] [G loss: 4.021305]\n",
            "643 [D loss: 0.473024, acc: 78.12%] [G loss: 3.433321]\n",
            "644 [D loss: 0.686090, acc: 57.81%] [G loss: 3.567482]\n",
            "645 [D loss: 0.490985, acc: 71.88%] [G loss: 3.239201]\n",
            "646 [D loss: 0.521614, acc: 70.31%] [G loss: 3.164503]\n",
            "647 [D loss: 0.528264, acc: 75.00%] [G loss: 3.518158]\n",
            "648 [D loss: 0.499248, acc: 79.69%] [G loss: 3.770807]\n",
            "649 [D loss: 0.570341, acc: 70.31%] [G loss: 3.561257]\n",
            "650 [D loss: 0.472286, acc: 79.69%] [G loss: 3.159291]\n",
            "651 [D loss: 0.462989, acc: 75.00%] [G loss: 4.127712]\n",
            "652 [D loss: 0.632152, acc: 68.75%] [G loss: 3.240998]\n",
            "653 [D loss: 0.445796, acc: 75.00%] [G loss: 3.111785]\n",
            "654 [D loss: 0.481570, acc: 81.25%] [G loss: 3.559944]\n",
            "655 [D loss: 0.563940, acc: 68.75%] [G loss: 3.334187]\n",
            "656 [D loss: 0.455323, acc: 79.69%] [G loss: 3.483374]\n",
            "657 [D loss: 0.476781, acc: 76.56%] [G loss: 3.642440]\n",
            "658 [D loss: 0.501745, acc: 73.44%] [G loss: 3.646934]\n",
            "659 [D loss: 0.444636, acc: 79.69%] [G loss: 3.587186]\n",
            "660 [D loss: 0.501806, acc: 78.12%] [G loss: 4.135115]\n",
            "661 [D loss: 0.586056, acc: 68.75%] [G loss: 2.992956]\n",
            "662 [D loss: 0.481195, acc: 70.31%] [G loss: 3.015477]\n",
            "663 [D loss: 0.447623, acc: 78.12%] [G loss: 2.933863]\n",
            "664 [D loss: 0.709823, acc: 57.81%] [G loss: 3.161597]\n",
            "665 [D loss: 0.481016, acc: 79.69%] [G loss: 3.527938]\n",
            "666 [D loss: 0.424259, acc: 82.81%] [G loss: 2.916490]\n",
            "667 [D loss: 0.665970, acc: 71.88%] [G loss: 3.482315]\n",
            "668 [D loss: 0.676262, acc: 67.19%] [G loss: 3.221129]\n",
            "669 [D loss: 0.416883, acc: 81.25%] [G loss: 2.964615]\n",
            "670 [D loss: 0.563028, acc: 73.44%] [G loss: 3.347621]\n",
            "671 [D loss: 0.560344, acc: 67.19%] [G loss: 3.007967]\n",
            "672 [D loss: 0.491794, acc: 71.88%] [G loss: 3.286705]\n",
            "673 [D loss: 0.536823, acc: 78.12%] [G loss: 3.694989]\n",
            "674 [D loss: 0.596649, acc: 78.12%] [G loss: 3.400864]\n",
            "675 [D loss: 0.522423, acc: 75.00%] [G loss: 3.188268]\n",
            "676 [D loss: 0.549493, acc: 65.62%] [G loss: 3.301631]\n",
            "677 [D loss: 0.466039, acc: 81.25%] [G loss: 3.239120]\n",
            "678 [D loss: 0.505559, acc: 76.56%] [G loss: 3.194434]\n",
            "679 [D loss: 0.534416, acc: 65.62%] [G loss: 3.174297]\n",
            "680 [D loss: 0.584355, acc: 68.75%] [G loss: 3.088698]\n",
            "681 [D loss: 0.433106, acc: 87.50%] [G loss: 3.197509]\n",
            "682 [D loss: 0.476681, acc: 79.69%] [G loss: 2.934828]\n",
            "683 [D loss: 0.564587, acc: 71.88%] [G loss: 2.894618]\n",
            "684 [D loss: 0.624545, acc: 65.62%] [G loss: 3.540518]\n",
            "685 [D loss: 0.417607, acc: 82.81%] [G loss: 3.048179]\n",
            "686 [D loss: 0.466783, acc: 84.38%] [G loss: 3.507239]\n",
            "687 [D loss: 0.503527, acc: 75.00%] [G loss: 3.405501]\n",
            "688 [D loss: 0.526440, acc: 76.56%] [G loss: 3.070062]\n",
            "689 [D loss: 0.464078, acc: 78.12%] [G loss: 3.966931]\n",
            "690 [D loss: 0.678899, acc: 64.06%] [G loss: 3.026008]\n",
            "691 [D loss: 0.716209, acc: 56.25%] [G loss: 3.223896]\n",
            "692 [D loss: 0.543932, acc: 65.62%] [G loss: 3.138844]\n",
            "693 [D loss: 0.582168, acc: 71.88%] [G loss: 3.003269]\n",
            "694 [D loss: 0.438131, acc: 84.38%] [G loss: 2.899682]\n",
            "695 [D loss: 0.459948, acc: 78.12%] [G loss: 3.314741]\n",
            "696 [D loss: 0.583773, acc: 71.88%] [G loss: 2.790355]\n",
            "697 [D loss: 0.487770, acc: 76.56%] [G loss: 2.887301]\n",
            "698 [D loss: 0.638093, acc: 67.19%] [G loss: 3.190055]\n",
            "699 [D loss: 0.693409, acc: 67.19%] [G loss: 2.945214]\n",
            "700 [D loss: 0.479537, acc: 81.25%] [G loss: 3.216143]\n",
            "701 [D loss: 0.419781, acc: 81.25%] [G loss: 3.671168]\n",
            "702 [D loss: 0.661818, acc: 65.62%] [G loss: 2.891521]\n",
            "703 [D loss: 0.539985, acc: 75.00%] [G loss: 3.012546]\n",
            "704 [D loss: 0.460539, acc: 79.69%] [G loss: 3.430438]\n",
            "705 [D loss: 0.523409, acc: 76.56%] [G loss: 3.132766]\n",
            "706 [D loss: 0.514328, acc: 71.88%] [G loss: 3.044656]\n",
            "707 [D loss: 0.542834, acc: 73.44%] [G loss: 3.192164]\n",
            "708 [D loss: 0.514034, acc: 76.56%] [G loss: 3.032051]\n",
            "709 [D loss: 0.574495, acc: 73.44%] [G loss: 3.099252]\n",
            "710 [D loss: 0.614760, acc: 62.50%] [G loss: 3.038374]\n",
            "711 [D loss: 0.489516, acc: 79.69%] [G loss: 2.889461]\n",
            "712 [D loss: 0.488331, acc: 76.56%] [G loss: 3.462840]\n",
            "713 [D loss: 0.693079, acc: 57.81%] [G loss: 3.004961]\n",
            "714 [D loss: 0.438192, acc: 76.56%] [G loss: 3.015314]\n",
            "715 [D loss: 0.509626, acc: 71.88%] [G loss: 3.475904]\n",
            "716 [D loss: 0.585823, acc: 70.31%] [G loss: 3.072348]\n",
            "717 [D loss: 0.473386, acc: 76.56%] [G loss: 3.241976]\n",
            "718 [D loss: 0.492039, acc: 78.12%] [G loss: 3.449997]\n",
            "719 [D loss: 0.527786, acc: 70.31%] [G loss: 3.075821]\n",
            "720 [D loss: 0.493171, acc: 71.88%] [G loss: 2.950829]\n",
            "721 [D loss: 0.589783, acc: 67.19%] [G loss: 2.992092]\n",
            "722 [D loss: 0.570588, acc: 71.88%] [G loss: 3.157237]\n",
            "723 [D loss: 0.468619, acc: 76.56%] [G loss: 2.906540]\n",
            "724 [D loss: 0.650566, acc: 62.50%] [G loss: 3.031111]\n",
            "725 [D loss: 0.464910, acc: 81.25%] [G loss: 2.925891]\n",
            "726 [D loss: 0.653167, acc: 68.75%] [G loss: 3.165279]\n",
            "727 [D loss: 0.484118, acc: 79.69%] [G loss: 3.093807]\n",
            "728 [D loss: 0.602755, acc: 65.62%] [G loss: 3.157190]\n",
            "729 [D loss: 0.633672, acc: 64.06%] [G loss: 3.149530]\n",
            "730 [D loss: 0.484022, acc: 76.56%] [G loss: 2.881099]\n",
            "731 [D loss: 0.561680, acc: 73.44%] [G loss: 2.921008]\n",
            "732 [D loss: 0.554445, acc: 68.75%] [G loss: 3.158763]\n",
            "733 [D loss: 0.528471, acc: 73.44%] [G loss: 2.851666]\n",
            "734 [D loss: 0.546764, acc: 70.31%] [G loss: 3.169015]\n",
            "735 [D loss: 0.648750, acc: 65.62%] [G loss: 3.132424]\n",
            "736 [D loss: 0.416272, acc: 82.81%] [G loss: 3.041214]\n",
            "737 [D loss: 0.621042, acc: 67.19%] [G loss: 2.709936]\n",
            "738 [D loss: 0.528282, acc: 64.06%] [G loss: 3.530897]\n",
            "739 [D loss: 0.448082, acc: 79.69%] [G loss: 2.907799]\n",
            "740 [D loss: 0.575528, acc: 71.88%] [G loss: 2.853834]\n",
            "741 [D loss: 0.488372, acc: 76.56%] [G loss: 2.992526]\n",
            "742 [D loss: 0.688321, acc: 59.38%] [G loss: 3.267147]\n",
            "743 [D loss: 0.589696, acc: 70.31%] [G loss: 2.994801]\n",
            "744 [D loss: 0.574957, acc: 73.44%] [G loss: 3.067948]\n",
            "745 [D loss: 0.549242, acc: 75.00%] [G loss: 2.982830]\n",
            "746 [D loss: 0.579677, acc: 67.19%] [G loss: 2.882122]\n",
            "747 [D loss: 0.462963, acc: 78.12%] [G loss: 3.364861]\n",
            "748 [D loss: 0.542257, acc: 73.44%] [G loss: 2.974795]\n",
            "749 [D loss: 0.419247, acc: 87.50%] [G loss: 3.343379]\n",
            "750 [D loss: 0.588614, acc: 65.62%] [G loss: 2.850056]\n",
            "751 [D loss: 0.539753, acc: 75.00%] [G loss: 2.925156]\n",
            "752 [D loss: 0.542767, acc: 68.75%] [G loss: 2.689558]\n",
            "753 [D loss: 0.603566, acc: 59.38%] [G loss: 2.989927]\n",
            "754 [D loss: 0.599518, acc: 67.19%] [G loss: 3.162783]\n",
            "755 [D loss: 0.525705, acc: 73.44%] [G loss: 3.039018]\n",
            "756 [D loss: 0.543891, acc: 71.88%] [G loss: 3.001717]\n",
            "757 [D loss: 0.581431, acc: 62.50%] [G loss: 2.745799]\n",
            "758 [D loss: 0.424429, acc: 79.69%] [G loss: 3.033654]\n",
            "759 [D loss: 0.555981, acc: 73.44%] [G loss: 3.072634]\n",
            "760 [D loss: 0.422255, acc: 81.25%] [G loss: 3.293753]\n",
            "761 [D loss: 0.515469, acc: 79.69%] [G loss: 3.049660]\n",
            "762 [D loss: 0.570556, acc: 73.44%] [G loss: 2.907883]\n",
            "763 [D loss: 0.666053, acc: 64.06%] [G loss: 3.174262]\n",
            "764 [D loss: 0.507735, acc: 75.00%] [G loss: 3.502883]\n",
            "765 [D loss: 0.469194, acc: 84.38%] [G loss: 3.050604]\n",
            "766 [D loss: 0.664369, acc: 67.19%] [G loss: 3.109556]\n",
            "767 [D loss: 0.446767, acc: 82.81%] [G loss: 3.180296]\n",
            "768 [D loss: 0.605077, acc: 65.62%] [G loss: 3.230464]\n",
            "769 [D loss: 0.429880, acc: 81.25%] [G loss: 3.167758]\n",
            "770 [D loss: 0.506314, acc: 71.88%] [G loss: 3.069552]\n",
            "771 [D loss: 0.519472, acc: 64.06%] [G loss: 3.345551]\n",
            "772 [D loss: 0.575162, acc: 67.19%] [G loss: 3.360268]\n",
            "773 [D loss: 0.473500, acc: 78.12%] [G loss: 2.609784]\n",
            "774 [D loss: 0.537702, acc: 73.44%] [G loss: 3.398069]\n",
            "775 [D loss: 0.466289, acc: 79.69%] [G loss: 2.848073]\n",
            "776 [D loss: 0.568478, acc: 65.62%] [G loss: 2.767276]\n",
            "777 [D loss: 0.525744, acc: 73.44%] [G loss: 2.989751]\n",
            "778 [D loss: 0.646448, acc: 64.06%] [G loss: 2.733563]\n",
            "779 [D loss: 0.592542, acc: 67.19%] [G loss: 3.083807]\n",
            "780 [D loss: 0.518258, acc: 76.56%] [G loss: 2.811667]\n",
            "781 [D loss: 0.539490, acc: 71.88%] [G loss: 3.462140]\n",
            "782 [D loss: 0.476290, acc: 76.56%] [G loss: 3.032013]\n",
            "783 [D loss: 0.437410, acc: 78.12%] [G loss: 3.113229]\n",
            "784 [D loss: 0.540313, acc: 70.31%] [G loss: 2.996660]\n",
            "785 [D loss: 0.516278, acc: 82.81%] [G loss: 2.746598]\n",
            "786 [D loss: 0.634013, acc: 64.06%] [G loss: 3.032224]\n",
            "787 [D loss: 0.475941, acc: 76.56%] [G loss: 3.012582]\n",
            "788 [D loss: 0.549639, acc: 76.56%] [G loss: 2.747423]\n",
            "789 [D loss: 0.528712, acc: 75.00%] [G loss: 2.915339]\n",
            "790 [D loss: 0.502316, acc: 73.44%] [G loss: 2.563059]\n",
            "791 [D loss: 0.622258, acc: 64.06%] [G loss: 3.228395]\n",
            "792 [D loss: 0.577516, acc: 73.44%] [G loss: 2.828244]\n",
            "793 [D loss: 0.611650, acc: 73.44%] [G loss: 2.914404]\n",
            "794 [D loss: 0.489499, acc: 81.25%] [G loss: 2.747626]\n",
            "795 [D loss: 0.603580, acc: 64.06%] [G loss: 2.894326]\n",
            "796 [D loss: 0.611951, acc: 64.06%] [G loss: 3.347190]\n",
            "797 [D loss: 0.594686, acc: 64.06%] [G loss: 2.847950]\n",
            "798 [D loss: 0.520707, acc: 71.88%] [G loss: 2.963098]\n",
            "799 [D loss: 0.449102, acc: 82.81%] [G loss: 3.156837]\n",
            "800 [D loss: 0.469291, acc: 78.12%] [G loss: 2.846912]\n",
            "801 [D loss: 0.540836, acc: 82.81%] [G loss: 3.500021]\n",
            "802 [D loss: 0.589669, acc: 78.12%] [G loss: 3.073672]\n",
            "803 [D loss: 0.509334, acc: 79.69%] [G loss: 2.827268]\n",
            "804 [D loss: 0.508896, acc: 76.56%] [G loss: 3.237115]\n",
            "805 [D loss: 0.573948, acc: 65.62%] [G loss: 2.760084]\n",
            "806 [D loss: 0.758517, acc: 67.19%] [G loss: 3.096158]\n",
            "807 [D loss: 0.726322, acc: 50.00%] [G loss: 2.918680]\n",
            "808 [D loss: 0.608521, acc: 67.19%] [G loss: 2.698892]\n",
            "809 [D loss: 0.563946, acc: 67.19%] [G loss: 2.835591]\n",
            "810 [D loss: 0.688300, acc: 65.62%] [G loss: 2.653985]\n",
            "811 [D loss: 0.547604, acc: 75.00%] [G loss: 2.561876]\n",
            "812 [D loss: 0.489207, acc: 79.69%] [G loss: 2.767485]\n",
            "813 [D loss: 0.601991, acc: 65.62%] [G loss: 2.575216]\n",
            "814 [D loss: 0.441643, acc: 78.12%] [G loss: 3.320810]\n",
            "815 [D loss: 0.592704, acc: 70.31%] [G loss: 2.939556]\n",
            "816 [D loss: 0.486338, acc: 71.88%] [G loss: 3.114604]\n",
            "817 [D loss: 0.726247, acc: 53.12%] [G loss: 2.883318]\n",
            "818 [D loss: 0.476690, acc: 76.56%] [G loss: 3.079381]\n",
            "819 [D loss: 0.542877, acc: 75.00%] [G loss: 3.253909]\n",
            "820 [D loss: 0.484824, acc: 81.25%] [G loss: 3.389604]\n",
            "821 [D loss: 0.523322, acc: 75.00%] [G loss: 3.274330]\n",
            "822 [D loss: 0.629727, acc: 62.50%] [G loss: 2.836401]\n",
            "823 [D loss: 0.576835, acc: 71.88%] [G loss: 2.734948]\n",
            "824 [D loss: 0.617659, acc: 64.06%] [G loss: 2.928608]\n",
            "825 [D loss: 0.579924, acc: 76.56%] [G loss: 3.133276]\n",
            "826 [D loss: 0.526079, acc: 76.56%] [G loss: 2.800353]\n",
            "827 [D loss: 0.672254, acc: 59.38%] [G loss: 2.793499]\n",
            "828 [D loss: 0.645817, acc: 67.19%] [G loss: 2.938618]\n",
            "829 [D loss: 0.530777, acc: 68.75%] [G loss: 2.626219]\n",
            "830 [D loss: 0.544820, acc: 79.69%] [G loss: 3.142670]\n",
            "831 [D loss: 0.640513, acc: 62.50%] [G loss: 2.654124]\n",
            "832 [D loss: 0.493482, acc: 79.69%] [G loss: 2.816720]\n",
            "833 [D loss: 0.552776, acc: 70.31%] [G loss: 2.596597]\n",
            "834 [D loss: 0.584989, acc: 67.19%] [G loss: 2.946398]\n",
            "835 [D loss: 0.593043, acc: 70.31%] [G loss: 2.851270]\n",
            "836 [D loss: 0.538909, acc: 76.56%] [G loss: 3.008996]\n",
            "837 [D loss: 0.701122, acc: 57.81%] [G loss: 2.829685]\n",
            "838 [D loss: 0.474152, acc: 78.12%] [G loss: 2.965832]\n",
            "839 [D loss: 0.550106, acc: 68.75%] [G loss: 3.023506]\n",
            "840 [D loss: 0.586898, acc: 68.75%] [G loss: 2.708256]\n",
            "841 [D loss: 0.619267, acc: 65.62%] [G loss: 2.994701]\n",
            "842 [D loss: 0.567016, acc: 70.31%] [G loss: 2.823295]\n",
            "843 [D loss: 0.570736, acc: 68.75%] [G loss: 2.591699]\n",
            "844 [D loss: 0.575460, acc: 64.06%] [G loss: 3.062936]\n",
            "845 [D loss: 0.594908, acc: 68.75%] [G loss: 2.646632]\n",
            "846 [D loss: 0.549242, acc: 70.31%] [G loss: 2.491084]\n",
            "847 [D loss: 0.496312, acc: 79.69%] [G loss: 3.142646]\n",
            "848 [D loss: 0.494590, acc: 78.12%] [G loss: 2.690696]\n",
            "849 [D loss: 0.567489, acc: 70.31%] [G loss: 2.714164]\n",
            "850 [D loss: 0.724992, acc: 46.88%] [G loss: 2.365880]\n",
            "851 [D loss: 0.583830, acc: 70.31%] [G loss: 2.899729]\n",
            "852 [D loss: 0.745695, acc: 56.25%] [G loss: 2.621701]\n",
            "853 [D loss: 0.559378, acc: 75.00%] [G loss: 2.710845]\n",
            "854 [D loss: 0.422961, acc: 79.69%] [G loss: 2.748811]\n",
            "855 [D loss: 0.596667, acc: 70.31%] [G loss: 2.936887]\n",
            "856 [D loss: 0.589826, acc: 70.31%] [G loss: 3.123636]\n",
            "857 [D loss: 0.668552, acc: 64.06%] [G loss: 2.738210]\n",
            "858 [D loss: 0.588605, acc: 70.31%] [G loss: 2.563965]\n",
            "859 [D loss: 0.633941, acc: 67.19%] [G loss: 2.740793]\n",
            "860 [D loss: 0.536337, acc: 75.00%] [G loss: 2.562672]\n",
            "861 [D loss: 0.615316, acc: 67.19%] [G loss: 2.546531]\n",
            "862 [D loss: 0.631940, acc: 60.94%] [G loss: 2.711594]\n",
            "863 [D loss: 0.524629, acc: 76.56%] [G loss: 2.677053]\n",
            "864 [D loss: 0.482695, acc: 78.12%] [G loss: 2.768151]\n",
            "865 [D loss: 0.475029, acc: 76.56%] [G loss: 2.900476]\n",
            "866 [D loss: 0.585548, acc: 65.62%] [G loss: 2.581635]\n",
            "867 [D loss: 0.514648, acc: 75.00%] [G loss: 2.705598]\n",
            "868 [D loss: 0.632973, acc: 70.31%] [G loss: 2.782640]\n",
            "869 [D loss: 0.575690, acc: 73.44%] [G loss: 2.681120]\n",
            "870 [D loss: 0.540303, acc: 70.31%] [G loss: 2.641595]\n",
            "871 [D loss: 0.635724, acc: 67.19%] [G loss: 2.602178]\n",
            "872 [D loss: 0.544961, acc: 75.00%] [G loss: 2.615211]\n",
            "873 [D loss: 0.608999, acc: 68.75%] [G loss: 2.429818]\n",
            "874 [D loss: 0.618269, acc: 71.88%] [G loss: 2.627407]\n",
            "875 [D loss: 0.738986, acc: 57.81%] [G loss: 2.595165]\n",
            "876 [D loss: 0.530467, acc: 70.31%] [G loss: 2.788373]\n",
            "877 [D loss: 0.586502, acc: 64.06%] [G loss: 2.589261]\n",
            "878 [D loss: 0.607918, acc: 68.75%] [G loss: 2.913311]\n",
            "879 [D loss: 0.623380, acc: 65.62%] [G loss: 2.466012]\n",
            "880 [D loss: 0.483924, acc: 79.69%] [G loss: 2.503218]\n",
            "881 [D loss: 0.655107, acc: 65.62%] [G loss: 2.643944]\n",
            "882 [D loss: 0.682846, acc: 67.19%] [G loss: 2.612919]\n",
            "883 [D loss: 0.496381, acc: 75.00%] [G loss: 2.644430]\n",
            "884 [D loss: 0.622182, acc: 65.62%] [G loss: 2.720296]\n",
            "885 [D loss: 0.569328, acc: 75.00%] [G loss: 2.855122]\n",
            "886 [D loss: 0.653025, acc: 59.38%] [G loss: 2.860247]\n",
            "887 [D loss: 0.584233, acc: 75.00%] [G loss: 2.475907]\n",
            "888 [D loss: 0.593088, acc: 64.06%] [G loss: 2.687859]\n",
            "889 [D loss: 0.579636, acc: 71.88%] [G loss: 2.577138]\n",
            "890 [D loss: 0.534783, acc: 75.00%] [G loss: 2.596410]\n",
            "891 [D loss: 0.599606, acc: 70.31%] [G loss: 2.391713]\n",
            "892 [D loss: 0.611770, acc: 62.50%] [G loss: 2.613025]\n",
            "893 [D loss: 0.607375, acc: 70.31%] [G loss: 2.681716]\n",
            "894 [D loss: 0.567527, acc: 67.19%] [G loss: 2.821734]\n",
            "895 [D loss: 0.537384, acc: 71.88%] [G loss: 2.789377]\n",
            "896 [D loss: 0.568882, acc: 68.75%] [G loss: 2.946203]\n",
            "897 [D loss: 0.555672, acc: 75.00%] [G loss: 2.592968]\n",
            "898 [D loss: 0.580215, acc: 68.75%] [G loss: 2.663953]\n",
            "899 [D loss: 0.659777, acc: 62.50%] [G loss: 2.689710]\n",
            "900 [D loss: 0.567058, acc: 76.56%] [G loss: 2.642622]\n",
            "901 [D loss: 0.629944, acc: 67.19%] [G loss: 2.833507]\n",
            "902 [D loss: 0.554146, acc: 68.75%] [G loss: 2.663516]\n",
            "903 [D loss: 0.458219, acc: 81.25%] [G loss: 3.212451]\n",
            "904 [D loss: 0.676401, acc: 60.94%] [G loss: 2.756536]\n",
            "905 [D loss: 0.563378, acc: 70.31%] [G loss: 2.656799]\n",
            "906 [D loss: 0.591517, acc: 67.19%] [G loss: 2.606966]\n",
            "907 [D loss: 0.495056, acc: 75.00%] [G loss: 2.881021]\n",
            "908 [D loss: 0.552794, acc: 75.00%] [G loss: 2.523706]\n",
            "909 [D loss: 0.481268, acc: 75.00%] [G loss: 2.999432]\n",
            "910 [D loss: 0.579042, acc: 68.75%] [G loss: 2.650206]\n",
            "911 [D loss: 0.524264, acc: 78.12%] [G loss: 2.913440]\n",
            "912 [D loss: 0.492693, acc: 75.00%] [G loss: 2.530054]\n",
            "913 [D loss: 0.523656, acc: 71.88%] [G loss: 2.947371]\n",
            "914 [D loss: 0.550393, acc: 70.31%] [G loss: 2.908637]\n",
            "915 [D loss: 0.620380, acc: 65.62%] [G loss: 2.849917]\n",
            "916 [D loss: 0.543825, acc: 71.88%] [G loss: 2.439010]\n",
            "917 [D loss: 0.551343, acc: 70.31%] [G loss: 2.420115]\n",
            "918 [D loss: 0.560881, acc: 67.19%] [G loss: 2.647440]\n",
            "919 [D loss: 0.602719, acc: 67.19%] [G loss: 2.802692]\n",
            "920 [D loss: 0.600876, acc: 65.62%] [G loss: 2.964479]\n",
            "921 [D loss: 0.539656, acc: 71.88%] [G loss: 3.183966]\n",
            "922 [D loss: 0.585680, acc: 71.88%] [G loss: 3.135771]\n",
            "923 [D loss: 0.538432, acc: 71.88%] [G loss: 2.554796]\n",
            "924 [D loss: 0.650293, acc: 62.50%] [G loss: 2.934243]\n",
            "925 [D loss: 0.564325, acc: 65.62%] [G loss: 2.725151]\n",
            "926 [D loss: 0.520433, acc: 67.19%] [G loss: 2.835588]\n",
            "927 [D loss: 0.718839, acc: 60.94%] [G loss: 2.753579]\n",
            "928 [D loss: 0.634482, acc: 60.94%] [G loss: 2.652214]\n",
            "929 [D loss: 0.562248, acc: 70.31%] [G loss: 2.642687]\n",
            "930 [D loss: 0.595198, acc: 64.06%] [G loss: 2.999422]\n",
            "931 [D loss: 0.601677, acc: 62.50%] [G loss: 2.735344]\n",
            "932 [D loss: 0.615062, acc: 71.88%] [G loss: 2.574404]\n",
            "933 [D loss: 0.683863, acc: 64.06%] [G loss: 2.681971]\n",
            "934 [D loss: 0.681669, acc: 54.69%] [G loss: 2.544991]\n",
            "935 [D loss: 0.605703, acc: 60.94%] [G loss: 2.806633]\n",
            "936 [D loss: 0.668217, acc: 67.19%] [G loss: 2.681535]\n",
            "937 [D loss: 0.533268, acc: 71.88%] [G loss: 2.841335]\n",
            "938 [D loss: 0.562267, acc: 75.00%] [G loss: 2.457929]\n",
            "939 [D loss: 0.711896, acc: 64.06%] [G loss: 2.491921]\n",
            "940 [D loss: 0.551565, acc: 71.88%] [G loss: 2.541023]\n",
            "941 [D loss: 0.510321, acc: 73.44%] [G loss: 2.776931]\n",
            "942 [D loss: 0.716375, acc: 56.25%] [G loss: 2.443546]\n",
            "943 [D loss: 0.644657, acc: 57.81%] [G loss: 2.612260]\n",
            "944 [D loss: 0.637209, acc: 64.06%] [G loss: 2.485797]\n",
            "945 [D loss: 0.589198, acc: 68.75%] [G loss: 2.766536]\n",
            "946 [D loss: 0.537826, acc: 73.44%] [G loss: 2.713924]\n",
            "947 [D loss: 0.701749, acc: 62.50%] [G loss: 2.917759]\n",
            "948 [D loss: 0.625497, acc: 57.81%] [G loss: 2.710528]\n",
            "949 [D loss: 0.621544, acc: 64.06%] [G loss: 2.505268]\n",
            "950 [D loss: 0.586882, acc: 65.62%] [G loss: 2.766952]\n",
            "951 [D loss: 0.458683, acc: 79.69%] [G loss: 2.733329]\n",
            "952 [D loss: 0.607436, acc: 68.75%] [G loss: 2.715724]\n",
            "953 [D loss: 0.585973, acc: 68.75%] [G loss: 2.512214]\n",
            "954 [D loss: 0.589024, acc: 70.31%] [G loss: 2.680430]\n",
            "955 [D loss: 0.594574, acc: 65.62%] [G loss: 2.632441]\n",
            "956 [D loss: 0.729463, acc: 57.81%] [G loss: 2.464032]\n",
            "957 [D loss: 0.568853, acc: 73.44%] [G loss: 2.820048]\n",
            "958 [D loss: 0.612366, acc: 60.94%] [G loss: 2.478628]\n",
            "959 [D loss: 0.496012, acc: 76.56%] [G loss: 2.529121]\n",
            "960 [D loss: 0.607215, acc: 67.19%] [G loss: 2.912323]\n",
            "961 [D loss: 0.530847, acc: 71.88%] [G loss: 2.877827]\n",
            "962 [D loss: 0.686054, acc: 60.94%] [G loss: 2.387528]\n",
            "963 [D loss: 0.549582, acc: 75.00%] [G loss: 2.435016]\n",
            "964 [D loss: 0.611614, acc: 62.50%] [G loss: 2.253958]\n",
            "965 [D loss: 0.529781, acc: 73.44%] [G loss: 2.930668]\n",
            "966 [D loss: 0.615928, acc: 68.75%] [G loss: 2.602762]\n",
            "967 [D loss: 0.529752, acc: 73.44%] [G loss: 2.690684]\n",
            "968 [D loss: 0.472841, acc: 76.56%] [G loss: 2.579234]\n",
            "969 [D loss: 0.601714, acc: 68.75%] [G loss: 2.548672]\n",
            "970 [D loss: 0.585978, acc: 67.19%] [G loss: 2.457937]\n",
            "971 [D loss: 0.520445, acc: 78.12%] [G loss: 2.575811]\n",
            "972 [D loss: 0.609334, acc: 64.06%] [G loss: 2.653770]\n",
            "973 [D loss: 0.631451, acc: 65.62%] [G loss: 2.459762]\n",
            "974 [D loss: 0.549949, acc: 68.75%] [G loss: 2.702203]\n",
            "975 [D loss: 0.562508, acc: 67.19%] [G loss: 2.946569]\n",
            "976 [D loss: 0.754792, acc: 45.31%] [G loss: 2.485887]\n",
            "977 [D loss: 0.616788, acc: 68.75%] [G loss: 2.373828]\n",
            "978 [D loss: 0.650360, acc: 59.38%] [G loss: 2.469485]\n",
            "979 [D loss: 0.604270, acc: 67.19%] [G loss: 2.494586]\n",
            "980 [D loss: 0.618567, acc: 67.19%] [G loss: 2.464125]\n",
            "981 [D loss: 0.569409, acc: 70.31%] [G loss: 2.360145]\n",
            "982 [D loss: 0.508927, acc: 76.56%] [G loss: 2.703619]\n",
            "983 [D loss: 0.533291, acc: 73.44%] [G loss: 2.703797]\n",
            "984 [D loss: 0.656287, acc: 60.94%] [G loss: 2.788382]\n",
            "985 [D loss: 0.618376, acc: 65.62%] [G loss: 2.629396]\n",
            "986 [D loss: 0.482127, acc: 70.31%] [G loss: 2.845292]\n",
            "987 [D loss: 0.683793, acc: 59.38%] [G loss: 2.556674]\n",
            "988 [D loss: 0.594296, acc: 67.19%] [G loss: 2.859063]\n",
            "989 [D loss: 0.572869, acc: 71.88%] [G loss: 2.627269]\n",
            "990 [D loss: 0.690813, acc: 53.12%] [G loss: 2.636591]\n",
            "991 [D loss: 0.609245, acc: 65.62%] [G loss: 2.720392]\n",
            "992 [D loss: 0.701913, acc: 53.12%] [G loss: 2.515098]\n",
            "993 [D loss: 0.569074, acc: 73.44%] [G loss: 2.277296]\n",
            "994 [D loss: 0.552277, acc: 76.56%] [G loss: 2.598541]\n",
            "995 [D loss: 0.725406, acc: 56.25%] [G loss: 2.445515]\n",
            "996 [D loss: 0.578281, acc: 64.06%] [G loss: 2.870986]\n",
            "997 [D loss: 0.545763, acc: 73.44%] [G loss: 2.532640]\n",
            "998 [D loss: 0.741026, acc: 60.94%] [G loss: 2.633260]\n",
            "999 [D loss: 0.616697, acc: 57.81%] [G loss: 2.500682]\n",
            "1000 [D loss: 0.560565, acc: 71.88%] [G loss: 2.739280]\n",
            "1001 [D loss: 0.625464, acc: 59.38%] [G loss: 2.459123]\n",
            "1002 [D loss: 0.553398, acc: 75.00%] [G loss: 2.905971]\n",
            "1003 [D loss: 0.610858, acc: 65.62%] [G loss: 2.533502]\n",
            "1004 [D loss: 0.575727, acc: 70.31%] [G loss: 2.352979]\n",
            "1005 [D loss: 0.585843, acc: 68.75%] [G loss: 2.775225]\n",
            "1006 [D loss: 0.595741, acc: 71.88%] [G loss: 2.827172]\n",
            "1007 [D loss: 0.505987, acc: 76.56%] [G loss: 2.489114]\n",
            "1008 [D loss: 0.659807, acc: 64.06%] [G loss: 2.605648]\n",
            "1009 [D loss: 0.636621, acc: 62.50%] [G loss: 2.570142]\n",
            "1010 [D loss: 0.607504, acc: 64.06%] [G loss: 2.791715]\n",
            "1011 [D loss: 0.550225, acc: 71.88%] [G loss: 2.534127]\n",
            "1012 [D loss: 0.635267, acc: 68.75%] [G loss: 2.541573]\n",
            "1013 [D loss: 0.616160, acc: 65.62%] [G loss: 2.364147]\n",
            "1014 [D loss: 0.626355, acc: 64.06%] [G loss: 2.272263]\n",
            "1015 [D loss: 0.540609, acc: 68.75%] [G loss: 2.403055]\n",
            "1016 [D loss: 0.560776, acc: 71.88%] [G loss: 2.516161]\n",
            "1017 [D loss: 0.504597, acc: 79.69%] [G loss: 2.406184]\n",
            "1018 [D loss: 0.659427, acc: 60.94%] [G loss: 2.668897]\n",
            "1019 [D loss: 0.579491, acc: 75.00%] [G loss: 2.506393]\n",
            "1020 [D loss: 0.619241, acc: 70.31%] [G loss: 2.389920]\n",
            "1021 [D loss: 0.596663, acc: 70.31%] [G loss: 2.689190]\n",
            "1022 [D loss: 0.652571, acc: 57.81%] [G loss: 2.486049]\n",
            "1023 [D loss: 0.576182, acc: 67.19%] [G loss: 2.806881]\n",
            "1024 [D loss: 0.539348, acc: 73.44%] [G loss: 2.456529]\n",
            "1025 [D loss: 0.739956, acc: 53.12%] [G loss: 2.370473]\n",
            "1026 [D loss: 0.627901, acc: 62.50%] [G loss: 2.652796]\n",
            "1027 [D loss: 0.622817, acc: 67.19%] [G loss: 2.508054]\n",
            "1028 [D loss: 0.558989, acc: 67.19%] [G loss: 2.619422]\n",
            "1029 [D loss: 0.667395, acc: 59.38%] [G loss: 2.317307]\n",
            "1030 [D loss: 0.567007, acc: 76.56%] [G loss: 2.283211]\n",
            "1031 [D loss: 0.563913, acc: 73.44%] [G loss: 2.231013]\n",
            "1032 [D loss: 0.611794, acc: 57.81%] [G loss: 2.665888]\n",
            "1033 [D loss: 0.565843, acc: 78.12%] [G loss: 2.495639]\n",
            "1034 [D loss: 0.772434, acc: 53.12%] [G loss: 2.433409]\n",
            "1035 [D loss: 0.596533, acc: 64.06%] [G loss: 2.305680]\n",
            "1036 [D loss: 0.535577, acc: 79.69%] [G loss: 2.429175]\n",
            "1037 [D loss: 0.585643, acc: 73.44%] [G loss: 2.488474]\n",
            "1038 [D loss: 0.642510, acc: 70.31%] [G loss: 2.481910]\n",
            "1039 [D loss: 0.532799, acc: 67.19%] [G loss: 2.585525]\n",
            "1040 [D loss: 0.782639, acc: 50.00%] [G loss: 2.108411]\n",
            "1041 [D loss: 0.588145, acc: 67.19%] [G loss: 2.395595]\n",
            "1042 [D loss: 0.751568, acc: 56.25%] [G loss: 2.336675]\n",
            "1043 [D loss: 0.678227, acc: 64.06%] [G loss: 2.493359]\n",
            "1044 [D loss: 0.629236, acc: 67.19%] [G loss: 2.317178]\n",
            "1045 [D loss: 0.592037, acc: 71.88%] [G loss: 2.301292]\n",
            "1046 [D loss: 0.506708, acc: 73.44%] [G loss: 2.305642]\n",
            "1047 [D loss: 0.577470, acc: 73.44%] [G loss: 2.548330]\n",
            "1048 [D loss: 0.620238, acc: 65.62%] [G loss: 2.501902]\n",
            "1049 [D loss: 0.497093, acc: 79.69%] [G loss: 2.720785]\n",
            "1050 [D loss: 0.596829, acc: 75.00%] [G loss: 2.443165]\n",
            "1051 [D loss: 0.537112, acc: 76.56%] [G loss: 2.279501]\n",
            "1052 [D loss: 0.540105, acc: 68.75%] [G loss: 2.395523]\n",
            "1053 [D loss: 0.573890, acc: 73.44%] [G loss: 2.667054]\n",
            "1054 [D loss: 0.562173, acc: 78.12%] [G loss: 2.781325]\n",
            "1055 [D loss: 0.590537, acc: 71.88%] [G loss: 2.699362]\n",
            "1056 [D loss: 0.583681, acc: 71.88%] [G loss: 2.866650]\n",
            "1057 [D loss: 0.669011, acc: 64.06%] [G loss: 2.419900]\n",
            "1058 [D loss: 0.542010, acc: 73.44%] [G loss: 2.467402]\n",
            "1059 [D loss: 0.527443, acc: 78.12%] [G loss: 2.573590]\n",
            "1060 [D loss: 0.544025, acc: 70.31%] [G loss: 2.391475]\n",
            "1061 [D loss: 0.588403, acc: 68.75%] [G loss: 2.672017]\n",
            "1062 [D loss: 0.518634, acc: 75.00%] [G loss: 2.577226]\n",
            "1063 [D loss: 0.649014, acc: 64.06%] [G loss: 2.587051]\n",
            "1064 [D loss: 0.563802, acc: 73.44%] [G loss: 2.796131]\n",
            "1065 [D loss: 0.598188, acc: 67.19%] [G loss: 2.521837]\n",
            "1066 [D loss: 0.667077, acc: 60.94%] [G loss: 2.567684]\n",
            "1067 [D loss: 0.575801, acc: 68.75%] [G loss: 2.532162]\n",
            "1068 [D loss: 0.608492, acc: 67.19%] [G loss: 2.655785]\n",
            "1069 [D loss: 0.682056, acc: 60.94%] [G loss: 2.330007]\n",
            "1070 [D loss: 0.738734, acc: 57.81%] [G loss: 2.585821]\n",
            "1071 [D loss: 0.572509, acc: 71.88%] [G loss: 2.474939]\n",
            "1072 [D loss: 0.704299, acc: 56.25%] [G loss: 2.511873]\n",
            "1073 [D loss: 0.574122, acc: 67.19%] [G loss: 2.674575]\n",
            "1074 [D loss: 0.612819, acc: 67.19%] [G loss: 2.569892]\n",
            "1075 [D loss: 0.610302, acc: 70.31%] [G loss: 2.398901]\n",
            "1076 [D loss: 0.701724, acc: 65.62%] [G loss: 2.579057]\n",
            "1077 [D loss: 0.578578, acc: 73.44%] [G loss: 2.578819]\n",
            "1078 [D loss: 0.605163, acc: 60.94%] [G loss: 2.191360]\n",
            "1079 [D loss: 0.582030, acc: 67.19%] [G loss: 2.305397]\n",
            "1080 [D loss: 0.608952, acc: 73.44%] [G loss: 2.474342]\n",
            "1081 [D loss: 0.732521, acc: 57.81%] [G loss: 2.248550]\n",
            "1082 [D loss: 0.477204, acc: 79.69%] [G loss: 2.483005]\n",
            "1083 [D loss: 0.630879, acc: 60.94%] [G loss: 2.286480]\n",
            "1084 [D loss: 0.725514, acc: 53.12%] [G loss: 3.035389]\n",
            "1085 [D loss: 0.638189, acc: 70.31%] [G loss: 2.142063]\n",
            "1086 [D loss: 0.630966, acc: 62.50%] [G loss: 2.527211]\n",
            "1087 [D loss: 0.705086, acc: 60.94%] [G loss: 2.405690]\n",
            "1088 [D loss: 0.576717, acc: 67.19%] [G loss: 2.330229]\n",
            "1089 [D loss: 0.639001, acc: 59.38%] [G loss: 2.318051]\n",
            "1090 [D loss: 0.679667, acc: 62.50%] [G loss: 2.248451]\n",
            "1091 [D loss: 0.778314, acc: 43.75%] [G loss: 2.428164]\n",
            "1092 [D loss: 0.568353, acc: 75.00%] [G loss: 2.562429]\n",
            "1093 [D loss: 0.475897, acc: 85.94%] [G loss: 2.650782]\n",
            "1094 [D loss: 0.574265, acc: 71.88%] [G loss: 2.375373]\n",
            "1095 [D loss: 0.555662, acc: 65.62%] [G loss: 2.820349]\n",
            "1096 [D loss: 0.641272, acc: 57.81%] [G loss: 2.766641]\n",
            "1097 [D loss: 0.531551, acc: 73.44%] [G loss: 2.357611]\n",
            "1098 [D loss: 0.610836, acc: 68.75%] [G loss: 2.312980]\n",
            "1099 [D loss: 0.572029, acc: 68.75%] [G loss: 2.569663]\n",
            "1100 [D loss: 0.584938, acc: 62.50%] [G loss: 2.739305]\n",
            "1101 [D loss: 0.549893, acc: 73.44%] [G loss: 2.402861]\n",
            "1102 [D loss: 0.583938, acc: 64.06%] [G loss: 2.558429]\n",
            "1103 [D loss: 0.624007, acc: 65.62%] [G loss: 2.830120]\n",
            "1104 [D loss: 0.558089, acc: 71.88%] [G loss: 2.543109]\n",
            "1105 [D loss: 0.568760, acc: 68.75%] [G loss: 2.839721]\n",
            "1106 [D loss: 0.567303, acc: 71.88%] [G loss: 2.444929]\n",
            "1107 [D loss: 0.623133, acc: 64.06%] [G loss: 2.401662]\n",
            "1108 [D loss: 0.552369, acc: 71.88%] [G loss: 2.595336]\n",
            "1109 [D loss: 0.592026, acc: 62.50%] [G loss: 2.245939]\n",
            "1110 [D loss: 0.528493, acc: 76.56%] [G loss: 2.535753]\n",
            "1111 [D loss: 0.547072, acc: 78.12%] [G loss: 2.835276]\n",
            "1112 [D loss: 0.696104, acc: 60.94%] [G loss: 2.443118]\n",
            "1113 [D loss: 0.557510, acc: 78.12%] [G loss: 2.811363]\n",
            "1114 [D loss: 0.601437, acc: 70.31%] [G loss: 2.703514]\n",
            "1115 [D loss: 0.609445, acc: 62.50%] [G loss: 2.631128]\n",
            "1116 [D loss: 0.720987, acc: 56.25%] [G loss: 2.558981]\n",
            "1117 [D loss: 0.646115, acc: 64.06%] [G loss: 2.660515]\n",
            "1118 [D loss: 0.530874, acc: 75.00%] [G loss: 2.504251]\n",
            "1119 [D loss: 0.568471, acc: 70.31%] [G loss: 2.720892]\n",
            "1120 [D loss: 0.560546, acc: 73.44%] [G loss: 2.480375]\n",
            "1121 [D loss: 0.650320, acc: 57.81%] [G loss: 2.398035]\n",
            "1122 [D loss: 0.550429, acc: 73.44%] [G loss: 2.490717]\n",
            "1123 [D loss: 0.575254, acc: 68.75%] [G loss: 2.271491]\n",
            "1124 [D loss: 0.525889, acc: 73.44%] [G loss: 2.523080]\n",
            "1125 [D loss: 0.627714, acc: 65.62%] [G loss: 2.611180]\n",
            "1126 [D loss: 0.736109, acc: 51.56%] [G loss: 2.823725]\n",
            "1127 [D loss: 0.504723, acc: 76.56%] [G loss: 2.721260]\n",
            "1128 [D loss: 0.620496, acc: 73.44%] [G loss: 2.605668]\n",
            "1129 [D loss: 0.462331, acc: 82.81%] [G loss: 2.571431]\n",
            "1130 [D loss: 0.618376, acc: 64.06%] [G loss: 3.066483]\n",
            "1131 [D loss: 0.668608, acc: 56.25%] [G loss: 2.517353]\n",
            "1132 [D loss: 0.623257, acc: 65.62%] [G loss: 2.456513]\n",
            "1133 [D loss: 0.525558, acc: 73.44%] [G loss: 2.698135]\n",
            "1134 [D loss: 0.591357, acc: 70.31%] [G loss: 2.730445]\n",
            "1135 [D loss: 0.556917, acc: 67.19%] [G loss: 2.625169]\n",
            "1136 [D loss: 0.638262, acc: 64.06%] [G loss: 2.217617]\n",
            "1137 [D loss: 0.668447, acc: 59.38%] [G loss: 2.294521]\n",
            "1138 [D loss: 0.510776, acc: 75.00%] [G loss: 2.626472]\n",
            "1139 [D loss: 0.506586, acc: 73.44%] [G loss: 2.684395]\n",
            "1140 [D loss: 0.579944, acc: 70.31%] [G loss: 2.786901]\n",
            "1141 [D loss: 0.601715, acc: 65.62%] [G loss: 2.549175]\n",
            "1142 [D loss: 0.658197, acc: 68.75%] [G loss: 2.379222]\n",
            "1143 [D loss: 0.473639, acc: 81.25%] [G loss: 3.041781]\n",
            "1144 [D loss: 0.658299, acc: 67.19%] [G loss: 2.425362]\n",
            "1145 [D loss: 0.667768, acc: 60.94%] [G loss: 2.447127]\n",
            "1146 [D loss: 0.587669, acc: 71.88%] [G loss: 2.469341]\n",
            "1147 [D loss: 0.643905, acc: 64.06%] [G loss: 2.540834]\n",
            "1148 [D loss: 0.619500, acc: 60.94%] [G loss: 2.813924]\n",
            "1149 [D loss: 0.644597, acc: 68.75%] [G loss: 2.498859]\n",
            "1150 [D loss: 0.517667, acc: 78.12%] [G loss: 2.613575]\n",
            "1151 [D loss: 0.651630, acc: 62.50%] [G loss: 2.435423]\n",
            "1152 [D loss: 0.715353, acc: 57.81%] [G loss: 2.548886]\n",
            "1153 [D loss: 0.519027, acc: 70.31%] [G loss: 2.862258]\n",
            "1154 [D loss: 0.671746, acc: 56.25%] [G loss: 2.785212]\n",
            "1155 [D loss: 0.438235, acc: 84.38%] [G loss: 3.036145]\n",
            "1156 [D loss: 0.603442, acc: 68.75%] [G loss: 2.517091]\n",
            "1157 [D loss: 0.567715, acc: 70.31%] [G loss: 2.655550]\n",
            "1158 [D loss: 0.564014, acc: 67.19%] [G loss: 2.729458]\n",
            "1159 [D loss: 0.499404, acc: 75.00%] [G loss: 2.767049]\n",
            "1160 [D loss: 0.646514, acc: 67.19%] [G loss: 2.932699]\n",
            "1161 [D loss: 0.673357, acc: 59.38%] [G loss: 2.877013]\n",
            "1162 [D loss: 0.504461, acc: 76.56%] [G loss: 2.746956]\n",
            "1163 [D loss: 0.679613, acc: 62.50%] [G loss: 2.605513]\n",
            "1164 [D loss: 0.557446, acc: 68.75%] [G loss: 2.335596]\n",
            "1165 [D loss: 0.519650, acc: 78.12%] [G loss: 3.020264]\n",
            "1166 [D loss: 0.608979, acc: 67.19%] [G loss: 2.450352]\n",
            "1167 [D loss: 0.416220, acc: 84.38%] [G loss: 2.805644]\n",
            "1168 [D loss: 0.561393, acc: 78.12%] [G loss: 2.667438]\n",
            "1169 [D loss: 0.800288, acc: 48.44%] [G loss: 2.360833]\n",
            "1170 [D loss: 0.523572, acc: 70.31%] [G loss: 2.543436]\n",
            "1171 [D loss: 0.587539, acc: 64.06%] [G loss: 2.456530]\n",
            "1172 [D loss: 0.470835, acc: 73.44%] [G loss: 2.460534]\n",
            "1173 [D loss: 0.530329, acc: 71.88%] [G loss: 2.811759]\n",
            "1174 [D loss: 0.684878, acc: 59.38%] [G loss: 2.558404]\n",
            "1175 [D loss: 0.552834, acc: 81.25%] [G loss: 2.702298]\n",
            "1176 [D loss: 0.685651, acc: 59.38%] [G loss: 2.731823]\n",
            "1177 [D loss: 0.484149, acc: 79.69%] [G loss: 2.888423]\n",
            "1178 [D loss: 0.622576, acc: 56.25%] [G loss: 2.675949]\n",
            "1179 [D loss: 0.662005, acc: 59.38%] [G loss: 2.357069]\n",
            "1180 [D loss: 0.637422, acc: 59.38%] [G loss: 2.648656]\n",
            "1181 [D loss: 0.568800, acc: 70.31%] [G loss: 2.785609]\n",
            "1182 [D loss: 0.501315, acc: 75.00%] [G loss: 2.636323]\n",
            "1183 [D loss: 0.592427, acc: 68.75%] [G loss: 2.544195]\n",
            "1184 [D loss: 0.561561, acc: 68.75%] [G loss: 2.866673]\n",
            "1185 [D loss: 0.537589, acc: 75.00%] [G loss: 2.599428]\n",
            "1186 [D loss: 0.598953, acc: 68.75%] [G loss: 2.617030]\n",
            "1187 [D loss: 0.566745, acc: 73.44%] [G loss: 2.923910]\n",
            "1188 [D loss: 0.564135, acc: 71.88%] [G loss: 2.498442]\n",
            "1189 [D loss: 0.626624, acc: 60.94%] [G loss: 2.485492]\n",
            "1190 [D loss: 0.682677, acc: 57.81%] [G loss: 2.486922]\n",
            "1191 [D loss: 0.570355, acc: 68.75%] [G loss: 2.634441]\n",
            "1192 [D loss: 0.629222, acc: 67.19%] [G loss: 2.732468]\n",
            "1193 [D loss: 0.653807, acc: 65.62%] [G loss: 2.300058]\n",
            "1194 [D loss: 0.566837, acc: 67.19%] [G loss: 2.336459]\n",
            "1195 [D loss: 0.534631, acc: 79.69%] [G loss: 2.584518]\n",
            "1196 [D loss: 0.555582, acc: 65.62%] [G loss: 2.583213]\n",
            "1197 [D loss: 0.607822, acc: 62.50%] [G loss: 2.691732]\n",
            "1198 [D loss: 0.507745, acc: 76.56%] [G loss: 2.782220]\n",
            "1199 [D loss: 0.589531, acc: 67.19%] [G loss: 2.464882]\n",
            "1200 [D loss: 0.553993, acc: 71.88%] [G loss: 2.396722]\n",
            "1201 [D loss: 0.541164, acc: 65.62%] [G loss: 2.728914]\n",
            "1202 [D loss: 0.559191, acc: 68.75%] [G loss: 2.688290]\n",
            "1203 [D loss: 0.534531, acc: 81.25%] [G loss: 2.337915]\n",
            "1204 [D loss: 0.509741, acc: 67.19%] [G loss: 2.499246]\n",
            "1205 [D loss: 0.666440, acc: 62.50%] [G loss: 2.806290]\n",
            "1206 [D loss: 0.614809, acc: 67.19%] [G loss: 2.680304]\n",
            "1207 [D loss: 0.614689, acc: 67.19%] [G loss: 2.759307]\n",
            "1208 [D loss: 0.583476, acc: 70.31%] [G loss: 2.798881]\n",
            "1209 [D loss: 0.715425, acc: 57.81%] [G loss: 2.408574]\n",
            "1210 [D loss: 0.701758, acc: 64.06%] [G loss: 2.742480]\n",
            "1211 [D loss: 0.645613, acc: 62.50%] [G loss: 2.493104]\n",
            "1212 [D loss: 0.659170, acc: 54.69%] [G loss: 2.351450]\n",
            "1213 [D loss: 0.648033, acc: 65.62%] [G loss: 2.350370]\n",
            "1214 [D loss: 0.483989, acc: 76.56%] [G loss: 2.636228]\n",
            "1215 [D loss: 0.553619, acc: 73.44%] [G loss: 2.442884]\n",
            "1216 [D loss: 0.684708, acc: 56.25%] [G loss: 1.943930]\n",
            "1217 [D loss: 0.600252, acc: 65.62%] [G loss: 2.335651]\n",
            "1218 [D loss: 0.548651, acc: 76.56%] [G loss: 2.382205]\n",
            "1219 [D loss: 0.632966, acc: 59.38%] [G loss: 2.777437]\n",
            "1220 [D loss: 0.528746, acc: 73.44%] [G loss: 2.482972]\n",
            "1221 [D loss: 0.618587, acc: 70.31%] [G loss: 2.586634]\n",
            "1222 [D loss: 0.708586, acc: 62.50%] [G loss: 2.617537]\n",
            "1223 [D loss: 0.596177, acc: 65.62%] [G loss: 2.502139]\n",
            "1224 [D loss: 0.541898, acc: 75.00%] [G loss: 2.479752]\n",
            "1225 [D loss: 0.582037, acc: 71.88%] [G loss: 2.401220]\n",
            "1226 [D loss: 0.609099, acc: 67.19%] [G loss: 2.486557]\n",
            "1227 [D loss: 0.564224, acc: 67.19%] [G loss: 2.751652]\n",
            "1228 [D loss: 0.595794, acc: 62.50%] [G loss: 2.473033]\n",
            "1229 [D loss: 0.671619, acc: 60.94%] [G loss: 2.093915]\n",
            "1230 [D loss: 0.659891, acc: 59.38%] [G loss: 2.336769]\n",
            "1231 [D loss: 0.534895, acc: 73.44%] [G loss: 2.835611]\n",
            "1232 [D loss: 0.447188, acc: 82.81%] [G loss: 2.349108]\n",
            "1233 [D loss: 0.643636, acc: 60.94%] [G loss: 2.243438]\n",
            "1234 [D loss: 0.685605, acc: 59.38%] [G loss: 2.816754]\n",
            "1235 [D loss: 0.658185, acc: 59.38%] [G loss: 2.670119]\n",
            "1236 [D loss: 0.572155, acc: 65.62%] [G loss: 2.687920]\n",
            "1237 [D loss: 0.583090, acc: 65.62%] [G loss: 2.514754]\n",
            "1238 [D loss: 0.515706, acc: 75.00%] [G loss: 2.698917]\n",
            "1239 [D loss: 0.559389, acc: 71.88%] [G loss: 2.366948]\n",
            "1240 [D loss: 0.575460, acc: 68.75%] [G loss: 2.389614]\n",
            "1241 [D loss: 0.568551, acc: 68.75%] [G loss: 2.330598]\n",
            "1242 [D loss: 0.605666, acc: 70.31%] [G loss: 2.538346]\n",
            "1243 [D loss: 0.603295, acc: 62.50%] [G loss: 2.572648]\n",
            "1244 [D loss: 0.668466, acc: 65.62%] [G loss: 2.474992]\n",
            "1245 [D loss: 0.598236, acc: 71.88%] [G loss: 2.517493]\n",
            "1246 [D loss: 0.529801, acc: 71.88%] [G loss: 2.422845]\n",
            "1247 [D loss: 0.766495, acc: 48.44%] [G loss: 2.339445]\n",
            "1248 [D loss: 0.501309, acc: 76.56%] [G loss: 2.426641]\n",
            "1249 [D loss: 0.627353, acc: 65.62%] [G loss: 2.805479]\n",
            "1250 [D loss: 0.597499, acc: 65.62%] [G loss: 2.603158]\n",
            "1251 [D loss: 0.596305, acc: 60.94%] [G loss: 2.786889]\n",
            "1252 [D loss: 0.664312, acc: 60.94%] [G loss: 2.448849]\n",
            "1253 [D loss: 0.508294, acc: 76.56%] [G loss: 2.934975]\n",
            "1254 [D loss: 0.618336, acc: 70.31%] [G loss: 2.565436]\n",
            "1255 [D loss: 0.680345, acc: 51.56%] [G loss: 2.384617]\n",
            "1256 [D loss: 0.590261, acc: 65.62%] [G loss: 2.407934]\n",
            "1257 [D loss: 0.652681, acc: 62.50%] [G loss: 2.271324]\n",
            "1258 [D loss: 0.660326, acc: 65.62%] [G loss: 2.296375]\n",
            "1259 [D loss: 0.569230, acc: 67.19%] [G loss: 2.255587]\n",
            "1260 [D loss: 0.596615, acc: 71.88%] [G loss: 2.731612]\n",
            "1261 [D loss: 0.576801, acc: 70.31%] [G loss: 2.642754]\n",
            "1262 [D loss: 0.743676, acc: 57.81%] [G loss: 2.128528]\n",
            "1263 [D loss: 0.593043, acc: 70.31%] [G loss: 2.491077]\n",
            "1264 [D loss: 0.592262, acc: 65.62%] [G loss: 2.532326]\n",
            "1265 [D loss: 0.591235, acc: 65.62%] [G loss: 2.422159]\n",
            "1266 [D loss: 0.689817, acc: 59.38%] [G loss: 2.685611]\n",
            "1267 [D loss: 0.545864, acc: 73.44%] [G loss: 2.608980]\n",
            "1268 [D loss: 0.745120, acc: 59.38%] [G loss: 2.433552]\n",
            "1269 [D loss: 0.514867, acc: 76.56%] [G loss: 2.463404]\n",
            "1270 [D loss: 0.619627, acc: 65.62%] [G loss: 2.493275]\n",
            "1271 [D loss: 0.652841, acc: 65.62%] [G loss: 2.364221]\n",
            "1272 [D loss: 0.549783, acc: 71.88%] [G loss: 2.162015]\n",
            "1273 [D loss: 0.626088, acc: 67.19%] [G loss: 2.581833]\n",
            "1274 [D loss: 0.601678, acc: 67.19%] [G loss: 2.192293]\n",
            "1275 [D loss: 0.604374, acc: 67.19%] [G loss: 2.256510]\n",
            "1276 [D loss: 0.720652, acc: 56.25%] [G loss: 2.339635]\n",
            "1277 [D loss: 0.631235, acc: 68.75%] [G loss: 2.178851]\n",
            "1278 [D loss: 0.622750, acc: 70.31%] [G loss: 2.411118]\n",
            "1279 [D loss: 0.576366, acc: 65.62%] [G loss: 2.514406]\n",
            "1280 [D loss: 0.617624, acc: 67.19%] [G loss: 2.264504]\n",
            "1281 [D loss: 0.587622, acc: 71.88%] [G loss: 2.612537]\n",
            "1282 [D loss: 0.590372, acc: 64.06%] [G loss: 2.291906]\n",
            "1283 [D loss: 0.621711, acc: 64.06%] [G loss: 2.682707]\n",
            "1284 [D loss: 0.719651, acc: 64.06%] [G loss: 2.506337]\n",
            "1285 [D loss: 0.644748, acc: 54.69%] [G loss: 2.201926]\n",
            "1286 [D loss: 0.626342, acc: 67.19%] [G loss: 2.778528]\n",
            "1287 [D loss: 0.684244, acc: 57.81%] [G loss: 2.278736]\n",
            "1288 [D loss: 0.565940, acc: 73.44%] [G loss: 2.384767]\n",
            "1289 [D loss: 0.632695, acc: 62.50%] [G loss: 2.693917]\n",
            "1290 [D loss: 0.532919, acc: 73.44%] [G loss: 2.454105]\n",
            "1291 [D loss: 0.516303, acc: 68.75%] [G loss: 2.431703]\n",
            "1292 [D loss: 0.632537, acc: 64.06%] [G loss: 2.377461]\n",
            "1293 [D loss: 0.669396, acc: 62.50%] [G loss: 2.172243]\n",
            "1294 [D loss: 0.604260, acc: 67.19%] [G loss: 2.512422]\n",
            "1295 [D loss: 0.657556, acc: 62.50%] [G loss: 2.597065]\n",
            "1296 [D loss: 0.498301, acc: 79.69%] [G loss: 2.832833]\n",
            "1297 [D loss: 0.524131, acc: 73.44%] [G loss: 2.309227]\n",
            "1298 [D loss: 0.568801, acc: 70.31%] [G loss: 2.949715]\n",
            "1299 [D loss: 0.676688, acc: 65.62%] [G loss: 2.029261]\n",
            "1300 [D loss: 0.607586, acc: 62.50%] [G loss: 2.517904]\n",
            "1301 [D loss: 0.561285, acc: 65.62%] [G loss: 2.668991]\n",
            "1302 [D loss: 0.668247, acc: 68.75%] [G loss: 2.553164]\n",
            "1303 [D loss: 0.630340, acc: 57.81%] [G loss: 2.449702]\n",
            "1304 [D loss: 0.639308, acc: 65.62%] [G loss: 2.726245]\n",
            "1305 [D loss: 0.619699, acc: 68.75%] [G loss: 2.521382]\n",
            "1306 [D loss: 0.636412, acc: 65.62%] [G loss: 2.163620]\n",
            "1307 [D loss: 0.509225, acc: 79.69%] [G loss: 2.738154]\n",
            "1308 [D loss: 0.483712, acc: 84.38%] [G loss: 2.765839]\n",
            "1309 [D loss: 0.606274, acc: 64.06%] [G loss: 2.612881]\n",
            "1310 [D loss: 0.525806, acc: 73.44%] [G loss: 2.728609]\n",
            "1311 [D loss: 0.628269, acc: 70.31%] [G loss: 2.295437]\n",
            "1312 [D loss: 0.705055, acc: 65.62%] [G loss: 2.435123]\n",
            "1313 [D loss: 0.591442, acc: 67.19%] [G loss: 2.614075]\n",
            "1314 [D loss: 0.705434, acc: 57.81%] [G loss: 2.614064]\n",
            "1315 [D loss: 0.758745, acc: 50.00%] [G loss: 2.260784]\n",
            "1316 [D loss: 0.579816, acc: 68.75%] [G loss: 2.682441]\n",
            "1317 [D loss: 0.497054, acc: 79.69%] [G loss: 2.206673]\n",
            "1318 [D loss: 0.626061, acc: 64.06%] [G loss: 2.648800]\n",
            "1319 [D loss: 0.592168, acc: 68.75%] [G loss: 2.292172]\n",
            "1320 [D loss: 0.597749, acc: 59.38%] [G loss: 2.773982]\n",
            "1321 [D loss: 0.596520, acc: 57.81%] [G loss: 2.450663]\n",
            "1322 [D loss: 0.549748, acc: 67.19%] [G loss: 2.670770]\n",
            "1323 [D loss: 0.569182, acc: 68.75%] [G loss: 2.356936]\n",
            "1324 [D loss: 0.574907, acc: 68.75%] [G loss: 2.844046]\n",
            "1325 [D loss: 0.676120, acc: 56.25%] [G loss: 2.554833]\n",
            "1326 [D loss: 0.541077, acc: 75.00%] [G loss: 2.488865]\n",
            "1327 [D loss: 0.642225, acc: 65.62%] [G loss: 2.516213]\n",
            "1328 [D loss: 0.700682, acc: 57.81%] [G loss: 2.382834]\n",
            "1329 [D loss: 0.656934, acc: 65.62%] [G loss: 2.057184]\n",
            "1330 [D loss: 0.584066, acc: 67.19%] [G loss: 2.527826]\n",
            "1331 [D loss: 0.539000, acc: 75.00%] [G loss: 2.553478]\n",
            "1332 [D loss: 0.645017, acc: 57.81%] [G loss: 2.151271]\n",
            "1333 [D loss: 0.717837, acc: 56.25%] [G loss: 2.308917]\n",
            "1334 [D loss: 0.515491, acc: 73.44%] [G loss: 2.746828]\n",
            "1335 [D loss: 0.625975, acc: 64.06%] [G loss: 2.719762]\n",
            "1336 [D loss: 0.722609, acc: 53.12%] [G loss: 2.428356]\n",
            "1337 [D loss: 0.597642, acc: 71.88%] [G loss: 2.044646]\n",
            "1338 [D loss: 0.572510, acc: 65.62%] [G loss: 2.273615]\n",
            "1339 [D loss: 0.704870, acc: 59.38%] [G loss: 2.909502]\n",
            "1340 [D loss: 0.648430, acc: 64.06%] [G loss: 2.442819]\n",
            "1341 [D loss: 0.633548, acc: 67.19%] [G loss: 2.326104]\n",
            "1342 [D loss: 0.571608, acc: 71.88%] [G loss: 2.599542]\n",
            "1343 [D loss: 0.542663, acc: 78.12%] [G loss: 2.426279]\n",
            "1344 [D loss: 0.622829, acc: 65.62%] [G loss: 2.496325]\n",
            "1345 [D loss: 0.693518, acc: 60.94%] [G loss: 2.526049]\n",
            "1346 [D loss: 0.652000, acc: 68.75%] [G loss: 2.657401]\n",
            "1347 [D loss: 0.650020, acc: 62.50%] [G loss: 2.442488]\n",
            "1348 [D loss: 0.522290, acc: 76.56%] [G loss: 2.874060]\n",
            "1349 [D loss: 0.579743, acc: 67.19%] [G loss: 2.498022]\n",
            "1350 [D loss: 0.646169, acc: 65.62%] [G loss: 2.673536]\n",
            "1351 [D loss: 0.692424, acc: 64.06%] [G loss: 2.495178]\n",
            "1352 [D loss: 0.617577, acc: 67.19%] [G loss: 2.404090]\n",
            "1353 [D loss: 0.568388, acc: 67.19%] [G loss: 2.422364]\n",
            "1354 [D loss: 0.564986, acc: 73.44%] [G loss: 2.201231]\n",
            "1355 [D loss: 0.615192, acc: 64.06%] [G loss: 2.572898]\n",
            "1356 [D loss: 0.629575, acc: 70.31%] [G loss: 2.349911]\n",
            "1357 [D loss: 0.534971, acc: 75.00%] [G loss: 2.941192]\n",
            "1358 [D loss: 0.627303, acc: 64.06%] [G loss: 2.543741]\n",
            "1359 [D loss: 0.592278, acc: 67.19%] [G loss: 2.464631]\n",
            "1360 [D loss: 0.619568, acc: 68.75%] [G loss: 2.878680]\n",
            "1361 [D loss: 0.608124, acc: 62.50%] [G loss: 2.587023]\n",
            "1362 [D loss: 0.589331, acc: 70.31%] [G loss: 2.291192]\n",
            "1363 [D loss: 0.622083, acc: 62.50%] [G loss: 2.291138]\n",
            "1364 [D loss: 0.553370, acc: 65.62%] [G loss: 2.395904]\n",
            "1365 [D loss: 0.599768, acc: 67.19%] [G loss: 2.283044]\n",
            "1366 [D loss: 0.646447, acc: 65.62%] [G loss: 2.466827]\n",
            "1367 [D loss: 0.694212, acc: 59.38%] [G loss: 2.694407]\n",
            "1368 [D loss: 0.625684, acc: 67.19%] [G loss: 2.309683]\n",
            "1369 [D loss: 0.723209, acc: 59.38%] [G loss: 2.410423]\n",
            "1370 [D loss: 0.676216, acc: 60.94%] [G loss: 2.229455]\n",
            "1371 [D loss: 0.749027, acc: 51.56%] [G loss: 2.031953]\n",
            "1372 [D loss: 0.682190, acc: 60.94%] [G loss: 2.239639]\n",
            "1373 [D loss: 0.524160, acc: 68.75%] [G loss: 2.773845]\n",
            "1374 [D loss: 0.520932, acc: 75.00%] [G loss: 2.421067]\n",
            "1375 [D loss: 0.641342, acc: 62.50%] [G loss: 2.468749]\n",
            "1376 [D loss: 0.596444, acc: 67.19%] [G loss: 2.297992]\n",
            "1377 [D loss: 0.597791, acc: 64.06%] [G loss: 2.289314]\n",
            "1378 [D loss: 0.588599, acc: 68.75%] [G loss: 2.811906]\n",
            "1379 [D loss: 0.587918, acc: 71.88%] [G loss: 2.300636]\n",
            "1380 [D loss: 0.745685, acc: 56.25%] [G loss: 2.153534]\n",
            "1381 [D loss: 0.577613, acc: 73.44%] [G loss: 2.316737]\n",
            "1382 [D loss: 0.563376, acc: 68.75%] [G loss: 2.574499]\n",
            "1383 [D loss: 0.628481, acc: 64.06%] [G loss: 2.482050]\n",
            "1384 [D loss: 0.640232, acc: 59.38%] [G loss: 2.530131]\n",
            "1385 [D loss: 0.565302, acc: 73.44%] [G loss: 2.502151]\n",
            "1386 [D loss: 0.564158, acc: 70.31%] [G loss: 2.304380]\n",
            "1387 [D loss: 0.543589, acc: 75.00%] [G loss: 2.787056]\n",
            "1388 [D loss: 0.568834, acc: 73.44%] [G loss: 2.627463]\n",
            "1389 [D loss: 0.672252, acc: 64.06%] [G loss: 2.447667]\n",
            "1390 [D loss: 0.684748, acc: 57.81%] [G loss: 2.318260]\n",
            "1391 [D loss: 0.549123, acc: 75.00%] [G loss: 2.684446]\n",
            "1392 [D loss: 0.576472, acc: 73.44%] [G loss: 2.395890]\n",
            "1393 [D loss: 0.677813, acc: 59.38%] [G loss: 2.390339]\n",
            "1394 [D loss: 0.638918, acc: 59.38%] [G loss: 2.462302]\n",
            "1395 [D loss: 0.611941, acc: 65.62%] [G loss: 2.693612]\n",
            "1396 [D loss: 0.588000, acc: 68.75%] [G loss: 2.284963]\n",
            "1397 [D loss: 0.596555, acc: 65.62%] [G loss: 2.640512]\n",
            "1398 [D loss: 0.756307, acc: 50.00%] [G loss: 2.262245]\n",
            "1399 [D loss: 0.608052, acc: 67.19%] [G loss: 2.531442]\n",
            "1400 [D loss: 0.603466, acc: 73.44%] [G loss: 2.508026]\n",
            "1401 [D loss: 0.675301, acc: 62.50%] [G loss: 2.699406]\n",
            "1402 [D loss: 0.501971, acc: 73.44%] [G loss: 2.498524]\n",
            "1403 [D loss: 0.684090, acc: 60.94%] [G loss: 2.529521]\n",
            "1404 [D loss: 0.689174, acc: 56.25%] [G loss: 2.496906]\n",
            "1405 [D loss: 0.631893, acc: 67.19%] [G loss: 2.375664]\n",
            "1406 [D loss: 0.557962, acc: 71.88%] [G loss: 2.658214]\n",
            "1407 [D loss: 0.696858, acc: 54.69%] [G loss: 2.568238]\n",
            "1408 [D loss: 0.583346, acc: 64.06%] [G loss: 2.313847]\n",
            "1409 [D loss: 0.645035, acc: 57.81%] [G loss: 2.240274]\n",
            "1410 [D loss: 0.620929, acc: 65.62%] [G loss: 2.433024]\n",
            "1411 [D loss: 0.560912, acc: 68.75%] [G loss: 2.559132]\n",
            "1412 [D loss: 0.563571, acc: 76.56%] [G loss: 2.447313]\n",
            "1413 [D loss: 0.773079, acc: 57.81%] [G loss: 2.291090]\n",
            "1414 [D loss: 0.706155, acc: 56.25%] [G loss: 2.405494]\n",
            "1415 [D loss: 0.598121, acc: 67.19%] [G loss: 2.310870]\n",
            "1416 [D loss: 0.556147, acc: 68.75%] [G loss: 2.303803]\n",
            "1417 [D loss: 0.625465, acc: 62.50%] [G loss: 2.516493]\n",
            "1418 [D loss: 0.619143, acc: 60.94%] [G loss: 2.624238]\n",
            "1419 [D loss: 0.558676, acc: 68.75%] [G loss: 2.448374]\n",
            "1420 [D loss: 0.637823, acc: 57.81%] [G loss: 2.390457]\n",
            "1421 [D loss: 0.540804, acc: 81.25%] [G loss: 2.285404]\n",
            "1422 [D loss: 0.614229, acc: 68.75%] [G loss: 2.501993]\n",
            "1423 [D loss: 0.583354, acc: 68.75%] [G loss: 2.754823]\n",
            "1424 [D loss: 0.427822, acc: 85.94%] [G loss: 2.430982]\n",
            "1425 [D loss: 0.540186, acc: 73.44%] [G loss: 2.344713]\n",
            "1426 [D loss: 0.629345, acc: 71.88%] [G loss: 2.341676]\n",
            "1427 [D loss: 0.593485, acc: 62.50%] [G loss: 2.499250]\n",
            "1428 [D loss: 0.629737, acc: 62.50%] [G loss: 2.454156]\n",
            "1429 [D loss: 0.574424, acc: 71.88%] [G loss: 2.796840]\n",
            "1430 [D loss: 0.701519, acc: 53.12%] [G loss: 2.329230]\n",
            "1431 [D loss: 0.515990, acc: 71.88%] [G loss: 2.323279]\n",
            "1432 [D loss: 0.617650, acc: 65.62%] [G loss: 2.496869]\n",
            "1433 [D loss: 0.587560, acc: 75.00%] [G loss: 2.333902]\n",
            "1434 [D loss: 0.694156, acc: 56.25%] [G loss: 2.518721]\n",
            "1435 [D loss: 0.662080, acc: 59.38%] [G loss: 2.251957]\n",
            "1436 [D loss: 0.667973, acc: 65.62%] [G loss: 2.650508]\n",
            "1437 [D loss: 0.520215, acc: 79.69%] [G loss: 2.540738]\n",
            "1438 [D loss: 0.665296, acc: 59.38%] [G loss: 2.317441]\n",
            "1439 [D loss: 0.629670, acc: 65.62%] [G loss: 2.530288]\n",
            "1440 [D loss: 0.604202, acc: 70.31%] [G loss: 2.571037]\n",
            "1441 [D loss: 0.481456, acc: 78.12%] [G loss: 2.676753]\n",
            "1442 [D loss: 0.624214, acc: 67.19%] [G loss: 2.461154]\n",
            "1443 [D loss: 0.629246, acc: 71.88%] [G loss: 2.607696]\n",
            "1444 [D loss: 0.525323, acc: 70.31%] [G loss: 2.759747]\n",
            "1445 [D loss: 0.559765, acc: 68.75%] [G loss: 2.696322]\n",
            "1446 [D loss: 0.534530, acc: 73.44%] [G loss: 2.852673]\n",
            "1447 [D loss: 0.579871, acc: 65.62%] [G loss: 2.370807]\n",
            "1448 [D loss: 0.541476, acc: 73.44%] [G loss: 2.397561]\n",
            "1449 [D loss: 0.670115, acc: 65.62%] [G loss: 2.625487]\n",
            "1450 [D loss: 0.753997, acc: 56.25%] [G loss: 2.488966]\n",
            "1451 [D loss: 0.531671, acc: 79.69%] [G loss: 2.392132]\n",
            "1452 [D loss: 0.590747, acc: 65.62%] [G loss: 2.595642]\n",
            "1453 [D loss: 0.654861, acc: 60.94%] [G loss: 2.589419]\n",
            "1454 [D loss: 0.555644, acc: 68.75%] [G loss: 2.250705]\n",
            "1455 [D loss: 0.616289, acc: 70.31%] [G loss: 2.573437]\n",
            "1456 [D loss: 0.541779, acc: 73.44%] [G loss: 2.781721]\n",
            "1457 [D loss: 0.526823, acc: 70.31%] [G loss: 2.218315]\n",
            "1458 [D loss: 0.602224, acc: 73.44%] [G loss: 2.634492]\n",
            "1459 [D loss: 0.537248, acc: 76.56%] [G loss: 2.687307]\n",
            "1460 [D loss: 0.662736, acc: 65.62%] [G loss: 2.613495]\n",
            "1461 [D loss: 0.691087, acc: 59.38%] [G loss: 2.592046]\n",
            "1462 [D loss: 0.592170, acc: 65.62%] [G loss: 2.667480]\n",
            "1463 [D loss: 0.683870, acc: 62.50%] [G loss: 2.327464]\n",
            "1464 [D loss: 0.620596, acc: 64.06%] [G loss: 2.759426]\n",
            "1465 [D loss: 0.587661, acc: 68.75%] [G loss: 2.234371]\n",
            "1466 [D loss: 0.627523, acc: 64.06%] [G loss: 2.594361]\n",
            "1467 [D loss: 0.591116, acc: 71.88%] [G loss: 2.553661]\n",
            "1468 [D loss: 0.616172, acc: 65.62%] [G loss: 2.235524]\n",
            "1469 [D loss: 0.505385, acc: 78.12%] [G loss: 2.406611]\n",
            "1470 [D loss: 0.540044, acc: 76.56%] [G loss: 2.815582]\n",
            "1471 [D loss: 0.617107, acc: 56.25%] [G loss: 2.916617]\n",
            "1472 [D loss: 0.630103, acc: 57.81%] [G loss: 2.725057]\n",
            "1473 [D loss: 0.621378, acc: 64.06%] [G loss: 2.369909]\n",
            "1474 [D loss: 0.732657, acc: 51.56%] [G loss: 2.453183]\n",
            "1475 [D loss: 0.594934, acc: 65.62%] [G loss: 2.491513]\n",
            "1476 [D loss: 0.591274, acc: 73.44%] [G loss: 2.653172]\n",
            "1477 [D loss: 0.514757, acc: 71.88%] [G loss: 2.456858]\n",
            "1478 [D loss: 0.614186, acc: 62.50%] [G loss: 2.616665]\n",
            "1479 [D loss: 0.603467, acc: 67.19%] [G loss: 2.854899]\n",
            "1480 [D loss: 0.606348, acc: 67.19%] [G loss: 2.799241]\n",
            "1481 [D loss: 0.517195, acc: 73.44%] [G loss: 3.199845]\n",
            "1482 [D loss: 0.698380, acc: 60.94%] [G loss: 2.641522]\n",
            "1483 [D loss: 0.637420, acc: 67.19%] [G loss: 2.795897]\n",
            "1484 [D loss: 0.516786, acc: 73.44%] [G loss: 2.688289]\n",
            "1485 [D loss: 0.595167, acc: 67.19%] [G loss: 2.449752]\n",
            "1486 [D loss: 0.674010, acc: 60.94%] [G loss: 2.513932]\n",
            "1487 [D loss: 0.523614, acc: 70.31%] [G loss: 2.491698]\n",
            "1488 [D loss: 0.591448, acc: 67.19%] [G loss: 2.350647]\n",
            "1489 [D loss: 0.557439, acc: 75.00%] [G loss: 2.396480]\n",
            "1490 [D loss: 0.505574, acc: 78.12%] [G loss: 2.809111]\n",
            "1491 [D loss: 0.567190, acc: 68.75%] [G loss: 3.140118]\n",
            "1492 [D loss: 0.597330, acc: 67.19%] [G loss: 2.471622]\n",
            "1493 [D loss: 0.631954, acc: 64.06%] [G loss: 2.317038]\n",
            "1494 [D loss: 0.576439, acc: 68.75%] [G loss: 3.023673]\n",
            "1495 [D loss: 0.721737, acc: 60.94%] [G loss: 2.172465]\n",
            "1496 [D loss: 0.498699, acc: 79.69%] [G loss: 2.622280]\n",
            "1497 [D loss: 0.589618, acc: 70.31%] [G loss: 2.709041]\n",
            "1498 [D loss: 0.683584, acc: 50.00%] [G loss: 2.474544]\n",
            "1499 [D loss: 0.604109, acc: 65.62%] [G loss: 2.639434]\n",
            "1500 [D loss: 0.693576, acc: 51.56%] [G loss: 2.539967]\n",
            "1501 [D loss: 0.479269, acc: 81.25%] [G loss: 2.994213]\n",
            "1502 [D loss: 0.662852, acc: 53.12%] [G loss: 2.715773]\n",
            "1503 [D loss: 0.633869, acc: 64.06%] [G loss: 2.568012]\n",
            "1504 [D loss: 0.587178, acc: 64.06%] [G loss: 2.426105]\n",
            "1505 [D loss: 0.649678, acc: 64.06%] [G loss: 2.371152]\n",
            "1506 [D loss: 0.589694, acc: 64.06%] [G loss: 2.423773]\n",
            "1507 [D loss: 0.574901, acc: 73.44%] [G loss: 2.239426]\n",
            "1508 [D loss: 0.689240, acc: 60.94%] [G loss: 2.929364]\n",
            "1509 [D loss: 0.519368, acc: 75.00%] [G loss: 3.159289]\n",
            "1510 [D loss: 0.656640, acc: 62.50%] [G loss: 2.407009]\n",
            "1511 [D loss: 0.593343, acc: 75.00%] [G loss: 2.628212]\n",
            "1512 [D loss: 0.538897, acc: 70.31%] [G loss: 2.760002]\n",
            "1513 [D loss: 0.697708, acc: 54.69%] [G loss: 2.544292]\n",
            "1514 [D loss: 0.663122, acc: 57.81%] [G loss: 2.889918]\n",
            "1515 [D loss: 0.551769, acc: 75.00%] [G loss: 2.610268]\n",
            "1516 [D loss: 0.508753, acc: 76.56%] [G loss: 2.998249]\n",
            "1517 [D loss: 0.579184, acc: 70.31%] [G loss: 2.501165]\n",
            "1518 [D loss: 0.540199, acc: 68.75%] [G loss: 2.647780]\n",
            "1519 [D loss: 0.475463, acc: 76.56%] [G loss: 2.957675]\n",
            "1520 [D loss: 0.465558, acc: 81.25%] [G loss: 2.773330]\n",
            "1521 [D loss: 0.522508, acc: 68.75%] [G loss: 2.602588]\n",
            "1522 [D loss: 0.601666, acc: 67.19%] [G loss: 2.975898]\n",
            "1523 [D loss: 0.461600, acc: 78.12%] [G loss: 3.069539]\n",
            "1524 [D loss: 0.612574, acc: 64.06%] [G loss: 2.668781]\n",
            "1525 [D loss: 0.635491, acc: 56.25%] [G loss: 2.774122]\n",
            "1526 [D loss: 0.561547, acc: 71.88%] [G loss: 2.586766]\n",
            "1527 [D loss: 0.641023, acc: 65.62%] [G loss: 2.530481]\n",
            "1528 [D loss: 0.550216, acc: 65.62%] [G loss: 2.755064]\n",
            "1529 [D loss: 0.633000, acc: 65.62%] [G loss: 2.716895]\n",
            "1530 [D loss: 0.648882, acc: 57.81%] [G loss: 2.388409]\n",
            "1531 [D loss: 0.722150, acc: 57.81%] [G loss: 2.532900]\n",
            "1532 [D loss: 0.566355, acc: 67.19%] [G loss: 2.829126]\n",
            "1533 [D loss: 0.652075, acc: 67.19%] [G loss: 2.368423]\n",
            "1534 [D loss: 0.699206, acc: 59.38%] [G loss: 2.765294]\n",
            "1535 [D loss: 0.560258, acc: 73.44%] [G loss: 2.795110]\n",
            "1536 [D loss: 0.594237, acc: 70.31%] [G loss: 2.622915]\n",
            "1537 [D loss: 0.666906, acc: 59.38%] [G loss: 2.602627]\n",
            "1538 [D loss: 0.669413, acc: 65.62%] [G loss: 2.358442]\n",
            "1539 [D loss: 0.644594, acc: 57.81%] [G loss: 2.460957]\n",
            "1540 [D loss: 0.507585, acc: 81.25%] [G loss: 2.640794]\n",
            "1541 [D loss: 0.630722, acc: 67.19%] [G loss: 2.553244]\n",
            "1542 [D loss: 0.604437, acc: 71.88%] [G loss: 2.630263]\n",
            "1543 [D loss: 0.597758, acc: 64.06%] [G loss: 2.877532]\n",
            "1544 [D loss: 0.779013, acc: 53.12%] [G loss: 2.382067]\n",
            "1545 [D loss: 0.521144, acc: 70.31%] [G loss: 2.169474]\n",
            "1546 [D loss: 0.827084, acc: 42.19%] [G loss: 2.588585]\n",
            "1547 [D loss: 0.547205, acc: 70.31%] [G loss: 2.790175]\n",
            "1548 [D loss: 0.513853, acc: 73.44%] [G loss: 2.677984]\n",
            "1549 [D loss: 0.671855, acc: 56.25%] [G loss: 2.438030]\n",
            "1550 [D loss: 0.577618, acc: 68.75%] [G loss: 2.244331]\n",
            "1551 [D loss: 0.517320, acc: 76.56%] [G loss: 2.222196]\n",
            "1552 [D loss: 0.553411, acc: 76.56%] [G loss: 2.465903]\n",
            "1553 [D loss: 0.739885, acc: 54.69%] [G loss: 2.588971]\n",
            "1554 [D loss: 0.586390, acc: 70.31%] [G loss: 2.583109]\n",
            "1555 [D loss: 0.604680, acc: 70.31%] [G loss: 2.520021]\n",
            "1556 [D loss: 0.520376, acc: 75.00%] [G loss: 2.373456]\n",
            "1557 [D loss: 0.614613, acc: 67.19%] [G loss: 2.634982]\n",
            "1558 [D loss: 0.666856, acc: 67.19%] [G loss: 2.583634]\n",
            "1559 [D loss: 0.655255, acc: 65.62%] [G loss: 2.533873]\n",
            "1560 [D loss: 0.526233, acc: 75.00%] [G loss: 2.412094]\n",
            "1561 [D loss: 0.676856, acc: 67.19%] [G loss: 2.644069]\n",
            "1562 [D loss: 0.773490, acc: 59.38%] [G loss: 2.294907]\n",
            "1563 [D loss: 0.633222, acc: 71.88%] [G loss: 2.387970]\n",
            "1564 [D loss: 0.632289, acc: 67.19%] [G loss: 2.488186]\n",
            "1565 [D loss: 0.502934, acc: 73.44%] [G loss: 2.450643]\n",
            "1566 [D loss: 0.667064, acc: 60.94%] [G loss: 2.328262]\n",
            "1567 [D loss: 0.515479, acc: 76.56%] [G loss: 2.328999]\n",
            "1568 [D loss: 0.632551, acc: 70.31%] [G loss: 2.403553]\n",
            "1569 [D loss: 0.460717, acc: 78.12%] [G loss: 2.593872]\n",
            "1570 [D loss: 0.643699, acc: 67.19%] [G loss: 2.807808]\n",
            "1571 [D loss: 0.604327, acc: 59.38%] [G loss: 2.582804]\n",
            "1572 [D loss: 0.608791, acc: 70.31%] [G loss: 2.242393]\n",
            "1573 [D loss: 0.615023, acc: 68.75%] [G loss: 2.668984]\n",
            "1574 [D loss: 0.441171, acc: 84.38%] [G loss: 2.509460]\n",
            "1575 [D loss: 0.623118, acc: 62.50%] [G loss: 2.680082]\n",
            "1576 [D loss: 0.496946, acc: 75.00%] [G loss: 2.871036]\n",
            "1577 [D loss: 0.542454, acc: 76.56%] [G loss: 2.426384]\n",
            "1578 [D loss: 0.757257, acc: 53.12%] [G loss: 2.466212]\n",
            "1579 [D loss: 0.658814, acc: 64.06%] [G loss: 2.314995]\n",
            "1580 [D loss: 0.584299, acc: 68.75%] [G loss: 2.774776]\n",
            "1581 [D loss: 0.637584, acc: 60.94%] [G loss: 2.498614]\n",
            "1582 [D loss: 0.624414, acc: 67.19%] [G loss: 2.627785]\n",
            "1583 [D loss: 0.697755, acc: 57.81%] [G loss: 2.326519]\n",
            "1584 [D loss: 0.540742, acc: 75.00%] [G loss: 2.539505]\n",
            "1585 [D loss: 0.667207, acc: 62.50%] [G loss: 2.216554]\n",
            "1586 [D loss: 0.614812, acc: 65.62%] [G loss: 2.391845]\n",
            "1587 [D loss: 0.604187, acc: 68.75%] [G loss: 2.698383]\n",
            "1588 [D loss: 0.575101, acc: 67.19%] [G loss: 2.561915]\n",
            "1589 [D loss: 0.547076, acc: 65.62%] [G loss: 2.444597]\n",
            "1590 [D loss: 0.704165, acc: 60.94%] [G loss: 2.339032]\n",
            "1591 [D loss: 0.493480, acc: 84.38%] [G loss: 2.568054]\n",
            "1592 [D loss: 0.672283, acc: 62.50%] [G loss: 2.564675]\n",
            "1593 [D loss: 0.528079, acc: 68.75%] [G loss: 2.627305]\n",
            "1594 [D loss: 0.570702, acc: 71.88%] [G loss: 2.688839]\n",
            "1595 [D loss: 0.574020, acc: 68.75%] [G loss: 2.447076]\n",
            "1596 [D loss: 0.746314, acc: 57.81%] [G loss: 2.180392]\n",
            "1597 [D loss: 0.611086, acc: 70.31%] [G loss: 2.580065]\n",
            "1598 [D loss: 0.649044, acc: 60.94%] [G loss: 2.373196]\n",
            "1599 [D loss: 0.679413, acc: 67.19%] [G loss: 2.297685]\n",
            "1600 [D loss: 0.656688, acc: 59.38%] [G loss: 2.792543]\n",
            "1601 [D loss: 0.604076, acc: 68.75%] [G loss: 2.656898]\n",
            "1602 [D loss: 0.636092, acc: 64.06%] [G loss: 2.724821]\n",
            "1603 [D loss: 0.641981, acc: 57.81%] [G loss: 2.771154]\n",
            "1604 [D loss: 0.684775, acc: 59.38%] [G loss: 2.219938]\n",
            "1605 [D loss: 0.588147, acc: 70.31%] [G loss: 2.766153]\n",
            "1606 [D loss: 0.670995, acc: 70.31%] [G loss: 2.215486]\n",
            "1607 [D loss: 0.631476, acc: 65.62%] [G loss: 2.200208]\n",
            "1608 [D loss: 0.680408, acc: 59.38%] [G loss: 2.328990]\n",
            "1609 [D loss: 0.672331, acc: 65.62%] [G loss: 2.079035]\n",
            "1610 [D loss: 0.519947, acc: 65.62%] [G loss: 2.245676]\n",
            "1611 [D loss: 0.545276, acc: 76.56%] [G loss: 3.049015]\n",
            "1612 [D loss: 0.681886, acc: 56.25%] [G loss: 2.673400]\n",
            "1613 [D loss: 0.531413, acc: 75.00%] [G loss: 2.286479]\n",
            "1614 [D loss: 0.572752, acc: 70.31%] [G loss: 2.493356]\n",
            "1615 [D loss: 0.699693, acc: 54.69%] [G loss: 2.594898]\n",
            "1616 [D loss: 0.632152, acc: 68.75%] [G loss: 2.577553]\n",
            "1617 [D loss: 0.609745, acc: 65.62%] [G loss: 2.601063]\n",
            "1618 [D loss: 0.632771, acc: 68.75%] [G loss: 2.453703]\n",
            "1619 [D loss: 0.510398, acc: 68.75%] [G loss: 2.521751]\n",
            "1620 [D loss: 0.673032, acc: 60.94%] [G loss: 2.495111]\n",
            "1621 [D loss: 0.593695, acc: 68.75%] [G loss: 2.451278]\n",
            "1622 [D loss: 0.614400, acc: 71.88%] [G loss: 2.525582]\n",
            "1623 [D loss: 0.660118, acc: 62.50%] [G loss: 2.318362]\n",
            "1624 [D loss: 0.623714, acc: 65.62%] [G loss: 2.382458]\n",
            "1625 [D loss: 0.594939, acc: 68.75%] [G loss: 2.252194]\n",
            "1626 [D loss: 0.660908, acc: 53.12%] [G loss: 2.526948]\n",
            "1627 [D loss: 0.613083, acc: 62.50%] [G loss: 2.256817]\n",
            "1628 [D loss: 0.494106, acc: 78.12%] [G loss: 2.976436]\n",
            "1629 [D loss: 0.735796, acc: 51.56%] [G loss: 2.399044]\n",
            "1630 [D loss: 0.731805, acc: 56.25%] [G loss: 2.390420]\n",
            "1631 [D loss: 0.588807, acc: 65.62%] [G loss: 2.756002]\n",
            "1632 [D loss: 0.674765, acc: 62.50%] [G loss: 2.406350]\n",
            "1633 [D loss: 0.591287, acc: 68.75%] [G loss: 2.234572]\n",
            "1634 [D loss: 0.574372, acc: 68.75%] [G loss: 2.601484]\n",
            "1635 [D loss: 0.595219, acc: 70.31%] [G loss: 2.406009]\n",
            "1636 [D loss: 0.612805, acc: 65.62%] [G loss: 2.479160]\n",
            "1637 [D loss: 0.560236, acc: 76.56%] [G loss: 2.566615]\n",
            "1638 [D loss: 0.588880, acc: 71.88%] [G loss: 2.244482]\n",
            "1639 [D loss: 0.615983, acc: 60.94%] [G loss: 2.451893]\n",
            "1640 [D loss: 0.709217, acc: 64.06%] [G loss: 2.365935]\n",
            "1641 [D loss: 0.523684, acc: 70.31%] [G loss: 2.655875]\n",
            "1642 [D loss: 0.561617, acc: 76.56%] [G loss: 2.766120]\n",
            "1643 [D loss: 0.557016, acc: 64.06%] [G loss: 2.364559]\n",
            "1644 [D loss: 0.582703, acc: 68.75%] [G loss: 2.366047]\n",
            "1645 [D loss: 0.661896, acc: 62.50%] [G loss: 2.332137]\n",
            "1646 [D loss: 0.589672, acc: 67.19%] [G loss: 2.430712]\n",
            "1647 [D loss: 0.532616, acc: 73.44%] [G loss: 2.813139]\n",
            "1648 [D loss: 0.625288, acc: 65.62%] [G loss: 2.369556]\n",
            "1649 [D loss: 0.606721, acc: 59.38%] [G loss: 2.438344]\n",
            "1650 [D loss: 0.601258, acc: 60.94%] [G loss: 2.400156]\n",
            "1651 [D loss: 0.649989, acc: 57.81%] [G loss: 2.009835]\n",
            "1652 [D loss: 0.560420, acc: 70.31%] [G loss: 2.657382]\n",
            "1653 [D loss: 0.612242, acc: 62.50%] [G loss: 2.483718]\n",
            "1654 [D loss: 0.580785, acc: 64.06%] [G loss: 2.637811]\n",
            "1655 [D loss: 0.570356, acc: 70.31%] [G loss: 2.580652]\n",
            "1656 [D loss: 0.578830, acc: 67.19%] [G loss: 2.901902]\n",
            "1657 [D loss: 0.612112, acc: 67.19%] [G loss: 2.636717]\n",
            "1658 [D loss: 0.630320, acc: 64.06%] [G loss: 2.483569]\n",
            "1659 [D loss: 0.797075, acc: 54.69%] [G loss: 2.789004]\n",
            "1660 [D loss: 0.604403, acc: 68.75%] [G loss: 2.436459]\n",
            "1661 [D loss: 0.734822, acc: 59.38%] [G loss: 2.693779]\n",
            "1662 [D loss: 0.694595, acc: 56.25%] [G loss: 2.486864]\n",
            "1663 [D loss: 0.590302, acc: 68.75%] [G loss: 2.743342]\n",
            "1664 [D loss: 0.674198, acc: 59.38%] [G loss: 2.034689]\n",
            "1665 [D loss: 0.609221, acc: 67.19%] [G loss: 2.546243]\n",
            "1666 [D loss: 0.610412, acc: 64.06%] [G loss: 2.153175]\n",
            "1667 [D loss: 0.571523, acc: 65.62%] [G loss: 2.130942]\n",
            "1668 [D loss: 0.650800, acc: 62.50%] [G loss: 2.477930]\n",
            "1669 [D loss: 0.574719, acc: 65.62%] [G loss: 2.877463]\n",
            "1670 [D loss: 0.549606, acc: 75.00%] [G loss: 2.756717]\n",
            "1671 [D loss: 0.562991, acc: 67.19%] [G loss: 2.550362]\n",
            "1672 [D loss: 0.636067, acc: 60.94%] [G loss: 2.497455]\n",
            "1673 [D loss: 0.616944, acc: 62.50%] [G loss: 2.460008]\n",
            "1674 [D loss: 0.542305, acc: 73.44%] [G loss: 2.535730]\n",
            "1675 [D loss: 0.576578, acc: 68.75%] [G loss: 2.675755]\n",
            "1676 [D loss: 0.729711, acc: 56.25%] [G loss: 2.620190]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}