{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObc+FJ73tfS33xApJg0QH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/deep-learning-notebooks/blob/master/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJGSjmWIrggP",
        "colab_type": "text"
      },
      "source": [
        "While The generator of the simple GAN is a simple fully connected network. DCGAN architecture uses a standard CNN architecture on the discriminative model.In DCGAN convolutions are replaced with upconvolutions, so the representation at each layer of the generator is actually successively larger, as it mapes from a low-dimensional latent vector onto a high-dimensional image. \n",
        "\n",
        "\n",
        "\n",
        "*   Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
        "\n",
        "*   Use batch normalization in both the generator and the discriminator.\n",
        "\n",
        "*   Remove fully connected hidden layers for deeper architectures.\n",
        "\n",
        "*   Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
        "\n",
        "*   Use LeakyReLU activation in the discriminator for all layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myASiwRUmrBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "!mkdir images\n",
        "!mkdir saved_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teDY6BAUmvre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQLP1sbWm8HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_generator(self):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=(self.latent_dim,))\n",
        "    img = model(noise)\n",
        "\n",
        "    return Model(noise, img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VkjXolsnbRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(self):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=self.img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVQFeBEm_Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    # Load the dataset\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # Rescale -1 to 1\n",
        "    X_train = X_train / 127.5 - 1.\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        # Select a random half of images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Sample noise and generate a batch of new images\n",
        "        noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "        # Plot the progress\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            self.save_imgs(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGpzw-ELnA9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def save_imgs(self, epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "    gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "    plt.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOFRYFYnUu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96e8efd2-c91c-49cb-f8db-473913cb99b6"
      },
      "source": [
        "DCGAN.build_generator = build_generator\n",
        "DCGAN.build_discriminator = build_discriminator\n",
        "DCGAN.save_imgs = save_imgs\n",
        "DCGAN.train = train\n",
        "\n",
        "del build_generator,build_discriminator,save_imgs,train\n",
        "\n",
        "dcgan = DCGAN()\n",
        "dcgan.train(epochs=4000, batch_size=32, save_interval=50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 393,729\n",
            "Trainable params: 392,833\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 1.418547, acc.: 42.19%] [G loss: 0.271847]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.869006, acc.: 59.38%] [G loss: 0.687862]\n",
            "2 [D loss: 0.740671, acc.: 59.38%] [G loss: 1.095612]\n",
            "3 [D loss: 0.628077, acc.: 64.06%] [G loss: 1.363721]\n",
            "4 [D loss: 0.626445, acc.: 60.94%] [G loss: 0.999026]\n",
            "5 [D loss: 0.331230, acc.: 87.50%] [G loss: 0.935868]\n",
            "6 [D loss: 0.149492, acc.: 100.00%] [G loss: 0.425406]\n",
            "7 [D loss: 0.194222, acc.: 96.88%] [G loss: 0.563550]\n",
            "8 [D loss: 0.201568, acc.: 93.75%] [G loss: 0.441487]\n",
            "9 [D loss: 0.452541, acc.: 76.56%] [G loss: 0.601318]\n",
            "10 [D loss: 0.734070, acc.: 59.38%] [G loss: 0.920150]\n",
            "11 [D loss: 0.892166, acc.: 45.31%] [G loss: 1.892003]\n",
            "12 [D loss: 0.914740, acc.: 40.62%] [G loss: 1.997537]\n",
            "13 [D loss: 0.767300, acc.: 62.50%] [G loss: 1.474104]\n",
            "14 [D loss: 0.766599, acc.: 51.56%] [G loss: 1.252801]\n",
            "15 [D loss: 0.578992, acc.: 65.62%] [G loss: 1.452215]\n",
            "16 [D loss: 0.664312, acc.: 68.75%] [G loss: 1.773631]\n",
            "17 [D loss: 1.209761, acc.: 40.62%] [G loss: 1.870447]\n",
            "18 [D loss: 0.954697, acc.: 45.31%] [G loss: 1.552208]\n",
            "19 [D loss: 0.738759, acc.: 60.94%] [G loss: 1.162582]\n",
            "20 [D loss: 0.587434, acc.: 62.50%] [G loss: 1.100135]\n",
            "21 [D loss: 0.785408, acc.: 60.94%] [G loss: 1.106912]\n",
            "22 [D loss: 0.810626, acc.: 57.81%] [G loss: 1.579286]\n",
            "23 [D loss: 0.857961, acc.: 45.31%] [G loss: 1.821755]\n",
            "24 [D loss: 1.052974, acc.: 37.50%] [G loss: 1.554534]\n",
            "25 [D loss: 1.014936, acc.: 46.88%] [G loss: 1.127197]\n",
            "26 [D loss: 0.839591, acc.: 53.12%] [G loss: 1.235969]\n",
            "27 [D loss: 0.853521, acc.: 51.56%] [G loss: 1.267291]\n",
            "28 [D loss: 1.015030, acc.: 42.19%] [G loss: 1.331794]\n",
            "29 [D loss: 1.023754, acc.: 45.31%] [G loss: 1.253086]\n",
            "30 [D loss: 0.980102, acc.: 40.62%] [G loss: 1.392524]\n",
            "31 [D loss: 0.976297, acc.: 42.19%] [G loss: 1.211184]\n",
            "32 [D loss: 0.892134, acc.: 45.31%] [G loss: 1.526976]\n",
            "33 [D loss: 0.845221, acc.: 51.56%] [G loss: 1.460087]\n",
            "34 [D loss: 0.928314, acc.: 46.88%] [G loss: 1.214974]\n",
            "35 [D loss: 0.866764, acc.: 43.75%] [G loss: 0.959687]\n",
            "36 [D loss: 0.861698, acc.: 51.56%] [G loss: 1.255561]\n",
            "37 [D loss: 0.862904, acc.: 46.88%] [G loss: 0.973781]\n",
            "38 [D loss: 0.902562, acc.: 40.62%] [G loss: 0.912571]\n",
            "39 [D loss: 0.961120, acc.: 37.50%] [G loss: 1.118921]\n",
            "40 [D loss: 0.788751, acc.: 60.94%] [G loss: 1.003029]\n",
            "41 [D loss: 1.219753, acc.: 29.69%] [G loss: 1.301121]\n",
            "42 [D loss: 0.956211, acc.: 46.88%] [G loss: 1.730531]\n",
            "43 [D loss: 1.073691, acc.: 32.81%] [G loss: 1.468482]\n",
            "44 [D loss: 0.966592, acc.: 42.19%] [G loss: 1.355458]\n",
            "45 [D loss: 0.892557, acc.: 50.00%] [G loss: 1.296931]\n",
            "46 [D loss: 0.899072, acc.: 46.88%] [G loss: 1.412210]\n",
            "47 [D loss: 0.887151, acc.: 43.75%] [G loss: 1.310597]\n",
            "48 [D loss: 0.849118, acc.: 40.62%] [G loss: 1.058228]\n",
            "49 [D loss: 0.881782, acc.: 46.88%] [G loss: 0.973965]\n",
            "50 [D loss: 0.813718, acc.: 54.69%] [G loss: 1.037491]\n",
            "51 [D loss: 0.995616, acc.: 40.62%] [G loss: 1.194465]\n",
            "52 [D loss: 0.794576, acc.: 46.88%] [G loss: 1.148864]\n",
            "53 [D loss: 0.971904, acc.: 53.12%] [G loss: 0.977553]\n",
            "54 [D loss: 0.844032, acc.: 45.31%] [G loss: 1.365905]\n",
            "55 [D loss: 1.020632, acc.: 48.44%] [G loss: 1.161120]\n",
            "56 [D loss: 0.719793, acc.: 62.50%] [G loss: 1.053262]\n",
            "57 [D loss: 0.749750, acc.: 57.81%] [G loss: 0.957306]\n",
            "58 [D loss: 0.670105, acc.: 59.38%] [G loss: 1.569725]\n",
            "59 [D loss: 0.830541, acc.: 45.31%] [G loss: 1.262190]\n",
            "60 [D loss: 1.088467, acc.: 34.38%] [G loss: 1.215718]\n",
            "61 [D loss: 1.044647, acc.: 43.75%] [G loss: 1.279721]\n",
            "62 [D loss: 0.942677, acc.: 43.75%] [G loss: 1.105776]\n",
            "63 [D loss: 0.651365, acc.: 68.75%] [G loss: 1.117875]\n",
            "64 [D loss: 0.842372, acc.: 50.00%] [G loss: 1.336826]\n",
            "65 [D loss: 0.749748, acc.: 57.81%] [G loss: 1.255499]\n",
            "66 [D loss: 0.903864, acc.: 42.19%] [G loss: 1.301827]\n",
            "67 [D loss: 0.696363, acc.: 60.94%] [G loss: 1.181288]\n",
            "68 [D loss: 0.751434, acc.: 56.25%] [G loss: 1.242583]\n",
            "69 [D loss: 0.667178, acc.: 59.38%] [G loss: 1.268152]\n",
            "70 [D loss: 0.735091, acc.: 57.81%] [G loss: 0.953662]\n",
            "71 [D loss: 0.897002, acc.: 45.31%] [G loss: 1.180594]\n",
            "72 [D loss: 0.943759, acc.: 40.62%] [G loss: 1.309829]\n",
            "73 [D loss: 1.086634, acc.: 34.38%] [G loss: 1.238583]\n",
            "74 [D loss: 0.831291, acc.: 46.88%] [G loss: 1.286604]\n",
            "75 [D loss: 0.896606, acc.: 45.31%] [G loss: 0.973513]\n",
            "76 [D loss: 0.721806, acc.: 59.38%] [G loss: 1.008905]\n",
            "77 [D loss: 0.786113, acc.: 53.12%] [G loss: 1.231203]\n",
            "78 [D loss: 0.631492, acc.: 65.62%] [G loss: 1.305767]\n",
            "79 [D loss: 0.833961, acc.: 46.88%] [G loss: 1.620097]\n",
            "80 [D loss: 0.758518, acc.: 57.81%] [G loss: 1.216481]\n",
            "81 [D loss: 0.901606, acc.: 43.75%] [G loss: 1.253337]\n",
            "82 [D loss: 0.891892, acc.: 43.75%] [G loss: 1.040770]\n",
            "83 [D loss: 0.726223, acc.: 62.50%] [G loss: 0.971223]\n",
            "84 [D loss: 0.709766, acc.: 62.50%] [G loss: 1.021570]\n",
            "85 [D loss: 0.808000, acc.: 45.31%] [G loss: 1.183746]\n",
            "86 [D loss: 0.793263, acc.: 50.00%] [G loss: 1.432653]\n",
            "87 [D loss: 1.026994, acc.: 40.62%] [G loss: 1.143777]\n",
            "88 [D loss: 0.817940, acc.: 53.12%] [G loss: 1.289583]\n",
            "89 [D loss: 0.899359, acc.: 46.88%] [G loss: 1.052785]\n",
            "90 [D loss: 0.826686, acc.: 53.12%] [G loss: 1.080276]\n",
            "91 [D loss: 0.828054, acc.: 45.31%] [G loss: 1.216904]\n",
            "92 [D loss: 0.936074, acc.: 45.31%] [G loss: 1.341630]\n",
            "93 [D loss: 0.911714, acc.: 50.00%] [G loss: 1.241842]\n",
            "94 [D loss: 0.856560, acc.: 40.62%] [G loss: 1.169481]\n",
            "95 [D loss: 1.055066, acc.: 39.06%] [G loss: 1.082194]\n",
            "96 [D loss: 0.916798, acc.: 40.62%] [G loss: 1.167818]\n",
            "97 [D loss: 0.826879, acc.: 53.12%] [G loss: 1.106688]\n",
            "98 [D loss: 0.980152, acc.: 43.75%] [G loss: 1.005956]\n",
            "99 [D loss: 0.795211, acc.: 53.12%] [G loss: 1.175450]\n",
            "100 [D loss: 0.926136, acc.: 34.38%] [G loss: 1.036728]\n",
            "101 [D loss: 1.156775, acc.: 28.12%] [G loss: 1.107813]\n",
            "102 [D loss: 1.085504, acc.: 37.50%] [G loss: 0.966550]\n",
            "103 [D loss: 0.794146, acc.: 56.25%] [G loss: 1.183431]\n",
            "104 [D loss: 0.794304, acc.: 51.56%] [G loss: 1.158548]\n",
            "105 [D loss: 0.596565, acc.: 67.19%] [G loss: 1.295426]\n",
            "106 [D loss: 0.940844, acc.: 54.69%] [G loss: 1.180503]\n",
            "107 [D loss: 0.961929, acc.: 43.75%] [G loss: 1.071166]\n",
            "108 [D loss: 0.716376, acc.: 60.94%] [G loss: 1.257370]\n",
            "109 [D loss: 0.941805, acc.: 32.81%] [G loss: 0.729046]\n",
            "110 [D loss: 0.849608, acc.: 48.44%] [G loss: 1.220347]\n",
            "111 [D loss: 0.819917, acc.: 42.19%] [G loss: 1.259233]\n",
            "112 [D loss: 0.925591, acc.: 34.38%] [G loss: 1.371032]\n",
            "113 [D loss: 0.943648, acc.: 34.38%] [G loss: 1.042738]\n",
            "114 [D loss: 1.017014, acc.: 31.25%] [G loss: 0.909045]\n",
            "115 [D loss: 0.757118, acc.: 54.69%] [G loss: 1.223309]\n",
            "116 [D loss: 0.919805, acc.: 43.75%] [G loss: 0.981309]\n",
            "117 [D loss: 0.863153, acc.: 46.88%] [G loss: 1.155104]\n",
            "118 [D loss: 0.903199, acc.: 43.75%] [G loss: 1.079268]\n",
            "119 [D loss: 0.977684, acc.: 37.50%] [G loss: 0.949525]\n",
            "120 [D loss: 0.866924, acc.: 40.62%] [G loss: 1.052778]\n",
            "121 [D loss: 0.852496, acc.: 53.12%] [G loss: 1.033884]\n",
            "122 [D loss: 0.843723, acc.: 48.44%] [G loss: 1.159880]\n",
            "123 [D loss: 1.059989, acc.: 31.25%] [G loss: 0.998113]\n",
            "124 [D loss: 0.856876, acc.: 50.00%] [G loss: 1.151657]\n",
            "125 [D loss: 0.767863, acc.: 51.56%] [G loss: 1.307344]\n",
            "126 [D loss: 1.092486, acc.: 28.12%] [G loss: 1.062637]\n",
            "127 [D loss: 0.749171, acc.: 51.56%] [G loss: 1.197328]\n",
            "128 [D loss: 0.887809, acc.: 48.44%] [G loss: 1.089343]\n",
            "129 [D loss: 0.705907, acc.: 59.38%] [G loss: 1.220513]\n",
            "130 [D loss: 0.783796, acc.: 54.69%] [G loss: 1.065540]\n",
            "131 [D loss: 0.772926, acc.: 56.25%] [G loss: 1.126728]\n",
            "132 [D loss: 0.844337, acc.: 50.00%] [G loss: 1.027433]\n",
            "133 [D loss: 0.797845, acc.: 48.44%] [G loss: 1.145804]\n",
            "134 [D loss: 0.972158, acc.: 39.06%] [G loss: 1.124712]\n",
            "135 [D loss: 0.981139, acc.: 40.62%] [G loss: 0.981783]\n",
            "136 [D loss: 1.075583, acc.: 34.38%] [G loss: 1.119280]\n",
            "137 [D loss: 0.762643, acc.: 45.31%] [G loss: 1.162222]\n",
            "138 [D loss: 0.817610, acc.: 46.88%] [G loss: 1.195005]\n",
            "139 [D loss: 1.032950, acc.: 32.81%] [G loss: 0.941962]\n",
            "140 [D loss: 0.787525, acc.: 53.12%] [G loss: 0.887150]\n",
            "141 [D loss: 0.823153, acc.: 50.00%] [G loss: 1.271799]\n",
            "142 [D loss: 0.939768, acc.: 31.25%] [G loss: 0.884865]\n",
            "143 [D loss: 0.915020, acc.: 39.06%] [G loss: 1.035914]\n",
            "144 [D loss: 0.728286, acc.: 50.00%] [G loss: 1.087113]\n",
            "145 [D loss: 0.963605, acc.: 32.81%] [G loss: 0.998309]\n",
            "146 [D loss: 0.937050, acc.: 35.94%] [G loss: 1.019421]\n",
            "147 [D loss: 1.011305, acc.: 42.19%] [G loss: 1.246032]\n",
            "148 [D loss: 0.854148, acc.: 45.31%] [G loss: 1.094565]\n",
            "149 [D loss: 0.863535, acc.: 39.06%] [G loss: 1.064138]\n",
            "150 [D loss: 0.663891, acc.: 59.38%] [G loss: 0.944371]\n",
            "151 [D loss: 0.874343, acc.: 40.62%] [G loss: 1.226156]\n",
            "152 [D loss: 0.871843, acc.: 40.62%] [G loss: 1.133437]\n",
            "153 [D loss: 0.925070, acc.: 43.75%] [G loss: 1.055710]\n",
            "154 [D loss: 0.733137, acc.: 53.12%] [G loss: 0.946701]\n",
            "155 [D loss: 0.781149, acc.: 56.25%] [G loss: 1.133904]\n",
            "156 [D loss: 0.933793, acc.: 37.50%] [G loss: 1.062741]\n",
            "157 [D loss: 0.847826, acc.: 48.44%] [G loss: 1.060999]\n",
            "158 [D loss: 0.835562, acc.: 35.94%] [G loss: 1.004661]\n",
            "159 [D loss: 0.967679, acc.: 34.38%] [G loss: 1.027366]\n",
            "160 [D loss: 0.831557, acc.: 42.19%] [G loss: 0.993093]\n",
            "161 [D loss: 0.953536, acc.: 28.12%] [G loss: 1.048904]\n",
            "162 [D loss: 0.869719, acc.: 42.19%] [G loss: 1.345800]\n",
            "163 [D loss: 0.902886, acc.: 43.75%] [G loss: 1.147658]\n",
            "164 [D loss: 0.837161, acc.: 48.44%] [G loss: 0.988916]\n",
            "165 [D loss: 0.810468, acc.: 46.88%] [G loss: 0.971048]\n",
            "166 [D loss: 1.055114, acc.: 32.81%] [G loss: 1.028479]\n",
            "167 [D loss: 0.810971, acc.: 46.88%] [G loss: 1.081276]\n",
            "168 [D loss: 0.899695, acc.: 37.50%] [G loss: 1.170841]\n",
            "169 [D loss: 0.858382, acc.: 40.62%] [G loss: 1.232099]\n",
            "170 [D loss: 0.835912, acc.: 46.88%] [G loss: 1.232855]\n",
            "171 [D loss: 0.839446, acc.: 42.19%] [G loss: 0.861031]\n",
            "172 [D loss: 0.906564, acc.: 37.50%] [G loss: 1.214859]\n",
            "173 [D loss: 0.828365, acc.: 46.88%] [G loss: 0.868255]\n",
            "174 [D loss: 0.967694, acc.: 39.06%] [G loss: 0.958165]\n",
            "175 [D loss: 0.761469, acc.: 46.88%] [G loss: 1.019912]\n",
            "176 [D loss: 0.836608, acc.: 45.31%] [G loss: 1.007410]\n",
            "177 [D loss: 0.894781, acc.: 34.38%] [G loss: 1.132746]\n",
            "178 [D loss: 0.878335, acc.: 45.31%] [G loss: 0.848529]\n",
            "179 [D loss: 0.877416, acc.: 45.31%] [G loss: 1.108997]\n",
            "180 [D loss: 0.933790, acc.: 35.94%] [G loss: 1.140165]\n",
            "181 [D loss: 0.988728, acc.: 34.38%] [G loss: 0.913084]\n",
            "182 [D loss: 0.885098, acc.: 37.50%] [G loss: 1.013004]\n",
            "183 [D loss: 0.914930, acc.: 32.81%] [G loss: 0.903744]\n",
            "184 [D loss: 0.886157, acc.: 40.62%] [G loss: 1.109226]\n",
            "185 [D loss: 0.781092, acc.: 51.56%] [G loss: 0.911270]\n",
            "186 [D loss: 0.698907, acc.: 56.25%] [G loss: 1.232210]\n",
            "187 [D loss: 0.823872, acc.: 45.31%] [G loss: 1.210855]\n",
            "188 [D loss: 1.022998, acc.: 35.94%] [G loss: 1.192074]\n",
            "189 [D loss: 0.868946, acc.: 53.12%] [G loss: 1.197883]\n",
            "190 [D loss: 0.867688, acc.: 45.31%] [G loss: 0.986075]\n",
            "191 [D loss: 0.829877, acc.: 46.88%] [G loss: 0.978532]\n",
            "192 [D loss: 0.955976, acc.: 37.50%] [G loss: 1.090712]\n",
            "193 [D loss: 0.694996, acc.: 59.38%] [G loss: 1.167233]\n",
            "194 [D loss: 0.889390, acc.: 34.38%] [G loss: 1.025362]\n",
            "195 [D loss: 0.835715, acc.: 45.31%] [G loss: 1.092370]\n",
            "196 [D loss: 0.853829, acc.: 43.75%] [G loss: 0.937037]\n",
            "197 [D loss: 0.860454, acc.: 45.31%] [G loss: 0.936375]\n",
            "198 [D loss: 0.789279, acc.: 54.69%] [G loss: 1.104544]\n",
            "199 [D loss: 0.861533, acc.: 43.75%] [G loss: 1.090112]\n",
            "200 [D loss: 0.801117, acc.: 40.62%] [G loss: 0.909372]\n",
            "201 [D loss: 0.929005, acc.: 37.50%] [G loss: 0.983990]\n",
            "202 [D loss: 0.822983, acc.: 48.44%] [G loss: 0.857632]\n",
            "203 [D loss: 0.722232, acc.: 56.25%] [G loss: 0.944087]\n",
            "204 [D loss: 0.833898, acc.: 43.75%] [G loss: 1.000560]\n",
            "205 [D loss: 0.852265, acc.: 42.19%] [G loss: 0.912412]\n",
            "206 [D loss: 0.789605, acc.: 37.50%] [G loss: 0.986690]\n",
            "207 [D loss: 0.828172, acc.: 50.00%] [G loss: 0.986502]\n",
            "208 [D loss: 1.021468, acc.: 39.06%] [G loss: 1.039095]\n",
            "209 [D loss: 0.737954, acc.: 54.69%] [G loss: 1.072166]\n",
            "210 [D loss: 0.863352, acc.: 34.38%] [G loss: 1.084928]\n",
            "211 [D loss: 0.739168, acc.: 56.25%] [G loss: 1.165949]\n",
            "212 [D loss: 0.870507, acc.: 35.94%] [G loss: 1.000806]\n",
            "213 [D loss: 0.760086, acc.: 53.12%] [G loss: 0.926768]\n",
            "214 [D loss: 0.734323, acc.: 57.81%] [G loss: 1.071486]\n",
            "215 [D loss: 0.875575, acc.: 45.31%] [G loss: 1.106216]\n",
            "216 [D loss: 0.841380, acc.: 43.75%] [G loss: 1.005650]\n",
            "217 [D loss: 0.788895, acc.: 51.56%] [G loss: 1.027005]\n",
            "218 [D loss: 0.762810, acc.: 45.31%] [G loss: 1.002384]\n",
            "219 [D loss: 0.824053, acc.: 43.75%] [G loss: 0.899055]\n",
            "220 [D loss: 0.892673, acc.: 40.62%] [G loss: 0.914769]\n",
            "221 [D loss: 0.700784, acc.: 53.12%] [G loss: 1.261999]\n",
            "222 [D loss: 0.806485, acc.: 50.00%] [G loss: 1.092446]\n",
            "223 [D loss: 0.840987, acc.: 48.44%] [G loss: 0.959017]\n",
            "224 [D loss: 0.835515, acc.: 46.88%] [G loss: 0.824108]\n",
            "225 [D loss: 0.852756, acc.: 39.06%] [G loss: 0.855750]\n",
            "226 [D loss: 0.784283, acc.: 51.56%] [G loss: 1.057159]\n",
            "227 [D loss: 0.925740, acc.: 45.31%] [G loss: 0.921033]\n",
            "228 [D loss: 0.826334, acc.: 42.19%] [G loss: 1.022727]\n",
            "229 [D loss: 0.775304, acc.: 45.31%] [G loss: 1.043473]\n",
            "230 [D loss: 0.796164, acc.: 48.44%] [G loss: 1.007708]\n",
            "231 [D loss: 0.655774, acc.: 59.38%] [G loss: 1.068046]\n",
            "232 [D loss: 0.923050, acc.: 35.94%] [G loss: 1.031756]\n",
            "233 [D loss: 0.823398, acc.: 42.19%] [G loss: 0.896474]\n",
            "234 [D loss: 0.838306, acc.: 45.31%] [G loss: 0.900550]\n",
            "235 [D loss: 0.796991, acc.: 59.38%] [G loss: 1.114821]\n",
            "236 [D loss: 0.750761, acc.: 50.00%] [G loss: 0.978760]\n",
            "237 [D loss: 0.822506, acc.: 48.44%] [G loss: 0.873162]\n",
            "238 [D loss: 0.852537, acc.: 39.06%] [G loss: 0.926630]\n",
            "239 [D loss: 0.801120, acc.: 48.44%] [G loss: 1.031734]\n",
            "240 [D loss: 0.808323, acc.: 42.19%] [G loss: 0.929356]\n",
            "241 [D loss: 0.835221, acc.: 42.19%] [G loss: 0.931735]\n",
            "242 [D loss: 0.843613, acc.: 43.75%] [G loss: 0.989076]\n",
            "243 [D loss: 0.861916, acc.: 42.19%] [G loss: 0.944337]\n",
            "244 [D loss: 0.952995, acc.: 32.81%] [G loss: 1.016925]\n",
            "245 [D loss: 0.836216, acc.: 40.62%] [G loss: 1.027991]\n",
            "246 [D loss: 0.687154, acc.: 60.94%] [G loss: 1.133947]\n",
            "247 [D loss: 0.952004, acc.: 37.50%] [G loss: 0.971238]\n",
            "248 [D loss: 0.783588, acc.: 54.69%] [G loss: 1.102923]\n",
            "249 [D loss: 0.792080, acc.: 51.56%] [G loss: 0.970190]\n",
            "250 [D loss: 0.692129, acc.: 53.12%] [G loss: 1.248345]\n",
            "251 [D loss: 0.762680, acc.: 54.69%] [G loss: 1.150386]\n",
            "252 [D loss: 0.934896, acc.: 43.75%] [G loss: 0.858004]\n",
            "253 [D loss: 0.927487, acc.: 32.81%] [G loss: 0.944496]\n",
            "254 [D loss: 0.796793, acc.: 50.00%] [G loss: 0.997548]\n",
            "255 [D loss: 0.891348, acc.: 39.06%] [G loss: 1.029997]\n",
            "256 [D loss: 0.807249, acc.: 53.12%] [G loss: 0.867993]\n",
            "257 [D loss: 0.869762, acc.: 43.75%] [G loss: 1.145581]\n",
            "258 [D loss: 0.877858, acc.: 43.75%] [G loss: 1.004964]\n",
            "259 [D loss: 0.874410, acc.: 42.19%] [G loss: 1.181997]\n",
            "260 [D loss: 0.666907, acc.: 59.38%] [G loss: 1.208974]\n",
            "261 [D loss: 0.743894, acc.: 50.00%] [G loss: 0.823670]\n",
            "262 [D loss: 0.924888, acc.: 42.19%] [G loss: 0.912060]\n",
            "263 [D loss: 0.846068, acc.: 45.31%] [G loss: 1.060179]\n",
            "264 [D loss: 0.872732, acc.: 42.19%] [G loss: 0.871201]\n",
            "265 [D loss: 0.740274, acc.: 50.00%] [G loss: 0.922749]\n",
            "266 [D loss: 0.960043, acc.: 26.56%] [G loss: 0.831511]\n",
            "267 [D loss: 0.838388, acc.: 48.44%] [G loss: 1.141031]\n",
            "268 [D loss: 0.893045, acc.: 42.19%] [G loss: 0.954382]\n",
            "269 [D loss: 0.761099, acc.: 51.56%] [G loss: 1.058975]\n",
            "270 [D loss: 0.897436, acc.: 29.69%] [G loss: 1.164406]\n",
            "271 [D loss: 0.969109, acc.: 37.50%] [G loss: 1.031923]\n",
            "272 [D loss: 0.782359, acc.: 42.19%] [G loss: 1.032139]\n",
            "273 [D loss: 0.818687, acc.: 40.62%] [G loss: 0.902750]\n",
            "274 [D loss: 0.804832, acc.: 42.19%] [G loss: 1.081871]\n",
            "275 [D loss: 0.767726, acc.: 46.88%] [G loss: 1.013745]\n",
            "276 [D loss: 0.720205, acc.: 53.12%] [G loss: 1.241271]\n",
            "277 [D loss: 0.876414, acc.: 39.06%] [G loss: 0.856266]\n",
            "278 [D loss: 0.802018, acc.: 48.44%] [G loss: 0.989917]\n",
            "279 [D loss: 0.846705, acc.: 35.94%] [G loss: 1.140089]\n",
            "280 [D loss: 0.869017, acc.: 39.06%] [G loss: 1.081679]\n",
            "281 [D loss: 0.812089, acc.: 43.75%] [G loss: 1.032243]\n",
            "282 [D loss: 0.835640, acc.: 39.06%] [G loss: 0.934827]\n",
            "283 [D loss: 0.941729, acc.: 35.94%] [G loss: 0.860118]\n",
            "284 [D loss: 0.882418, acc.: 40.62%] [G loss: 0.954250]\n",
            "285 [D loss: 0.784870, acc.: 50.00%] [G loss: 1.136203]\n",
            "286 [D loss: 0.875167, acc.: 43.75%] [G loss: 0.917884]\n",
            "287 [D loss: 0.820465, acc.: 40.62%] [G loss: 1.053459]\n",
            "288 [D loss: 0.715280, acc.: 53.12%] [G loss: 1.032150]\n",
            "289 [D loss: 0.759392, acc.: 54.69%] [G loss: 1.057304]\n",
            "290 [D loss: 0.876037, acc.: 35.94%] [G loss: 0.890916]\n",
            "291 [D loss: 0.820496, acc.: 46.88%] [G loss: 1.047320]\n",
            "292 [D loss: 0.923569, acc.: 37.50%] [G loss: 1.006201]\n",
            "293 [D loss: 0.792796, acc.: 50.00%] [G loss: 0.995419]\n",
            "294 [D loss: 0.863700, acc.: 37.50%] [G loss: 1.076510]\n",
            "295 [D loss: 0.821351, acc.: 50.00%] [G loss: 1.122230]\n",
            "296 [D loss: 0.790183, acc.: 50.00%] [G loss: 0.916710]\n",
            "297 [D loss: 0.949909, acc.: 43.75%] [G loss: 0.950558]\n",
            "298 [D loss: 0.767171, acc.: 51.56%] [G loss: 1.127741]\n",
            "299 [D loss: 0.786240, acc.: 43.75%] [G loss: 1.071609]\n",
            "300 [D loss: 0.817971, acc.: 46.88%] [G loss: 1.108925]\n",
            "301 [D loss: 0.849876, acc.: 48.44%] [G loss: 0.958909]\n",
            "302 [D loss: 0.904779, acc.: 40.62%] [G loss: 0.878932]\n",
            "303 [D loss: 0.698011, acc.: 50.00%] [G loss: 0.833800]\n",
            "304 [D loss: 0.743669, acc.: 51.56%] [G loss: 0.848281]\n",
            "305 [D loss: 0.804527, acc.: 43.75%] [G loss: 0.899815]\n",
            "306 [D loss: 0.710733, acc.: 56.25%] [G loss: 0.985581]\n",
            "307 [D loss: 0.809223, acc.: 46.88%] [G loss: 0.872518]\n",
            "308 [D loss: 0.792546, acc.: 48.44%] [G loss: 1.023187]\n",
            "309 [D loss: 0.839707, acc.: 51.56%] [G loss: 0.940197]\n",
            "310 [D loss: 0.775646, acc.: 48.44%] [G loss: 1.063470]\n",
            "311 [D loss: 0.793028, acc.: 51.56%] [G loss: 0.928776]\n",
            "312 [D loss: 0.780634, acc.: 45.31%] [G loss: 1.013483]\n",
            "313 [D loss: 0.796637, acc.: 45.31%] [G loss: 0.856076]\n",
            "314 [D loss: 0.781889, acc.: 46.88%] [G loss: 0.929899]\n",
            "315 [D loss: 0.755053, acc.: 51.56%] [G loss: 0.978897]\n",
            "316 [D loss: 0.839946, acc.: 43.75%] [G loss: 0.900965]\n",
            "317 [D loss: 0.813314, acc.: 43.75%] [G loss: 0.888481]\n",
            "318 [D loss: 0.765285, acc.: 48.44%] [G loss: 1.122080]\n",
            "319 [D loss: 0.825129, acc.: 40.62%] [G loss: 0.931009]\n",
            "320 [D loss: 0.867590, acc.: 45.31%] [G loss: 1.170027]\n",
            "321 [D loss: 0.813184, acc.: 53.12%] [G loss: 0.997115]\n",
            "322 [D loss: 0.869232, acc.: 35.94%] [G loss: 0.874396]\n",
            "323 [D loss: 0.876817, acc.: 37.50%] [G loss: 0.879889]\n",
            "324 [D loss: 0.762705, acc.: 45.31%] [G loss: 0.833332]\n",
            "325 [D loss: 0.785244, acc.: 51.56%] [G loss: 0.983523]\n",
            "326 [D loss: 0.818610, acc.: 46.88%] [G loss: 0.976401]\n",
            "327 [D loss: 0.859453, acc.: 48.44%] [G loss: 0.944410]\n",
            "328 [D loss: 0.833565, acc.: 35.94%] [G loss: 1.016816]\n",
            "329 [D loss: 0.724006, acc.: 57.81%] [G loss: 0.990777]\n",
            "330 [D loss: 0.793650, acc.: 54.69%] [G loss: 0.858657]\n",
            "331 [D loss: 0.835700, acc.: 43.75%] [G loss: 0.866925]\n",
            "332 [D loss: 0.847886, acc.: 45.31%] [G loss: 0.906252]\n",
            "333 [D loss: 0.823983, acc.: 37.50%] [G loss: 1.011933]\n",
            "334 [D loss: 0.676288, acc.: 65.62%] [G loss: 0.963279]\n",
            "335 [D loss: 0.739661, acc.: 54.69%] [G loss: 1.095030]\n",
            "336 [D loss: 0.775119, acc.: 53.12%] [G loss: 0.872030]\n",
            "337 [D loss: 0.733295, acc.: 54.69%] [G loss: 0.967104]\n",
            "338 [D loss: 0.812330, acc.: 40.62%] [G loss: 0.909239]\n",
            "339 [D loss: 0.868897, acc.: 40.62%] [G loss: 1.008969]\n",
            "340 [D loss: 0.871209, acc.: 40.62%] [G loss: 1.082580]\n",
            "341 [D loss: 0.775183, acc.: 43.75%] [G loss: 1.027667]\n",
            "342 [D loss: 0.683325, acc.: 54.69%] [G loss: 1.119935]\n",
            "343 [D loss: 0.835438, acc.: 46.88%] [G loss: 0.890955]\n",
            "344 [D loss: 0.783727, acc.: 43.75%] [G loss: 1.047311]\n",
            "345 [D loss: 0.682745, acc.: 56.25%] [G loss: 1.004892]\n",
            "346 [D loss: 0.829073, acc.: 43.75%] [G loss: 0.944018]\n",
            "347 [D loss: 0.752694, acc.: 50.00%] [G loss: 0.974124]\n",
            "348 [D loss: 0.927538, acc.: 28.12%] [G loss: 0.990645]\n",
            "349 [D loss: 0.827585, acc.: 42.19%] [G loss: 1.067247]\n",
            "350 [D loss: 0.751642, acc.: 48.44%] [G loss: 1.153444]\n",
            "351 [D loss: 0.891420, acc.: 31.25%] [G loss: 0.963036]\n",
            "352 [D loss: 0.741868, acc.: 50.00%] [G loss: 0.973263]\n",
            "353 [D loss: 0.866068, acc.: 37.50%] [G loss: 0.947132]\n",
            "354 [D loss: 0.729728, acc.: 54.69%] [G loss: 0.913032]\n",
            "355 [D loss: 0.759118, acc.: 51.56%] [G loss: 0.863750]\n",
            "356 [D loss: 0.938625, acc.: 34.38%] [G loss: 1.097125]\n",
            "357 [D loss: 0.783551, acc.: 51.56%] [G loss: 0.957816]\n",
            "358 [D loss: 0.718997, acc.: 53.12%] [G loss: 0.918457]\n",
            "359 [D loss: 0.818301, acc.: 48.44%] [G loss: 0.923213]\n",
            "360 [D loss: 0.875513, acc.: 32.81%] [G loss: 0.995894]\n",
            "361 [D loss: 0.806982, acc.: 39.06%] [G loss: 0.847705]\n",
            "362 [D loss: 0.783200, acc.: 51.56%] [G loss: 0.921848]\n",
            "363 [D loss: 0.733906, acc.: 48.44%] [G loss: 0.921206]\n",
            "364 [D loss: 0.877879, acc.: 39.06%] [G loss: 0.914793]\n",
            "365 [D loss: 0.774986, acc.: 45.31%] [G loss: 1.023406]\n",
            "366 [D loss: 0.762164, acc.: 45.31%] [G loss: 1.132853]\n",
            "367 [D loss: 0.897412, acc.: 43.75%] [G loss: 0.991213]\n",
            "368 [D loss: 0.895337, acc.: 37.50%] [G loss: 1.046036]\n",
            "369 [D loss: 0.901848, acc.: 40.62%] [G loss: 0.979493]\n",
            "370 [D loss: 0.829164, acc.: 43.75%] [G loss: 0.932672]\n",
            "371 [D loss: 0.740798, acc.: 45.31%] [G loss: 1.053112]\n",
            "372 [D loss: 0.842640, acc.: 45.31%] [G loss: 1.053849]\n",
            "373 [D loss: 0.748009, acc.: 50.00%] [G loss: 0.912124]\n",
            "374 [D loss: 0.754222, acc.: 51.56%] [G loss: 0.931612]\n",
            "375 [D loss: 0.778633, acc.: 51.56%] [G loss: 0.730029]\n",
            "376 [D loss: 0.756741, acc.: 46.88%] [G loss: 0.805043]\n",
            "377 [D loss: 0.866132, acc.: 35.94%] [G loss: 0.908340]\n",
            "378 [D loss: 0.790613, acc.: 45.31%] [G loss: 0.795201]\n",
            "379 [D loss: 0.765398, acc.: 51.56%] [G loss: 0.985198]\n",
            "380 [D loss: 0.822815, acc.: 48.44%] [G loss: 1.047388]\n",
            "381 [D loss: 0.761825, acc.: 50.00%] [G loss: 0.963316]\n",
            "382 [D loss: 0.898064, acc.: 37.50%] [G loss: 0.949443]\n",
            "383 [D loss: 0.812390, acc.: 45.31%] [G loss: 0.848818]\n",
            "384 [D loss: 0.846678, acc.: 42.19%] [G loss: 0.804222]\n",
            "385 [D loss: 0.784471, acc.: 42.19%] [G loss: 0.940248]\n",
            "386 [D loss: 0.788823, acc.: 45.31%] [G loss: 1.013363]\n",
            "387 [D loss: 0.853295, acc.: 43.75%] [G loss: 0.983677]\n",
            "388 [D loss: 0.779394, acc.: 46.88%] [G loss: 0.922808]\n",
            "389 [D loss: 0.670185, acc.: 64.06%] [G loss: 0.962669]\n",
            "390 [D loss: 0.766013, acc.: 43.75%] [G loss: 1.063801]\n",
            "391 [D loss: 0.820524, acc.: 39.06%] [G loss: 0.919731]\n",
            "392 [D loss: 0.740313, acc.: 45.31%] [G loss: 0.777239]\n",
            "393 [D loss: 0.882409, acc.: 48.44%] [G loss: 0.921734]\n",
            "394 [D loss: 0.808926, acc.: 45.31%] [G loss: 0.957555]\n",
            "395 [D loss: 0.773475, acc.: 56.25%] [G loss: 0.971289]\n",
            "396 [D loss: 0.727530, acc.: 54.69%] [G loss: 0.990784]\n",
            "397 [D loss: 0.888204, acc.: 40.62%] [G loss: 0.912064]\n",
            "398 [D loss: 0.805279, acc.: 43.75%] [G loss: 0.930569]\n",
            "399 [D loss: 0.730088, acc.: 51.56%] [G loss: 0.867527]\n",
            "400 [D loss: 0.796571, acc.: 43.75%] [G loss: 1.019994]\n",
            "401 [D loss: 0.902495, acc.: 39.06%] [G loss: 0.861175]\n",
            "402 [D loss: 0.771226, acc.: 46.88%] [G loss: 1.076452]\n",
            "403 [D loss: 0.897825, acc.: 34.38%] [G loss: 0.982529]\n",
            "404 [D loss: 0.742655, acc.: 46.88%] [G loss: 0.862910]\n",
            "405 [D loss: 0.779312, acc.: 46.88%] [G loss: 0.937055]\n",
            "406 [D loss: 0.800061, acc.: 39.06%] [G loss: 1.018534]\n",
            "407 [D loss: 0.755439, acc.: 43.75%] [G loss: 1.058264]\n",
            "408 [D loss: 0.842304, acc.: 35.94%] [G loss: 1.019792]\n",
            "409 [D loss: 0.787990, acc.: 43.75%] [G loss: 0.923088]\n",
            "410 [D loss: 0.768770, acc.: 48.44%] [G loss: 0.900092]\n",
            "411 [D loss: 0.820285, acc.: 48.44%] [G loss: 1.046460]\n",
            "412 [D loss: 0.791667, acc.: 46.88%] [G loss: 0.925561]\n",
            "413 [D loss: 0.863310, acc.: 42.19%] [G loss: 1.028515]\n",
            "414 [D loss: 0.793453, acc.: 39.06%] [G loss: 1.037043]\n",
            "415 [D loss: 0.861908, acc.: 34.38%] [G loss: 1.115863]\n",
            "416 [D loss: 0.754240, acc.: 53.12%] [G loss: 0.832227]\n",
            "417 [D loss: 0.836969, acc.: 31.25%] [G loss: 0.920617]\n",
            "418 [D loss: 0.802183, acc.: 39.06%] [G loss: 0.814727]\n",
            "419 [D loss: 0.787650, acc.: 54.69%] [G loss: 0.922308]\n",
            "420 [D loss: 0.756202, acc.: 48.44%] [G loss: 0.986693]\n",
            "421 [D loss: 0.793983, acc.: 35.94%] [G loss: 1.041670]\n",
            "422 [D loss: 0.745505, acc.: 45.31%] [G loss: 0.975040]\n",
            "423 [D loss: 0.740172, acc.: 56.25%] [G loss: 1.039238]\n",
            "424 [D loss: 0.792266, acc.: 39.06%] [G loss: 1.007382]\n",
            "425 [D loss: 0.793272, acc.: 50.00%] [G loss: 0.935222]\n",
            "426 [D loss: 0.775062, acc.: 46.88%] [G loss: 0.848907]\n",
            "427 [D loss: 0.707653, acc.: 53.12%] [G loss: 0.971724]\n",
            "428 [D loss: 0.838739, acc.: 42.19%] [G loss: 0.902347]\n",
            "429 [D loss: 0.839430, acc.: 42.19%] [G loss: 0.951398]\n",
            "430 [D loss: 0.770273, acc.: 48.44%] [G loss: 1.061304]\n",
            "431 [D loss: 0.808651, acc.: 51.56%] [G loss: 1.077861]\n",
            "432 [D loss: 0.725704, acc.: 59.38%] [G loss: 0.943820]\n",
            "433 [D loss: 0.775453, acc.: 53.12%] [G loss: 0.881802]\n",
            "434 [D loss: 0.787462, acc.: 40.62%] [G loss: 0.962176]\n",
            "435 [D loss: 0.781927, acc.: 43.75%] [G loss: 1.043498]\n",
            "436 [D loss: 0.750565, acc.: 48.44%] [G loss: 0.943844]\n",
            "437 [D loss: 0.772071, acc.: 51.56%] [G loss: 1.124654]\n",
            "438 [D loss: 0.748055, acc.: 51.56%] [G loss: 0.844742]\n",
            "439 [D loss: 0.724907, acc.: 51.56%] [G loss: 0.909047]\n",
            "440 [D loss: 0.792281, acc.: 43.75%] [G loss: 0.785900]\n",
            "441 [D loss: 0.751913, acc.: 59.38%] [G loss: 1.046697]\n",
            "442 [D loss: 0.827800, acc.: 53.12%] [G loss: 0.832685]\n",
            "443 [D loss: 0.745466, acc.: 45.31%] [G loss: 0.838509]\n",
            "444 [D loss: 0.904710, acc.: 39.06%] [G loss: 0.930315]\n",
            "445 [D loss: 0.721264, acc.: 50.00%] [G loss: 0.942120]\n",
            "446 [D loss: 0.718560, acc.: 54.69%] [G loss: 1.078320]\n",
            "447 [D loss: 0.779636, acc.: 45.31%] [G loss: 0.957747]\n",
            "448 [D loss: 0.838416, acc.: 51.56%] [G loss: 0.978889]\n",
            "449 [D loss: 0.757578, acc.: 48.44%] [G loss: 0.961974]\n",
            "450 [D loss: 0.883238, acc.: 39.06%] [G loss: 1.120731]\n",
            "451 [D loss: 0.824388, acc.: 46.88%] [G loss: 0.909374]\n",
            "452 [D loss: 0.774414, acc.: 50.00%] [G loss: 0.871990]\n",
            "453 [D loss: 0.819271, acc.: 37.50%] [G loss: 0.830240]\n",
            "454 [D loss: 0.772946, acc.: 51.56%] [G loss: 0.904794]\n",
            "455 [D loss: 0.852661, acc.: 39.06%] [G loss: 0.839177]\n",
            "456 [D loss: 0.813734, acc.: 43.75%] [G loss: 0.896307]\n",
            "457 [D loss: 0.755804, acc.: 50.00%] [G loss: 0.862116]\n",
            "458 [D loss: 0.735647, acc.: 46.88%] [G loss: 0.850640]\n",
            "459 [D loss: 0.805459, acc.: 48.44%] [G loss: 1.041750]\n",
            "460 [D loss: 0.765134, acc.: 51.56%] [G loss: 1.209938]\n",
            "461 [D loss: 0.788162, acc.: 43.75%] [G loss: 1.120576]\n",
            "462 [D loss: 0.744722, acc.: 50.00%] [G loss: 0.901800]\n",
            "463 [D loss: 0.834345, acc.: 43.75%] [G loss: 0.789687]\n",
            "464 [D loss: 0.785058, acc.: 43.75%] [G loss: 1.001039]\n",
            "465 [D loss: 0.781868, acc.: 46.88%] [G loss: 0.963862]\n",
            "466 [D loss: 0.815663, acc.: 45.31%] [G loss: 1.016079]\n",
            "467 [D loss: 0.809690, acc.: 40.62%] [G loss: 0.917264]\n",
            "468 [D loss: 0.750550, acc.: 43.75%] [G loss: 0.932284]\n",
            "469 [D loss: 0.783041, acc.: 43.75%] [G loss: 0.837757]\n",
            "470 [D loss: 0.816355, acc.: 45.31%] [G loss: 1.023212]\n",
            "471 [D loss: 0.748665, acc.: 42.19%] [G loss: 0.904314]\n",
            "472 [D loss: 0.827458, acc.: 42.19%] [G loss: 0.884664]\n",
            "473 [D loss: 0.836241, acc.: 29.69%] [G loss: 0.912761]\n",
            "474 [D loss: 0.796472, acc.: 40.62%] [G loss: 0.980082]\n",
            "475 [D loss: 0.819475, acc.: 39.06%] [G loss: 0.962306]\n",
            "476 [D loss: 0.717689, acc.: 56.25%] [G loss: 0.951893]\n",
            "477 [D loss: 0.824079, acc.: 45.31%] [G loss: 0.819628]\n",
            "478 [D loss: 0.658517, acc.: 59.38%] [G loss: 0.947973]\n",
            "479 [D loss: 0.828647, acc.: 39.06%] [G loss: 0.854444]\n",
            "480 [D loss: 0.734086, acc.: 56.25%] [G loss: 1.016659]\n",
            "481 [D loss: 0.781942, acc.: 42.19%] [G loss: 1.014114]\n",
            "482 [D loss: 0.785649, acc.: 50.00%] [G loss: 0.888601]\n",
            "483 [D loss: 0.798746, acc.: 50.00%] [G loss: 0.819629]\n",
            "484 [D loss: 0.701935, acc.: 56.25%] [G loss: 0.931798]\n",
            "485 [D loss: 0.773088, acc.: 39.06%] [G loss: 0.932211]\n",
            "486 [D loss: 0.809263, acc.: 46.88%] [G loss: 0.896015]\n",
            "487 [D loss: 0.819024, acc.: 45.31%] [G loss: 1.050961]\n",
            "488 [D loss: 0.795101, acc.: 37.50%] [G loss: 0.798924]\n",
            "489 [D loss: 0.741461, acc.: 50.00%] [G loss: 0.930651]\n",
            "490 [D loss: 0.743873, acc.: 54.69%] [G loss: 0.842152]\n",
            "491 [D loss: 0.770767, acc.: 48.44%] [G loss: 0.880249]\n",
            "492 [D loss: 0.831560, acc.: 43.75%] [G loss: 0.901362]\n",
            "493 [D loss: 0.767693, acc.: 56.25%] [G loss: 0.844131]\n",
            "494 [D loss: 0.678862, acc.: 53.12%] [G loss: 0.879938]\n",
            "495 [D loss: 0.724430, acc.: 59.38%] [G loss: 1.079133]\n",
            "496 [D loss: 0.726625, acc.: 56.25%] [G loss: 0.868188]\n",
            "497 [D loss: 0.825684, acc.: 46.88%] [G loss: 0.927841]\n",
            "498 [D loss: 0.710566, acc.: 56.25%] [G loss: 0.894118]\n",
            "499 [D loss: 0.738455, acc.: 48.44%] [G loss: 0.836284]\n",
            "500 [D loss: 0.710738, acc.: 46.88%] [G loss: 0.907228]\n",
            "501 [D loss: 0.633620, acc.: 62.50%] [G loss: 1.019022]\n",
            "502 [D loss: 0.878048, acc.: 39.06%] [G loss: 0.910916]\n",
            "503 [D loss: 0.780975, acc.: 45.31%] [G loss: 0.911452]\n",
            "504 [D loss: 0.826248, acc.: 40.62%] [G loss: 1.034407]\n",
            "505 [D loss: 0.733905, acc.: 50.00%] [G loss: 0.906725]\n",
            "506 [D loss: 0.769011, acc.: 50.00%] [G loss: 0.916728]\n",
            "507 [D loss: 0.771633, acc.: 46.88%] [G loss: 0.940873]\n",
            "508 [D loss: 0.781505, acc.: 43.75%] [G loss: 0.854521]\n",
            "509 [D loss: 0.812743, acc.: 39.06%] [G loss: 0.805733]\n",
            "510 [D loss: 0.797439, acc.: 40.62%] [G loss: 0.943579]\n",
            "511 [D loss: 0.743269, acc.: 53.12%] [G loss: 1.104452]\n",
            "512 [D loss: 0.651713, acc.: 60.94%] [G loss: 0.983921]\n",
            "513 [D loss: 0.666722, acc.: 57.81%] [G loss: 0.913547]\n",
            "514 [D loss: 0.786114, acc.: 37.50%] [G loss: 1.012892]\n",
            "515 [D loss: 0.696988, acc.: 56.25%] [G loss: 0.951084]\n",
            "516 [D loss: 0.733284, acc.: 48.44%] [G loss: 1.030873]\n",
            "517 [D loss: 0.729990, acc.: 51.56%] [G loss: 0.969096]\n",
            "518 [D loss: 0.784994, acc.: 46.88%] [G loss: 0.814796]\n",
            "519 [D loss: 0.687506, acc.: 53.12%] [G loss: 0.843934]\n",
            "520 [D loss: 0.693040, acc.: 57.81%] [G loss: 0.981427]\n",
            "521 [D loss: 0.725848, acc.: 45.31%] [G loss: 1.022621]\n",
            "522 [D loss: 0.794862, acc.: 37.50%] [G loss: 1.032390]\n",
            "523 [D loss: 0.948739, acc.: 28.12%] [G loss: 0.935237]\n",
            "524 [D loss: 0.853634, acc.: 31.25%] [G loss: 0.984295]\n",
            "525 [D loss: 0.800852, acc.: 48.44%] [G loss: 0.909978]\n",
            "526 [D loss: 0.738404, acc.: 46.88%] [G loss: 0.999809]\n",
            "527 [D loss: 0.767758, acc.: 45.31%] [G loss: 0.916774]\n",
            "528 [D loss: 0.704288, acc.: 48.44%] [G loss: 0.874088]\n",
            "529 [D loss: 0.863800, acc.: 31.25%] [G loss: 0.826289]\n",
            "530 [D loss: 0.683167, acc.: 53.12%] [G loss: 0.887788]\n",
            "531 [D loss: 0.725768, acc.: 54.69%] [G loss: 0.839864]\n",
            "532 [D loss: 0.747236, acc.: 45.31%] [G loss: 0.944916]\n",
            "533 [D loss: 0.816048, acc.: 37.50%] [G loss: 0.931348]\n",
            "534 [D loss: 0.781030, acc.: 51.56%] [G loss: 0.999359]\n",
            "535 [D loss: 0.745576, acc.: 46.88%] [G loss: 0.992773]\n",
            "536 [D loss: 0.708070, acc.: 56.25%] [G loss: 0.968044]\n",
            "537 [D loss: 0.781080, acc.: 51.56%] [G loss: 1.055522]\n",
            "538 [D loss: 0.939882, acc.: 37.50%] [G loss: 1.008669]\n",
            "539 [D loss: 0.774931, acc.: 46.88%] [G loss: 0.859371]\n",
            "540 [D loss: 0.804419, acc.: 43.75%] [G loss: 0.889990]\n",
            "541 [D loss: 0.773290, acc.: 48.44%] [G loss: 0.926368]\n",
            "542 [D loss: 0.795898, acc.: 42.19%] [G loss: 0.899718]\n",
            "543 [D loss: 0.734421, acc.: 51.56%] [G loss: 0.963666]\n",
            "544 [D loss: 0.862808, acc.: 31.25%] [G loss: 0.832105]\n",
            "545 [D loss: 0.803888, acc.: 45.31%] [G loss: 0.884393]\n",
            "546 [D loss: 0.804407, acc.: 45.31%] [G loss: 1.006760]\n",
            "547 [D loss: 0.727150, acc.: 54.69%] [G loss: 1.011009]\n",
            "548 [D loss: 0.783229, acc.: 43.75%] [G loss: 0.950229]\n",
            "549 [D loss: 0.756534, acc.: 50.00%] [G loss: 0.968895]\n",
            "550 [D loss: 0.774663, acc.: 50.00%] [G loss: 0.886903]\n",
            "551 [D loss: 0.679703, acc.: 59.38%] [G loss: 1.029082]\n",
            "552 [D loss: 0.728770, acc.: 50.00%] [G loss: 0.906592]\n",
            "553 [D loss: 0.830162, acc.: 46.88%] [G loss: 0.809054]\n",
            "554 [D loss: 0.713492, acc.: 48.44%] [G loss: 0.968097]\n",
            "555 [D loss: 0.806936, acc.: 42.19%] [G loss: 1.102771]\n",
            "556 [D loss: 0.745446, acc.: 51.56%] [G loss: 0.868843]\n",
            "557 [D loss: 0.788300, acc.: 42.19%] [G loss: 0.922857]\n",
            "558 [D loss: 0.798602, acc.: 45.31%] [G loss: 0.945693]\n",
            "559 [D loss: 0.734744, acc.: 51.56%] [G loss: 0.992084]\n",
            "560 [D loss: 0.821537, acc.: 43.75%] [G loss: 0.894855]\n",
            "561 [D loss: 0.743688, acc.: 48.44%] [G loss: 0.875530]\n",
            "562 [D loss: 0.744043, acc.: 51.56%] [G loss: 0.808937]\n",
            "563 [D loss: 0.738133, acc.: 56.25%] [G loss: 1.048986]\n",
            "564 [D loss: 0.744780, acc.: 45.31%] [G loss: 1.122423]\n",
            "565 [D loss: 0.764028, acc.: 45.31%] [G loss: 1.028499]\n",
            "566 [D loss: 0.843320, acc.: 45.31%] [G loss: 0.989885]\n",
            "567 [D loss: 0.806997, acc.: 37.50%] [G loss: 0.915237]\n",
            "568 [D loss: 0.769689, acc.: 48.44%] [G loss: 0.972014]\n",
            "569 [D loss: 0.717445, acc.: 50.00%] [G loss: 1.002466]\n",
            "570 [D loss: 0.746881, acc.: 45.31%] [G loss: 0.937259]\n",
            "571 [D loss: 0.753811, acc.: 43.75%] [G loss: 1.026511]\n",
            "572 [D loss: 0.840968, acc.: 40.62%] [G loss: 0.870540]\n",
            "573 [D loss: 0.800401, acc.: 42.19%] [G loss: 0.897404]\n",
            "574 [D loss: 0.716295, acc.: 51.56%] [G loss: 0.961555]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-26045c747ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-4ceac4632862>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Train the generator (wants discriminator to mistake images as real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91UCFlWkqxBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "cce822b4-d5cd-4e47-c564-4ce3e930760c"
      },
      "source": [
        "!ls images/*\n",
        "img = plt.imread(\"images/mnist_550.png\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images/mnist_0.png    images/mnist_250.png  images/mnist_450.png\n",
            "images/mnist_100.png  images/mnist_300.png  images/mnist_500.png\n",
            "images/mnist_150.png  images/mnist_350.png  images/mnist_50.png\n",
            "images/mnist_200.png  images/mnist_400.png  images/mnist_550.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efcad2c6320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nseQY/x80Fj1lIU/TFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}