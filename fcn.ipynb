{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMif1He4pm7LrcM5YjK9Nv2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "511e2464cfac42788e34e36561972662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_519c2b9048e3482b8d3ceadcffcaca8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_29b5c39fbd1541628dc36c11d32d7b32",
              "IPY_MODEL_1996d053253949e58282a1c9b66caad3"
            ]
          }
        },
        "519c2b9048e3482b8d3ceadcffcaca8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29b5c39fbd1541628dc36c11d32d7b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_755f6d74f31141f7870e8ebd4a3fc18e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdad8cda45054bb3821ca490ee1a2e09"
          }
        },
        "1996d053253949e58282a1c9b66caad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dcf45fa5fc5348e7963a684b95b0f9ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 528M/528M [00:09&lt;00:00, 61.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7adccb9a3c3d4e29a8db7bc3c2e5d6dc"
          }
        },
        "755f6d74f31141f7870e8ebd4a3fc18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdad8cda45054bb3821ca490ee1a2e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcf45fa5fc5348e7963a684b95b0f9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7adccb9a3c3d4e29a8db7bc3c2e5d6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/deep-learning-notebooks/blob/master/fcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VulImwegJ36G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "from PIL import Image\n",
        "import random\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import time\n",
        "import datetime\n",
        "import shutil\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.sampler import Sampler, BatchSampler\n",
        "from __future__ import division\n",
        "from bisect import bisect_right\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from google.colab import drive\n",
        "\n",
        "cur_path = \"/content/awesome-semantic-segmentation-pytorch/\"\n",
        "root_path = os.path.split(cur_path)[0]\n",
        "sys.path.append(root_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1j7PU3dMPzu",
        "colab_type": "code",
        "outputId": "6dbc5822-3499-416b-973a-007b67754a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UHwAYAzoqmnW",
        "colab": {}
      },
      "source": [
        "!pip install ninja tqdm\n",
        "!git clone https://github.com/RoozbehSanaei/awesome-semantic-segmentation-pytorch.git\n",
        "!cd awesome-semantic-segmentation-pytorch/core/nn && python setup.py build develop\n",
        "!cd awesome-semantic-segmentation-pytorch/datasets && wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar && tar -xvf VOCtrainval_11-May-2012.tar\n",
        "!cd awesome-semantic-segmentation-pytorch/datasets  && rm voc && rm *.tar && mv VOCdevkit voc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCIAJroh5qpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "cur_path = \"/content/awesome-semantic-segmentation-pytorch/\"\n",
        "root_path = os.path.split(cur_path)[0]\n",
        "sys.path.append(root_path)\n",
        "\n",
        "\n",
        "from core.models.base_models.vgg import vgg16\n",
        "\n",
        "__all__ = ['get_fcn32s', 'get_fcn16s', 'get_fcn8s',\n",
        "           'get_fcn32s_vgg16_voc', 'get_fcn16s_vgg16_voc', 'get_fcn8s_vgg16_voc']\n",
        "\n",
        "\n",
        "def get_model_file(name, root='/content/gdrive/My Drive/CheckPoints'):\n",
        "    root = os.path.expanduser(root)\n",
        "    file_path = os.path.join('/content/gdrive/My Drive/CheckPoints', name + '.pth')\n",
        "    print(file_path)\n",
        "    if os.path.exists(file_path):\n",
        "        return file_path\n",
        "    else:\n",
        "        raise ValueError('Model file is not found. Downloading or trainning.')\n",
        "\n",
        "\n",
        "class FCN32s(nn.Module):\n",
        "    \"\"\"There are some difference from original fcn\"\"\"\n",
        "\n",
        "    def __init__(self, nclass, backbone='vgg16', aux=False, pretrained_base=True,\n",
        "                 norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(FCN32s, self).__init__()\n",
        "        self.aux = aux\n",
        "        if backbone == 'vgg16':\n",
        "            self.pretrained = vgg16(pretrained=pretrained_base).features\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "        self.head = _FCNHead(512, nclass, norm_layer)\n",
        "        if aux:\n",
        "            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n",
        "\n",
        "        self.__setattr__('exclusive', ['head', 'auxlayer'] if aux else ['head'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "        pool5 = self.pretrained(x)\n",
        "\n",
        "        outputs = []\n",
        "        out = self.head(pool5)\n",
        "        out = F.interpolate(out, size, mode='bilinear', align_corners=True)\n",
        "        outputs.append(out)\n",
        "\n",
        "        if self.aux:\n",
        "            auxout = self.auxlayer(pool5)\n",
        "            auxout = F.interpolate(auxout, size, mode='bilinear', align_corners=True)\n",
        "            outputs.append(auxout)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "\n",
        "class FCN16s(nn.Module):\n",
        "    def __init__(self, nclass, backbone='vgg16', aux=False, pretrained_base=True, norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(FCN16s, self).__init__()\n",
        "        self.aux = aux\n",
        "        if backbone == 'vgg16':\n",
        "            self.pretrained = vgg16(pretrained=pretrained_base).features\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "        self.pool4 = nn.Sequential(*self.pretrained[:24])\n",
        "        self.pool5 = nn.Sequential(*self.pretrained[24:])\n",
        "        self.head = _FCNHead(512, nclass, norm_layer)\n",
        "        self.score_pool4 = nn.Conv2d(512, nclass, 1)\n",
        "        if aux:\n",
        "            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n",
        "\n",
        "        self.__setattr__('exclusive', ['head', 'score_pool4', 'auxlayer'] if aux else ['head', 'score_pool4'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        pool4 = self.pool4(x)\n",
        "        pool5 = self.pool5(pool4)\n",
        "\n",
        "        outputs = []\n",
        "        score_fr = self.head(pool5)\n",
        "\n",
        "        score_pool4 = self.score_pool4(pool4)\n",
        "\n",
        "        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode='bilinear', align_corners=True)\n",
        "        fuse_pool4 = upscore2 + score_pool4\n",
        "\n",
        "        out = F.interpolate(fuse_pool4, x.size()[2:], mode='bilinear', align_corners=True)\n",
        "        outputs.append(out)\n",
        "\n",
        "        if self.aux:\n",
        "            auxout = self.auxlayer(pool5)\n",
        "            auxout = F.interpolate(auxout, x.size()[2:], mode='bilinear', align_corners=True)\n",
        "            outputs.append(auxout)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "\n",
        "class FCN8s(nn.Module):\n",
        "    def __init__(self, nclass, backbone='vgg16', aux=False, pretrained_base=True, norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(FCN8s, self).__init__()\n",
        "        self.aux = aux\n",
        "        if backbone == 'vgg16':\n",
        "            self.pretrained = vgg16(pretrained=pretrained_base).features\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "        self.pool3 = nn.Sequential(*self.pretrained[:17])\n",
        "        self.pool4 = nn.Sequential(*self.pretrained[17:24])\n",
        "        self.pool5 = nn.Sequential(*self.pretrained[24:])\n",
        "        self.head = _FCNHead(512, nclass, norm_layer)\n",
        "        self.score_pool3 = nn.Conv2d(256, nclass, 1)\n",
        "        self.score_pool4 = nn.Conv2d(512, nclass, 1)\n",
        "        if aux:\n",
        "            self.auxlayer = _FCNHead(512, nclass, norm_layer)\n",
        "\n",
        "        self.__setattr__('exclusive',\n",
        "                         ['head', 'score_pool3', 'score_pool4', 'auxlayer'] if aux else ['head', 'score_pool3',\n",
        "                                                                                         'score_pool4'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        pool3 = self.pool3(x)\n",
        "        pool4 = self.pool4(pool3)\n",
        "        pool5 = self.pool5(pool4)\n",
        "\n",
        "        outputs = []\n",
        "        score_fr = self.head(pool5)\n",
        "\n",
        "        score_pool4 = self.score_pool4(pool4)\n",
        "        score_pool3 = self.score_pool3(pool3)\n",
        "\n",
        "        upscore2 = F.interpolate(score_fr, score_pool4.size()[2:], mode='bilinear', align_corners=True)\n",
        "        fuse_pool4 = upscore2 + score_pool4\n",
        "\n",
        "        upscore_pool4 = F.interpolate(fuse_pool4, score_pool3.size()[2:], mode='bilinear', align_corners=True)\n",
        "        fuse_pool3 = upscore_pool4 + score_pool3\n",
        "\n",
        "        out = F.interpolate(fuse_pool3, x.size()[2:], mode='bilinear', align_corners=True)\n",
        "        outputs.append(out)\n",
        "\n",
        "        if self.aux:\n",
        "            auxout = self.auxlayer(pool5)\n",
        "            auxout = F.interpolate(auxout, x.size()[2:], mode='bilinear', align_corners=True)\n",
        "            outputs.append(auxout)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "\n",
        "class _FCNHead(nn.Module):\n",
        "    def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(_FCNHead, self).__init__()\n",
        "        inter_channels = in_channels // 4\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            norm_layer(inter_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "def get_fcn32s(dataset='pascal_voc', backbone='vgg16', pretrained=False, root='~/.torch/models',\n",
        "               pretrained_base=True, **kwargs):\n",
        "    acronyms = {\n",
        "        'pascal_voc': 'pascal_voc',\n",
        "        'pascal_aug': 'pascal_aug',\n",
        "        'ade20k': 'ade',\n",
        "        'coco': 'coco',\n",
        "        'citys': 'citys',\n",
        "    }\n",
        "    from core.data.dataloader import datasets\n",
        "    model = FCN32s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n",
        "    if pretrained:\n",
        "        device = torch.device(kwargs['local_rank'])\n",
        "        model.load_state_dict(torch.load(get_model_file('fcn32s_%s_%s' % (backbone, acronyms[dataset]), root=root),\n",
        "                              map_location=device))\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_fcn16s(dataset='pascal_voc', backbone='vgg16', pretrained=False, root='~/.torch/models',\n",
        "               pretrained_base=True, **kwargs):\n",
        "    acronyms = {\n",
        "        'pascal_voc': 'pascal_voc',\n",
        "        'pascal_aug': 'pascal_aug',\n",
        "        'ade20k': 'ade',\n",
        "        'coco': 'coco',\n",
        "        'citys': 'citys',\n",
        "    }\n",
        "    from ..data.dataloader import datasets\n",
        "    model = FCN16s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n",
        "    if pretrained:\n",
        "        device = torch.device(kwargs['local_rank'])\n",
        "        model.load_state_dict(torch.load(get_model_file('fcn16s_%s_%s' % (backbone, acronyms[dataset]), root=root),\n",
        "                              map_location=device))\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_fcn8s(dataset='pascal_voc', backbone='vgg16', pretrained=False, root='~/.torch/models',\n",
        "              pretrained_base=True, **kwargs):\n",
        "    acronyms = {\n",
        "        'pascal_voc': 'pascal_voc',\n",
        "        'pascal_aug': 'pascal_aug',\n",
        "        'ade20k': 'ade',\n",
        "        'coco': 'coco',\n",
        "        'citys': 'citys',\n",
        "    }\n",
        "    from core.data.dataloader import datasets\n",
        "    model = FCN8s(datasets[dataset].NUM_CLASS, backbone=backbone, pretrained_base=pretrained_base, **kwargs)\n",
        "    if pretrained:\n",
        "        device = torch.device(kwargs['local_rank'])\n",
        "        model.load_state_dict(torch.load(get_model_file('fcn8s_%s_%s' % (backbone, acronyms[dataset]), root=root),\n",
        "                              map_location=device))\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_fcn32s_vgg16_voc(**kwargs):\n",
        "    return get_fcn32s('pascal_voc', 'vgg16', **kwargs)\n",
        "\n",
        "\n",
        "def get_fcn16s_vgg16_voc(**kwargs):\n",
        "    return get_fcn16s('pascal_voc', 'vgg16', **kwargs)\n",
        "\n",
        "\n",
        "def get_fcn8s_vgg16_voc(**kwargs):\n",
        "    return get_fcn8s('pascal_voc', 'vgg16', **kwargs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un4U6lKWP3m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class SegmentationMetric(object):\n",
        "    \"\"\"Computes pixAcc and mIoU metric scores\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nclass):\n",
        "        super(SegmentationMetric, self).__init__()\n",
        "        self.nclass = nclass\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, preds, labels):\n",
        "        \"\"\"Updates the internal evaluation result.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        labels : 'NumpyArray' or list of `NumpyArray`\n",
        "            The labels of the data.\n",
        "        preds : 'NumpyArray' or list of `NumpyArray`\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "\n",
        "        def evaluate_worker(self, pred, label):\n",
        "            correct, labeled = batch_pix_accuracy(pred, label)\n",
        "            inter, union = batch_intersection_union(pred, label, self.nclass)\n",
        "\n",
        "            self.total_correct += correct\n",
        "            self.total_label += labeled\n",
        "            if self.total_inter.device != inter.device:\n",
        "                self.total_inter = self.total_inter.to(inter.device)\n",
        "                self.total_union = self.total_union.to(union.device)\n",
        "            self.total_inter += inter\n",
        "            self.total_union += union\n",
        "\n",
        "        if isinstance(preds, torch.Tensor):\n",
        "            evaluate_worker(self, preds, labels)\n",
        "        elif isinstance(preds, (list, tuple)):\n",
        "            for (pred, label) in zip(preds, labels):\n",
        "                evaluate_worker(self, pred, label)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"Gets the current evaluation result.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        metrics : tuple of float\n",
        "            pixAcc and mIoU\n",
        "        \"\"\"\n",
        "        pixAcc = 1.0 * self.total_correct / (2.220446049250313e-16 + self.total_label)  # remove np.spacing(1)\n",
        "        IoU = 1.0 * self.total_inter / (2.220446049250313e-16 + self.total_union)\n",
        "        mIoU = IoU.mean().item()\n",
        "        return pixAcc, mIoU\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the internal evaluation result to initial state.\"\"\"\n",
        "        self.total_inter = torch.zeros(self.nclass)\n",
        "        self.total_union = torch.zeros(self.nclass)\n",
        "        self.total_correct = 0\n",
        "        self.total_label = 0\n",
        "\n",
        "\n",
        "# pytorch version\n",
        "def batch_pix_accuracy(output, target):\n",
        "    \"\"\"PixAcc\"\"\"\n",
        "    # inputs are numpy array, output 4D, target 3D\n",
        "    predict = torch.argmax(output.long(), 1) + 1\n",
        "    target = target.long() + 1\n",
        "\n",
        "    pixel_labeled = torch.sum(target > 0).item()\n",
        "    pixel_correct = torch.sum((predict == target) * (target > 0)).item()\n",
        "    assert pixel_correct <= pixel_labeled, \"Correct area should be smaller than Labeled\"\n",
        "    return pixel_correct, pixel_labeled\n",
        "\n",
        "\n",
        "def batch_intersection_union(output, target, nclass):\n",
        "    \"\"\"mIoU\"\"\"\n",
        "    # inputs are numpy array, output 4D, target 3D\n",
        "    mini = 1\n",
        "    maxi = nclass\n",
        "    nbins = nclass\n",
        "    predict = torch.argmax(output, 1) + 1\n",
        "    target = target.float() + 1\n",
        "\n",
        "    predict = predict.float() * (target > 0).float()\n",
        "    intersection = predict * (predict == target).float()\n",
        "    # areas of intersection and union\n",
        "    # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.\n",
        "    area_inter = torch.histc(intersection.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_pred = torch.histc(predict.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_lab = torch.histc(target.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_union = area_pred + area_lab - area_inter\n",
        "    assert torch.sum(area_inter > area_union).item() == 0, \"Intersection area should be smaller than Union area\"\n",
        "    return area_inter.float(), area_union.float()\n",
        "\n",
        "\n",
        "def pixelAccuracy(imPred, imLab):\n",
        "    \"\"\"\n",
        "    This function takes the prediction and label of a single image, returns pixel-wise accuracy\n",
        "    To compute over many images do:\n",
        "    for i = range(Nimages):\n",
        "         (pixel_accuracy[i], pixel_correct[i], pixel_labeled[i]) = \\\n",
        "            pixelAccuracy(imPred[i], imLab[i])\n",
        "    mean_pixel_accuracy = 1.0 * np.sum(pixel_correct) / (np.spacing(1) + np.sum(pixel_labeled))\n",
        "    \"\"\"\n",
        "    # Remove classes from unlabeled pixels in gt image.\n",
        "    # We should not penalize detections in unlabeled portions of the image.\n",
        "    pixel_labeled = np.sum(imLab >= 0)\n",
        "    pixel_correct = np.sum((imPred == imLab) * (imLab >= 0))\n",
        "    pixel_accuracy = 1.0 * pixel_correct / pixel_labeled\n",
        "    return (pixel_accuracy, pixel_correct, pixel_labeled)\n",
        "\n",
        "\n",
        "def intersectionAndUnion(imPred, imLab, numClass):\n",
        "    \"\"\"\n",
        "    This function takes the prediction and label of a single image,\n",
        "    returns intersection and union areas for each class\n",
        "    To compute over many images do:\n",
        "    for i in range(Nimages):\n",
        "        (area_intersection[:,i], area_union[:,i]) = intersectionAndUnion(imPred[i], imLab[i])\n",
        "    IoU = 1.0 * np.sum(area_intersection, axis=1) / np.sum(np.spacing(1)+area_union, axis=1)\n",
        "    \"\"\"\n",
        "    # Remove classes from unlabeled pixels in gt image.\n",
        "    # We should not penalize detections in unlabeled portions of the image.\n",
        "    imPred = imPred * (imLab >= 0)\n",
        "\n",
        "    # Compute area intersection:\n",
        "    intersection = imPred * (imPred == imLab)\n",
        "    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n",
        "\n",
        "    # Compute area union:\n",
        "    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n",
        "    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n",
        "    area_union = area_pred + area_lab - area_intersection\n",
        "    return (area_intersection, area_union)\n",
        "\n",
        "\n",
        "def hist_info(pred, label, num_cls):\n",
        "    assert pred.shape == label.shape\n",
        "    k = (label >= 0) & (label < num_cls)\n",
        "    labeled = np.sum(k)\n",
        "    correct = np.sum((pred[k] == label[k]))\n",
        "\n",
        "    return np.bincount(num_cls * label[k].astype(int) + pred[k], minlength=num_cls ** 2).reshape(num_cls,\n",
        "                                                                                                 num_cls), labeled, correct\n",
        "\n",
        "\n",
        "def compute_score(hist, correct, labeled):\n",
        "    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
        "    mean_IU = np.nanmean(iu)\n",
        "    mean_IU_no_back = np.nanmean(iu[1:])\n",
        "    freq = hist.sum(1) / hist.sum()\n",
        "    freq_IU = (iu[freq > 0] * freq[freq > 0]).sum()\n",
        "    mean_pixel_acc = correct / labeled\n",
        "\n",
        "    return iu, mean_IU, mean_IU_no_back, mean_pixel_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6iCyMpzTzgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# TODO: optim function\n",
        "class MixSoftmaxCrossEntropyLoss(nn.CrossEntropyLoss):\n",
        "    def __init__(self, aux=True, aux_weight=0.2, ignore_index=-1, **kwargs):\n",
        "        super(MixSoftmaxCrossEntropyLoss, self).__init__(ignore_index=ignore_index)\n",
        "        self.aux = aux\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "    def _aux_forward(self, *inputs, **kwargs):\n",
        "        *preds, target = tuple(inputs)\n",
        "\n",
        "        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n",
        "        for i in range(1, len(preds)):\n",
        "            aux_loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n",
        "            loss += self.aux_weight * aux_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, *inputs, **kwargs):\n",
        "        preds, target = tuple(inputs)\n",
        "        inputs = tuple(list(preds) + [target])\n",
        "        if self.aux:\n",
        "            return dict(loss=self._aux_forward(*inputs))\n",
        "        else:\n",
        "            return dict(loss=super(MixSoftmaxCrossEntropyLoss, self).forward(*inputs))\n",
        "\n",
        "\n",
        "# reference: https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/loss.py\n",
        "class EncNetLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"2D Cross Entropy Loss with SE Loss\"\"\"\n",
        "\n",
        "    def __init__(self, se_loss=True, se_weight=0.2, nclass=19, aux=False,\n",
        "                 aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n",
        "        super(EncNetLoss, self).__init__(weight, None, ignore_index)\n",
        "        self.se_loss = se_loss\n",
        "        self.aux = aux\n",
        "        self.nclass = nclass\n",
        "        self.se_weight = se_weight\n",
        "        self.aux_weight = aux_weight\n",
        "        self.bceloss = nn.BCELoss(weight)\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        preds, target = tuple(inputs)\n",
        "        inputs = tuple(list(preds) + [target])\n",
        "        if not self.se_loss and not self.aux:\n",
        "            return super(EncNetLoss, self).forward(*inputs)\n",
        "        elif not self.se_loss:\n",
        "            pred1, pred2, target = tuple(inputs)\n",
        "            loss1 = super(EncNetLoss, self).forward(pred1, target)\n",
        "            loss2 = super(EncNetLoss, self).forward(pred2, target)\n",
        "            return dict(loss=loss1 + self.aux_weight * loss2)\n",
        "        elif not self.aux:\n",
        "            pred, se_pred, target = tuple(inputs)\n",
        "            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred)\n",
        "            loss1 = super(EncNetLoss, self).forward(pred, target)\n",
        "            loss2 = self.bceloss(torch.sigmoid(se_pred), se_target)\n",
        "            return dict(loss=loss1 + self.se_weight * loss2)\n",
        "        else:\n",
        "            pred1, se_pred, pred2, target = tuple(inputs)\n",
        "            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred1)\n",
        "            loss1 = super(EncNetLoss, self).forward(pred1, target)\n",
        "            loss2 = super(EncNetLoss, self).forward(pred2, target)\n",
        "            loss3 = self.bceloss(torch.sigmoid(se_pred), se_target)\n",
        "            return dict(loss=loss1 + self.aux_weight * loss2 + self.se_weight * loss3)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_batch_label_vector(target, nclass):\n",
        "        # target is a 3D Variable BxHxW, output is 2D BxnClass\n",
        "        batch = target.size(0)\n",
        "        tvect = Variable(torch.zeros(batch, nclass))\n",
        "        for i in range(batch):\n",
        "            hist = torch.histc(target[i].cpu().data.float(),\n",
        "                               bins=nclass, min=0,\n",
        "                               max=nclass - 1)\n",
        "            vect = hist > 0\n",
        "            tvect[i] = vect\n",
        "        return tvect\n",
        "\n",
        "\n",
        "# TODO: optim function\n",
        "class ICNetLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"Cross Entropy Loss for ICNet\"\"\"\n",
        "\n",
        "    def __init__(self, nclass, aux_weight=0.4, ignore_index=-1, **kwargs):\n",
        "        super(ICNetLoss, self).__init__(ignore_index=ignore_index)\n",
        "        self.nclass = nclass\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        preds, target = tuple(inputs)\n",
        "        inputs = tuple(list(preds) + [target])\n",
        "\n",
        "        pred, pred_sub4, pred_sub8, pred_sub16, target = tuple(inputs)\n",
        "        # [batch, W, H] -> [batch, 1, W, H]\n",
        "        target = target.unsqueeze(1).float()\n",
        "        target_sub4 = F.interpolate(target, pred_sub4.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
        "        target_sub8 = F.interpolate(target, pred_sub8.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
        "        target_sub16 = F.interpolate(target, pred_sub16.size()[2:], mode='bilinear', align_corners=True).squeeze(\n",
        "            1).long()\n",
        "        loss1 = super(ICNetLoss, self).forward(pred_sub4, target_sub4)\n",
        "        loss2 = super(ICNetLoss, self).forward(pred_sub8, target_sub8)\n",
        "        loss3 = super(ICNetLoss, self).forward(pred_sub16, target_sub16)\n",
        "        return dict(loss=loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight)\n",
        "\n",
        "\n",
        "class OhemCrossEntropy2d(nn.Module):\n",
        "    def __init__(self, ignore_index=-1, thresh=0.7, min_kept=100000, use_weight=True, **kwargs):\n",
        "        super(OhemCrossEntropy2d, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.thresh = float(thresh)\n",
        "        self.min_kept = int(min_kept)\n",
        "        if use_weight:\n",
        "            weight = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754,\n",
        "                                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n",
        "                                        1.0865, 1.1529, 1.0507])\n",
        "            self.criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n",
        "        else:\n",
        "            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n, c, h, w = pred.size()\n",
        "        target = target.view(-1)\n",
        "        valid_mask = target.ne(self.ignore_index)\n",
        "        target = target * valid_mask.long()\n",
        "        num_valid = valid_mask.sum()\n",
        "\n",
        "        prob = F.softmax(pred, dim=1)\n",
        "        prob = prob.transpose(0, 1).reshape(c, -1)\n",
        "\n",
        "        if self.min_kept > num_valid:\n",
        "            print(\"Lables: {}\".format(num_valid))\n",
        "        elif num_valid > 0:\n",
        "            prob = prob.masked_fill_(1 - valid_mask, 1)\n",
        "            mask_prob = prob[target, torch.arange(len(target), dtype=torch.long)]\n",
        "            threshold = self.thresh\n",
        "            if self.min_kept > 0:\n",
        "                index = mask_prob.argsort()\n",
        "                threshold_index = index[min(len(index), self.min_kept) - 1]\n",
        "                if mask_prob[threshold_index] > self.thresh:\n",
        "                    threshold = mask_prob[threshold_index]\n",
        "            kept_mask = mask_prob.le(threshold)\n",
        "            valid_mask = valid_mask * kept_mask\n",
        "            target = target * kept_mask.long()\n",
        "\n",
        "        target = target.masked_fill_(1 - valid_mask, self.ignore_index)\n",
        "        target = target.view(n, h, w)\n",
        "\n",
        "        return self.criterion(pred, target)\n",
        "\n",
        "\n",
        "class MixSoftmaxCrossEntropyOHEMLoss(OhemCrossEntropy2d):\n",
        "    def __init__(self, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n",
        "        super(MixSoftmaxCrossEntropyOHEMLoss, self).__init__(ignore_index=ignore_index)\n",
        "        self.aux = aux\n",
        "        self.aux_weight = aux_weight\n",
        "        self.bceloss = nn.BCELoss(weight)\n",
        "\n",
        "    def _aux_forward(self, *inputs, **kwargs):\n",
        "        *preds, target = tuple(inputs)\n",
        "\n",
        "        loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[0], target)\n",
        "        for i in range(1, len(preds)):\n",
        "            aux_loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[i], target)\n",
        "            loss += self.aux_weight * aux_loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        preds, target = tuple(inputs)\n",
        "        inputs = tuple(list(preds) + [target])\n",
        "        if self.aux:\n",
        "            return dict(loss=self._aux_forward(*inputs))\n",
        "        else:\n",
        "            return dict(loss=super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(*inputs))\n",
        "\n",
        "\n",
        "def get_segmentation_loss(model, use_ohem=False, **kwargs):\n",
        "    if use_ohem:\n",
        "        return MixSoftmaxCrossEntropyOHEMLoss(**kwargs)\n",
        "\n",
        "    model = model.lower()\n",
        "    if model == 'encnet':\n",
        "        return EncNetLoss(**kwargs)\n",
        "    elif model == 'icnet':\n",
        "        return ICNetLoss(**kwargs)\n",
        "    else:\n",
        "        return MixSoftmaxCrossEntropyLoss(**kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hlt_CVnUNCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LRScheduler(object):\n",
        "    r\"\"\"Learning Rate Scheduler\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mode : str\n",
        "        Modes for learning rate scheduler.\n",
        "        Currently it supports 'constant', 'step', 'linear', 'poly' and 'cosine'.\n",
        "    base_lr : float\n",
        "        Base learning rate, i.e. the starting learning rate.\n",
        "    target_lr : float\n",
        "        Target learning rate, i.e. the ending learning rate.\n",
        "        With constant mode target_lr is ignored.\n",
        "    niters : int\n",
        "        Number of iterations to be scheduled.\n",
        "    nepochs : int\n",
        "        Number of epochs to be scheduled.\n",
        "    iters_per_epoch : int\n",
        "        Number of iterations in each epoch.\n",
        "    offset : int\n",
        "        Number of iterations before this scheduler.\n",
        "    power : float\n",
        "        Power parameter of poly scheduler.\n",
        "    step_iter : list\n",
        "        A list of iterations to decay the learning rate.\n",
        "    step_epoch : list\n",
        "        A list of epochs to decay the learning rate.\n",
        "    step_factor : float\n",
        "        Learning rate decay factor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode, base_lr=0.01, target_lr=0, niters=0, nepochs=0, iters_per_epoch=0,\n",
        "                 offset=0, power=0.9, step_iter=None, step_epoch=None, step_factor=0.1, warmup_epochs=0):\n",
        "        super(LRScheduler, self).__init__()\n",
        "        assert (mode in ['constant', 'step', 'linear', 'poly', 'cosine'])\n",
        "\n",
        "        if mode == 'step':\n",
        "            assert (step_iter is not None or step_epoch is not None)\n",
        "        self.niters = niters\n",
        "        self.step = step_iter\n",
        "        epoch_iters = nepochs * iters_per_epoch\n",
        "        if epoch_iters > 0:\n",
        "            self.niters = epoch_iters\n",
        "            if step_epoch is not None:\n",
        "                self.step = [s * iters_per_epoch for s in step_epoch]\n",
        "\n",
        "        self.step_factor = step_factor\n",
        "        self.base_lr = base_lr\n",
        "        self.target_lr = base_lr if mode == 'constant' else target_lr\n",
        "        self.offset = offset\n",
        "        self.power = power\n",
        "        self.warmup_iters = warmup_epochs * iters_per_epoch\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, optimizer, num_update):\n",
        "        self.update(num_update)\n",
        "        assert self.learning_rate >= 0\n",
        "        self._adjust_learning_rate(optimizer, self.learning_rate)\n",
        "\n",
        "    def update(self, num_update):\n",
        "        N = self.niters - 1\n",
        "        T = num_update - self.offset\n",
        "        T = min(max(0, T), N)\n",
        "\n",
        "        if self.mode == 'constant':\n",
        "            factor = 0\n",
        "        elif self.mode == 'linear':\n",
        "            factor = 1 - T / N\n",
        "        elif self.mode == 'poly':\n",
        "            factor = pow(1 - T / N, self.power)\n",
        "        elif self.mode == 'cosine':\n",
        "            factor = (1 + math.cos(math.pi * T / N)) / 2\n",
        "        elif self.mode == 'step':\n",
        "            if self.step is not None:\n",
        "                count = sum([1 for s in self.step if s <= T])\n",
        "                factor = pow(self.step_factor, count)\n",
        "            else:\n",
        "                factor = 1\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # warm up lr schedule\n",
        "        if self.warmup_iters > 0 and T < self.warmup_iters:\n",
        "            factor = factor * 1.0 * T / self.warmup_iters\n",
        "\n",
        "        if self.mode == 'step':\n",
        "            self.learning_rate = self.base_lr * factor\n",
        "        else:\n",
        "            self.learning_rate = self.target_lr + (self.base_lr - self.target_lr) * factor\n",
        "\n",
        "    def _adjust_learning_rate(self, optimizer, lr):\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "        # enlarge the lr at the head\n",
        "        for i in range(1, len(optimizer.param_groups)):\n",
        "            optimizer.param_groups[i]['lr'] = lr * 10\n",
        "\n",
        "\n",
        "# separating MultiStepLR with WarmupLR\n",
        "# but the current LRScheduler design doesn't allow it\n",
        "# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/solver/lr_scheduler.py\n",
        "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n",
        "                 warmup_iters=500, warmup_method=\"linear\", last_epoch=-1):\n",
        "        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n",
        "        if not list(milestones) == sorted(milestones):\n",
        "            raise ValueError(\n",
        "                \"Milestones should be a list of\" \" increasing integers. Got {}\", milestones)\n",
        "        if warmup_method not in (\"constant\", \"linear\"):\n",
        "            raise ValueError(\n",
        "                \"Only 'constant' or 'linear' warmup_method accepted got {}\".format(warmup_method))\n",
        "\n",
        "        self.milestones = milestones\n",
        "        self.gamma = gamma\n",
        "        self.warmup_factor = warmup_factor\n",
        "        self.warmup_iters = warmup_iters\n",
        "        self.warmup_method = warmup_method\n",
        "\n",
        "    def get_lr(self):\n",
        "        warmup_factor = 1\n",
        "        if self.last_epoch < self.warmup_iters:\n",
        "            if self.warmup_method == 'constant':\n",
        "                warmup_factor = self.warmup_factor\n",
        "            elif self.warmup_factor == 'linear':\n",
        "                alpha = float(self.last_epoch) / self.warmup_iters\n",
        "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n",
        "        return [base_lr * warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n",
        "                for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class WarmupPolyLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, target_lr=0, max_iters=0, power=0.9, warmup_factor=1.0 / 3,\n",
        "                 warmup_iters=500, warmup_method='linear', last_epoch=-1):\n",
        "        if warmup_method not in (\"constant\", \"linear\"):\n",
        "            raise ValueError(\n",
        "                \"Only 'constant' or 'linear' warmup_method accepted \"\n",
        "                \"got {}\".format(warmup_method))\n",
        "\n",
        "        self.target_lr = target_lr\n",
        "        self.max_iters = max_iters\n",
        "        self.power = power\n",
        "        self.warmup_factor = warmup_factor\n",
        "        self.warmup_iters = warmup_iters\n",
        "        self.warmup_method = warmup_method\n",
        "\n",
        "        super(WarmupPolyLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        N = self.max_iters - self.warmup_iters\n",
        "        T = self.last_epoch - self.warmup_iters\n",
        "        if self.last_epoch < self.warmup_iters:\n",
        "            if self.warmup_method == 'constant':\n",
        "                warmup_factor = self.warmup_factor\n",
        "            elif self.warmup_method == 'linear':\n",
        "                alpha = float(self.last_epoch) / self.warmup_iters\n",
        "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n",
        "            else:\n",
        "                raise ValueError(\"Unknown warmup type.\")\n",
        "            return [self.target_lr + (base_lr - self.target_lr) * warmup_factor for base_lr in self.base_lrs]\n",
        "        factor = pow(1 - T / N, self.power)\n",
        "        return [self.target_lr + (base_lr - self.target_lr) * factor for base_lr in self.base_lrs]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnnr1i8KgPSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reference: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/utils/comm.py\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def synchronize():\n",
        "    \"\"\"\n",
        "    Helper function to synchronize (barrier) among all processes when\n",
        "    using distributed training\n",
        "    \"\"\"\n",
        "    if not dist.is_available():\n",
        "        return\n",
        "    if not dist.is_initialized():\n",
        "        return\n",
        "    world_size = dist.get_world_size()\n",
        "    if world_size == 1:\n",
        "        return\n",
        "    dist.barrier()\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.IntTensor([tensor.numel()]).to(\"cuda\")\n",
        "    size_list = [torch.IntTensor([0]).to(\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.ByteTensor(size=(max_size - local_size,)).to(\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that process with rank\n",
        "    0 has the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.reduce(values, dst=0)\n",
        "        if dist.get_rank() == 0 and average:\n",
        "            # only main process gets accumulated, so only divide by\n",
        "            # world_size in this case\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "def reduce_loss_dict(loss_dict):\n",
        "    \"\"\"\n",
        "    Reduce the loss dictionary from all processes so that process with rank\n",
        "    0 has the averaged results. Returns a dict with the same fields as\n",
        "    loss_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return loss_dict\n",
        "    with torch.no_grad():\n",
        "        loss_names = []\n",
        "        all_losses = []\n",
        "        for k in sorted(loss_dict.keys()):\n",
        "            loss_names.append(k)\n",
        "            all_losses.append(loss_dict[k])\n",
        "        all_losses = torch.stack(all_losses, dim=0)\n",
        "        dist.reduce(all_losses, dst=0)\n",
        "        if dist.get_rank() == 0:\n",
        "            # only main process gets accumulated, so only divide by\n",
        "            # world_size in this case\n",
        "            all_losses /= world_size\n",
        "        reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\n",
        "    return reduced_losses\n",
        "\n",
        "\n",
        "def make_data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return DistributedSampler(dataset, shuffle=shuffle)\n",
        "    if shuffle:\n",
        "        sampler = data.sampler.RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = data.sampler.SequentialSampler(dataset)\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def make_batch_data_sampler(sampler, images_per_batch, num_iters=None, start_iter=0):\n",
        "    batch_sampler = data.sampler.BatchSampler(sampler, images_per_batch, drop_last=True)\n",
        "    if num_iters is not None:\n",
        "        batch_sampler = IterationBasedBatchSampler(batch_sampler, num_iters, start_iter)\n",
        "    return batch_sampler\n",
        "\n",
        "\n",
        "# Code is copy-pasted from https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/samplers/distributed.py\n",
        "class DistributedSampler(Sampler):\n",
        "    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n",
        "    It is especially useful in conjunction with\n",
        "    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n",
        "    process can pass a DistributedSampler instance as a DataLoader sampler,\n",
        "    and load a subset of the original dataset that is exclusive to it.\n",
        "    .. note::\n",
        "        Dataset is assumed to be of constant size.\n",
        "    Arguments:\n",
        "        dataset: Dataset used for sampling.\n",
        "        num_replicas (optional): Number of processes participating in\n",
        "            distributed training.\n",
        "        rank (optional): Rank of the current process within num_replicas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n",
        "        if num_replicas is None:\n",
        "            if not dist.is_available():\n",
        "                raise RuntimeError(\"Requires distributed package to be available\")\n",
        "            num_replicas = dist.get_world_size()\n",
        "        if rank is None:\n",
        "            if not dist.is_available():\n",
        "                raise RuntimeError(\"Requires distributed package to be available\")\n",
        "            rank = dist.get_rank()\n",
        "        self.dataset = dataset\n",
        "        self.num_replicas = num_replicas\n",
        "        self.rank = rank\n",
        "        self.epoch = 0\n",
        "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
        "        self.total_size = self.num_samples * self.num_replicas\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            # deterministically shuffle based on epoch\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(self.epoch)\n",
        "            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n",
        "        else:\n",
        "            indices = torch.arange(len(self.dataset)).tolist()\n",
        "\n",
        "        # add extra samples to make it evenly divisible\n",
        "        indices += indices[: (self.total_size - len(indices))]\n",
        "        assert len(indices) == self.total_size\n",
        "\n",
        "        # subsample\n",
        "        offset = self.num_samples * self.rank\n",
        "        indices = indices[offset: offset + self.num_samples]\n",
        "        assert len(indices) == self.num_samples\n",
        "\n",
        "        return iter(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch\n",
        "\n",
        "\n",
        "class IterationBasedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    Wraps a BatchSampler, resampling from it until\n",
        "    a specified number of iterations have been sampled\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n",
        "        self.batch_sampler = batch_sampler\n",
        "        self.num_iterations = num_iterations\n",
        "        self.start_iter = start_iter\n",
        "\n",
        "    def __iter__(self):\n",
        "        iteration = self.start_iter\n",
        "        while iteration <= self.num_iterations:\n",
        "            # if the underlying sampler has a set_epoch method, like\n",
        "            # DistributedSampler, used for making each process see\n",
        "            # a different split of the dataset, then set it\n",
        "            if hasattr(self.batch_sampler.sampler, \"set_epoch\"):\n",
        "                self.batch_sampler.sampler.set_epoch(iteration)\n",
        "            for batch in self.batch_sampler:\n",
        "                iteration += 1\n",
        "                if iteration > self.num_iterations:\n",
        "                    break\n",
        "                yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_iterations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBJraD5bhN8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SegmentationDataset(object):\n",
        "    \"\"\"Segmentation Base Dataset\"\"\"\n",
        "\n",
        "    def __init__(self, root, split, mode, transform, base_size=520, crop_size=480):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.mode = mode if mode is not None else split\n",
        "        self.base_size = base_size\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def _val_sync_transform(self, img, mask):\n",
        "        outsize = self.crop_size\n",
        "        short_size = outsize\n",
        "        w, h = img.size\n",
        "        if w > h:\n",
        "            oh = short_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        else:\n",
        "            ow = short_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # center crop\n",
        "        w, h = img.size\n",
        "        x1 = int(round((w - outsize) / 2.))\n",
        "        y1 = int(round((h - outsize) / 2.))\n",
        "        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
        "        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
        "        # final transform\n",
        "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        return img, mask\n",
        "\n",
        "    def _sync_transform(self, img, mask):\n",
        "        # random mirror\n",
        "        if random.random() < 0.5:\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        crop_size = self.crop_size\n",
        "        # random scale (short edge)\n",
        "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
        "        w, h = img.size\n",
        "        if h > w:\n",
        "            ow = short_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        else:\n",
        "            oh = short_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # pad crop\n",
        "        if short_size < crop_size:\n",
        "            padh = crop_size - oh if oh < crop_size else 0\n",
        "            padw = crop_size - ow if ow < crop_size else 0\n",
        "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
        "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)\n",
        "        # random crop crop_size\n",
        "        w, h = img.size\n",
        "        x1 = random.randint(0, w - crop_size)\n",
        "        y1 = random.randint(0, h - crop_size)\n",
        "        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
        "        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
        "        # gaussian blur as in PSP\n",
        "        if random.random() < 0.5:\n",
        "            img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))\n",
        "        # final transform\n",
        "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        return img, mask\n",
        "\n",
        "    def _img_transform(self, img):\n",
        "        return np.array(img)\n",
        "\n",
        "    def _mask_transform(self, mask):\n",
        "        return np.array(mask).astype('int32')\n",
        "\n",
        "    @property\n",
        "    def num_class(self):\n",
        "        \"\"\"Number of categories.\"\"\"\n",
        "        return self.NUM_CLASS\n",
        "\n",
        "    @property\n",
        "    def pred_offset(self):\n",
        "        return 0\n",
        "\n",
        "\n",
        "class VOCSegmentation(SegmentationDataset):\n",
        "    \"\"\"Pascal VOC Semantic Segmentation Dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : string\n",
        "        Path to VOCdevkit folder. Default is './datasets/VOCdevkit'\n",
        "    split: string\n",
        "        'train', 'val' or 'test'\n",
        "    transform : callable, optional\n",
        "        A function that transforms the image\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from torchvision import transforms\n",
        "    >>> import torch.utils.data as data\n",
        "    >>> # Transforms for Normalization\n",
        "    >>> input_transform = transforms.Compose([\n",
        "    >>>     transforms.ToTensor(),\n",
        "    >>>     transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "    >>> ])\n",
        "    >>> # Create Dataset\n",
        "    >>> trainset = VOCSegmentation(split='train', transform=input_transform)\n",
        "    >>> # Create Training Loader\n",
        "    >>> train_data = data.DataLoader(\n",
        "    >>>     trainset, 4, shuffle=True,\n",
        "    >>>     num_workers=4)\n",
        "    \"\"\"\n",
        "    BASE_DIR = 'VOC2012'\n",
        "    NUM_CLASS = 21\n",
        "\n",
        "    def __init__(self, root='/content/awesome-semantic-segmentation-pytorch/datasets/voc', split='train', mode=None, transform=None, **kwargs):\n",
        "        print(root)\n",
        "        super(VOCSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n",
        "        _voc_root = os.path.join(root, self.BASE_DIR)\n",
        "        _mask_dir = os.path.join(_voc_root, 'SegmentationClass')\n",
        "        _image_dir = os.path.join(_voc_root, 'JPEGImages')\n",
        "        # train/val/test splits are pre-cut\n",
        "        _splits_dir = os.path.join(_voc_root, 'ImageSets/Segmentation')\n",
        "        if split == 'train':\n",
        "            _split_f = os.path.join(_splits_dir, 'train.txt')\n",
        "        elif split == 'val':\n",
        "            _split_f = os.path.join(_splits_dir, 'val.txt')\n",
        "        elif split == 'test':\n",
        "            _split_f = os.path.join(_splits_dir, 'test.txt')\n",
        "        else:\n",
        "            raise RuntimeError('Unknown dataset split.')\n",
        "\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        with open(os.path.join(_split_f), \"r\") as lines:\n",
        "            for line in lines:\n",
        "                _image = os.path.join(_image_dir, line.rstrip('\\n') + \".jpg\")\n",
        "                assert os.path.isfile(_image)\n",
        "                self.images.append(_image)\n",
        "                if split != 'test':\n",
        "                    _mask = os.path.join(_mask_dir, line.rstrip('\\n') + \".png\")\n",
        "                    assert os.path.isfile(_mask)\n",
        "                    self.masks.append(_mask)\n",
        "\n",
        "        if split != 'test':\n",
        "            assert (len(self.images) == len(self.masks))\n",
        "        print('Found {} images in the folder {}'.format(len(self.images), _voc_root))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.mode == 'test':\n",
        "            img = self._img_transform(img)\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            return img, os.path.basename(self.images[index])\n",
        "        mask = Image.open(self.masks[index])\n",
        "        # synchronized transform\n",
        "        if self.mode == 'train':\n",
        "            img, mask = self._sync_transform(img, mask)\n",
        "        elif self.mode == 'val':\n",
        "            img, mask = self._val_sync_transform(img, mask)\n",
        "        else:\n",
        "            assert self.mode == 'testval'\n",
        "            img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        # general resize, normalize and toTensor\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, mask, os.path.basename(self.images[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def _mask_transform(self, mask):\n",
        "        target = np.array(mask).astype('int32')\n",
        "        target[target == 255] = -1\n",
        "        return torch.from_numpy(target).long()\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        \"\"\"Category names.\"\"\"\n",
        "        return ('background', 'airplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
        "                'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
        "                'motorcycle', 'person', 'potted-plant', 'sheep', 'sofa', 'train',\n",
        "                'tv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp1aHDhxjuCF",
        "colab_type": "code",
        "outputId": "0285e551-6aa3-4d29-d147-31400113a726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623,
          "referenced_widgets": [
            "511e2464cfac42788e34e36561972662",
            "519c2b9048e3482b8d3ceadcffcaca8d",
            "29b5c39fbd1541628dc36c11d32d7b32",
            "1996d053253949e58282a1c9b66caad3",
            "755f6d74f31141f7870e8ebd4a3fc18e",
            "cdad8cda45054bb3821ca490ee1a2e09",
            "dcf45fa5fc5348e7963a684b95b0f9ba",
            "7adccb9a3c3d4e29a8db7bc3c2e5d6dc"
          ]
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device(args.device)\n",
        "\n",
        "        # image transform\n",
        "        input_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "        ])\n",
        "        # dataset and dataloader\n",
        "        data_kwargs = {'transform': input_transform, 'base_size': args.base_size, 'crop_size': args.crop_size}\n",
        "        train_dataset = VOCSegmentation(split='train', mode='train', **data_kwargs)\n",
        "        val_dataset = VOCSegmentation(split='val', mode='val', **data_kwargs)\n",
        "        args.iters_per_epoch = len(train_dataset) // (args.num_gpus * args.batch_size)\n",
        "        args.max_iters = args.epochs * args.iters_per_epoch\n",
        "\n",
        "        train_sampler = make_data_sampler(train_dataset, shuffle=True, distributed=args.distributed)\n",
        "        train_batch_sampler = make_batch_data_sampler(train_sampler, args.batch_size, args.max_iters)\n",
        "        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n",
        "        val_batch_sampler = make_batch_data_sampler(val_sampler, args.batch_size)\n",
        "\n",
        "        self.train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_sampler=train_batch_sampler,\n",
        "                                            num_workers=args.workers,\n",
        "                                            pin_memory=True)\n",
        "        self.val_loader = data.DataLoader(dataset=val_dataset,\n",
        "                                          batch_sampler=val_batch_sampler,\n",
        "                                          num_workers=args.workers,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "        # create network\n",
        "        BatchNorm2d = nn.SyncBatchNorm if args.distributed else nn.BatchNorm2d\n",
        "        self.model = get_fcn32s(model=args.model, dataset=args.dataset, backbone=args.backbone,\n",
        "                                            aux=args.aux, jpu=args.jpu, norm_layer=BatchNorm2d).to(self.device)\n",
        "\n",
        "        # resume checkpoint if needed\n",
        "        if args.resume:\n",
        "            if os.path.isfile(args.resume):\n",
        "                name, ext = os.path.splitext(args.resume)\n",
        "                assert ext == '.pkl' or '.pth', 'Sorry only .pth and .pkl files supported.'\n",
        "                print('Resuming training, loading {}...'.format(args.resume))\n",
        "                self.model.load_state_dict(torch.load(args.resume, map_location=lambda storage, loc: storage))\n",
        "\n",
        "        # create criterion\n",
        "        self.criterion = get_segmentation_loss(args.model, use_ohem=args.use_ohem, aux=args.aux,\n",
        "                                               aux_weight=args.aux_weight, ignore_index=-1).to(self.device)\n",
        "\n",
        "        # optimizer, for model just includes pretrained, head and auxlayer\n",
        "        params_list = list()\n",
        "        if hasattr(self.model, 'pretrained'):\n",
        "            params_list.append({'params': self.model.pretrained.parameters(), 'lr': args.lr})\n",
        "        if hasattr(self.model, 'exclusive'):\n",
        "            for module in self.model.exclusive:\n",
        "                params_list.append({'params': getattr(self.model, module).parameters(), 'lr': args.lr * 10})\n",
        "        self.optimizer = torch.optim.SGD(params_list,\n",
        "                                         lr=args.lr,\n",
        "                                         momentum=args.momentum,\n",
        "                                         weight_decay=args.weight_decay)\n",
        "\n",
        "        # lr scheduling\n",
        "        self.lr_scheduler = WarmupPolyLR(self.optimizer,\n",
        "                                         max_iters=args.max_iters,\n",
        "                                         power=0.9,\n",
        "                                         warmup_factor=args.warmup_factor,\n",
        "                                         warmup_iters=args.warmup_iters,\n",
        "                                         warmup_method=args.warmup_method)\n",
        "\n",
        "        if args.distributed:\n",
        "            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[args.local_rank],\n",
        "                                                             output_device=args.local_rank)\n",
        "\n",
        "        # evaluation metrics\n",
        "        self.metric = SegmentationMetric(train_dataset.num_class)\n",
        "\n",
        "        self.best_pred = 0.0\n",
        "\n",
        "    def train(self):\n",
        "        save_to_disk = get_rank() == 0\n",
        "        epochs, max_iters = self.args.epochs, self.args.max_iters\n",
        "        log_per_iters, val_per_iters = self.args.log_iter, self.args.val_epoch * self.args.iters_per_epoch\n",
        "        save_per_iters = self.args.save_epoch * self.args.iters_per_epoch\n",
        "        start_time = time.time()\n",
        "        print('Start training, Total Epochs: {:d} = Total Iterations {:d}'.format(epochs, max_iters))\n",
        "\n",
        "        self.model.train()\n",
        "        for iteration, (images, targets, _) in enumerate(self.train_loader):\n",
        "            iteration = iteration + 1\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "\n",
        "            outputs = self.model(images)\n",
        "            loss_dict = self.criterion(outputs, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            # reduce losses over all GPUs for logging purposes\n",
        "            loss_dict_reduced = reduce_loss_dict(loss_dict)\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            eta_seconds = ((time.time() - start_time) / iteration) * (max_iters - iteration)\n",
        "            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "\n",
        "            if iteration % log_per_iters == 0 and save_to_disk:\n",
        "                print(\n",
        "                    \"Iters: {:d}/{:d} || Lr: {:.6f} || Loss: {:.4f} || Cost Time: {} || Estimated Time: {}\".format(\n",
        "                        iteration, max_iters, self.optimizer.param_groups[0]['lr'], losses_reduced.item(),\n",
        "                        str(datetime.timedelta(seconds=int(time.time() - start_time))), eta_string))\n",
        "\n",
        "            if iteration % save_per_iters == 0 and save_to_disk:\n",
        "                save_checkpoint(self.model, self.args, is_best=False)\n",
        "\n",
        "            if not self.args.skip_val and iteration % val_per_iters == 0:\n",
        "                self.validation()\n",
        "                self.model.train()\n",
        "\n",
        "        save_checkpoint(self.model, self.args, is_best=False)\n",
        "        total_training_time = time.time() - start_time\n",
        "        total_training_str = str(datetime.timedelta(seconds=total_training_time))\n",
        "        print(\n",
        "            \"Total training time: {} ({:.4f}s / it)\".format(\n",
        "                total_training_str, total_training_time / max_iters))\n",
        "\n",
        "    def validation(self):\n",
        "        # total_inter, total_union, total_correct, total_label = 0, 0, 0, 0\n",
        "        is_best = False\n",
        "        self.metric.reset()\n",
        "        if self.args.distributed:\n",
        "            model = self.model.module\n",
        "        else:\n",
        "            model = self.model\n",
        "        torch.cuda.empty_cache()  # TODO check if it helps\n",
        "        model.eval()\n",
        "        for i, (image, target, filename) in enumerate(self.val_loader):\n",
        "            image = image.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(image)\n",
        "            self.metric.update(outputs[0], target)\n",
        "            pixAcc, mIoU = self.metric.get()\n",
        "            print(\"Sample: {:d}, Validation pixAcc: {:.3f}, mIoU: {:.3f}\".format(i + 1, pixAcc, mIoU))\n",
        "\n",
        "        new_pred = (pixAcc + mIoU) / 2\n",
        "        if new_pred > self.best_pred:\n",
        "            is_best = True\n",
        "            self.best_pred = new_pred\n",
        "        save_checkpoint(self.model, self.args, is_best)\n",
        "        synchronize()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, args, is_best=False):\n",
        "    \"\"\"Save Checkpoint\"\"\"\n",
        "    directory = os.path.expanduser(args.save_dir)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    filename = '{}_{}_{}.pth'.format(args.model, args.backbone, args.dataset)\n",
        "    filename = os.path.join(directory, filename)\n",
        "\n",
        "    if args.distributed:\n",
        "        model = model.module\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    if is_best:\n",
        "        best_filename = '{}_{}_{}_best_model.pth'.format(args.model, args.backbone, args.dataset)\n",
        "        best_filename = os.path.join(directory, best_filename)\n",
        "        shutil.copyfile(filename, best_filename)\n",
        "\n",
        "\n",
        "class Args:\n",
        "    pass\n",
        "\n",
        "args = Args()\n",
        "\n",
        "\n",
        "#args = parse_args()\n",
        "print(\"args:\",args)\n",
        "args.aux=False;\n",
        "args.aux_weight=0.4; \n",
        "args.backbone='vgg16'; base_size=520; \n",
        "args.batch_size=4; \n",
        "args.base_size=520;\n",
        "args.crop_size=480; \n",
        "args.dataset='pascal_voc'; \n",
        "args.epochs=50; \n",
        "args.jpu=False; \n",
        "args.local_rank=0; \n",
        "args.log_dir='../runs/logs/'; \n",
        "args.log_iter=10;\n",
        "args.lr=0.0001;\n",
        "args.model='fcn32s';\n",
        "args.momentum=0.9;\n",
        "args.no_cuda=False; \n",
        "args.resume=None;\n",
        "args.save_dir='/content/gdrive/My Drive/CheckPoints'; \n",
        "args.save_epoch=10;\n",
        "args.skip_val=False; \n",
        "args.start_epoch=0;\n",
        "args.use_ohem=False; \n",
        "args.val_epoch=1;\n",
        "args.warmup_factor=0.3333333333333333;\n",
        "args.warmup_iters=0;\n",
        "args.warmup_method='linear';\n",
        "args.weight_decay=0.0001;\n",
        "args.workers=4;\n",
        "# reference maskrcnn-benchmark\n",
        "num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "args.num_gpus = num_gpus\n",
        "args.distributed = num_gpus > 1\n",
        "if not args.no_cuda and torch.cuda.is_available():\n",
        "    cudnn.benchmark = True\n",
        "    args.device = \"cuda\"\n",
        "else:\n",
        "    args.distributed = False\n",
        "    args.device = \"cpu\"\n",
        "if args.distributed:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "    synchronize()\n",
        "args.lr = args.lr * num_gpus\n",
        "\n",
        "#logger = setup_logger(\"semantic_segmentation\", args.log_dir, get_rank(), filename='{}_{}_{}_log.txt'.format(\n",
        "#   args.model, args.backbone, args.dataset))\n",
        "#logger.info(\"Using {} GPUs\".format(num_gpus))\n",
        "#logger.info(args)\n",
        "\n",
        "trainer = Trainer(args)\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "args: <__main__.Args object at 0x7fb1d416f1d0>\n",
            "/content/awesome-semantic-segmentation-pytorch/datasets/voc\n",
            "Found 1464 images in the folder /content/awesome-semantic-segmentation-pytorch/datasets/voc/VOC2012\n",
            "/content/awesome-semantic-segmentation-pytorch/datasets/voc\n",
            "Found 1449 images in the folder /content/awesome-semantic-segmentation-pytorch/datasets/voc/VOC2012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "511e2464cfac42788e34e36561972662",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=553433881), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training, Total Epochs: 50 = Total Iterations 18300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iters: 10/18300 || Lr: 0.000100 || Loss: 2.8011 || Cost Time: 0:00:09 || Estimated Time: 4:29:28\n",
            "Iters: 20/18300 || Lr: 0.000100 || Loss: 2.2140 || Cost Time: 0:00:11 || Estimated Time: 2:46:33\n",
            "Iters: 30/18300 || Lr: 0.000100 || Loss: 1.7636 || Cost Time: 0:00:13 || Estimated Time: 2:12:15\n",
            "Iters: 40/18300 || Lr: 0.000100 || Loss: 2.0777 || Cost Time: 0:00:15 || Estimated Time: 1:55:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-56d48d769a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-56d48d769a7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDg3xeie8rTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def _getvocpallete(num_cls):\n",
        "    n = num_cls\n",
        "    pallete = [0] * (n * 3)\n",
        "    for j in range(0, n):\n",
        "        lab = j\n",
        "        pallete[j * 3 + 0] = 0\n",
        "        pallete[j * 3 + 1] = 0\n",
        "        pallete[j * 3 + 2] = 0\n",
        "        i = 0\n",
        "        while (lab > 0):\n",
        "            pallete[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
        "            pallete[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
        "            pallete[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
        "            i = i + 1\n",
        "            lab >>= 3\n",
        "    return pallete\n",
        "\n",
        "\n",
        "vocpallete = _getvocpallete(256)\n",
        "\n",
        "def get_color_pallete(npimg, dataset='pascal_voc'):\n",
        "    \"\"\"Visualize image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    npimg : numpy.ndarray\n",
        "        Single channel image with shape `H, W, 1`.\n",
        "    dataset : str, default: 'pascal_voc'\n",
        "        The dataset that model pretrained on. ('pascal_voc', 'ade20k')\n",
        "    Returns\n",
        "    -------\n",
        "    out_img : PIL.Image\n",
        "        Image with color pallete\n",
        "    \"\"\"\n",
        "    # recovery boundary\n",
        "    if dataset in ('pascal_voc', 'pascal_aug'):\n",
        "        npimg[npimg == -1] = 255\n",
        "    # put colormap\n",
        "    if dataset == 'ade20k':\n",
        "        npimg = npimg + 1\n",
        "        out_img = Image.fromarray(npimg.astype('uint8'))\n",
        "        out_img.putpalette(adepallete)\n",
        "        return out_img\n",
        "    elif dataset == 'citys':\n",
        "        out_img = Image.fromarray(npimg.astype('uint8'))\n",
        "        out_img.putpalette(cityspallete)\n",
        "        return out_img\n",
        "    out_img = Image.fromarray(npimg.astype('uint8'))\n",
        "    out_img.putpalette(vocpallete)\n",
        "    return out_img\n",
        "\n",
        "\n",
        "class Evaluator(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device(args.device)\n",
        "\n",
        "        # image transform\n",
        "        input_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "        ])\n",
        "\n",
        "        # dataset and dataloader\n",
        "        val_dataset = VOCSegmentation(split='val', mode='testval', transform=input_transform)\n",
        "        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n",
        "        val_batch_sampler = make_batch_data_sampler(val_sampler, images_per_batch=1)\n",
        "        self.val_loader = data.DataLoader(dataset=val_dataset,\n",
        "                                          batch_sampler=val_batch_sampler,\n",
        "                                          num_workers=args.workers,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "        # create network\n",
        "        BatchNorm2d = nn.SyncBatchNorm if args.distributed else nn.BatchNorm2d\n",
        "        self.model = get_fcn32s(model=args.model, dataset=args.dataset, backbone=args.backbone,\n",
        "                                            aux=args.aux, pretrained=True, pretrained_base=False,\n",
        "                                            local_rank=args.local_rank,\n",
        "                                            norm_layer=BatchNorm2d).to(self.device)\n",
        "        if args.distributed:\n",
        "            self.model = nn.parallel.DistributedDataParallel(self.model,\n",
        "                device_ids=[args.local_rank], output_device=args.local_rank)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.metric = SegmentationMetric(val_dataset.num_class)\n",
        "\n",
        "    def eval(self):\n",
        "        self.metric.reset()\n",
        "        self.model.eval()\n",
        "        if self.args.distributed:\n",
        "            model = self.model.module\n",
        "        else:\n",
        "            model = self.model\n",
        "        print(\"Start validation, Total sample: {:d}\".format(len(self.val_loader)))\n",
        "        for i, (image, target, filename) in enumerate(self.val_loader):\n",
        "            image = image.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(image)\n",
        "            self.metric.update(outputs[0], target)\n",
        "            pixAcc, mIoU = self.metric.get()\n",
        "            print(\"Sample: {:d}, validation pixAcc: {:.3f}, mIoU: {:.3f}\".format(\n",
        "                i + 1, pixAcc * 100, mIoU * 100))\n",
        "\n",
        "            if self.args.save_pred:\n",
        "                pred = torch.argmax(outputs[0], 1)\n",
        "                pred = pred.cpu().data.numpy()\n",
        "\n",
        "                predict = pred.squeeze(0)\n",
        "                mask = get_color_pallete(predict, self.args.dataset)\n",
        "                mask.save(os.path.join(outdir, os.path.splitext(filename[0])[0] + '.png'))\n",
        "        synchronize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_4ylXE17JAY",
        "colab_type": "code",
        "outputId": "04f25e8e-fbe5-4448-80fe-9d69c91da3c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "  num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "  args.distributed = num_gpus > 1\n",
        "  if not args.no_cuda and torch.cuda.is_available():\n",
        "      cudnn.benchmark = True\n",
        "      args.device = \"cuda\"\n",
        "  else:\n",
        "      args.distributed = False\n",
        "      args.device = \"cpu\"\n",
        "  if args.distributed:\n",
        "      torch.cuda.set_device(args.local_rank)\n",
        "      torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "      synchronize()\n",
        "\n",
        "  # TODO: optim code\n",
        "  args.save_pred = True\n",
        "  if args.save_pred:\n",
        "      outdir = '/content/pred_pic/{}_{}_{}'.format(args.model, args.backbone, args.dataset)\n",
        "      if not os.path.exists(outdir):\n",
        "          os.makedirs(outdir)\n",
        "\n",
        "\n",
        "  evaluator = Evaluator(args)\n",
        "  evaluator.eval()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/awesome-semantic-segmentation-pytorch/datasets/voc\n",
            "Found 1449 images in the folder /content/awesome-semantic-segmentation-pytorch/datasets/voc/VOC2012\n",
            "/content/gdrive/My Drive/CheckPoints/fcn32s_vgg16_pascal_voc.pth\n",
            "Start validation, Total sample: 1449\n",
            "Sample: 1, validation pixAcc: 93.676, mIoU: 7.988\n",
            "Sample: 2, validation pixAcc: 89.371, mIoU: 11.240\n",
            "Sample: 3, validation pixAcc: 90.543, mIoU: 11.975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c8f38566f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3613e208d052>\u001b[0m in \u001b[0;36meval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mpixAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmIoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5c7c33e3c731>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mpool5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saarZSNGB5hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "files = glob.glob(\"pred_pic/fcn32s_vgg16_pascal_voc/*\")\n",
        "for filename in files:\n",
        "  img = plt.imread(filename)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}