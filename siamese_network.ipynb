{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNf5ed549FdRM6QLUhqiPGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/deep-learning-notebooks/blob/master/siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeGIWwKA2NrI",
        "colab_type": "text"
      },
      "source": [
        "[One Shot Learning with Siamese Networks](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nvS_fAcznE_",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Google Apache License\n",
        "\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu8c3edUzaN4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Utility functions\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Model, Input\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import ReLU, Dense, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import DepthwiseConv2D, SeparableConv2D, Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Activation, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.compat.v1.keras.initializers import glorot_uniform, he_normal\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "\n",
        "class Composable(object):\n",
        "    ''' Composable base (super) class for Models '''\n",
        "    init_weights = 'he_normal'\t# weight initialization\n",
        "    reg          = None         # kernel regularizer\n",
        "    relu         = None         # ReLU max value\n",
        "    bias         = True         # whether to use bias in dense/conv layers\n",
        "\n",
        "    def __init__(self, init_weights=None, reg=None, relu=None, bias=True):\n",
        "        \"\"\" Constructor\n",
        "            init_weights : kernel initializer\n",
        "            reg          : kernel regularizer\n",
        "            relu         : clip value for ReLU\n",
        "            bias         : whether to use bias\n",
        "        \"\"\"\n",
        "        if init_weights is not None:\n",
        "            self.init_weights = init_weights\n",
        "        if reg is not None:\n",
        "            self.reg = reg\n",
        "        if relu is not None:\n",
        "            self.relu = relu\n",
        "        if bias is not None:\n",
        "            self.bias = bias\n",
        "\n",
        "        # Feature maps encoding at the bottleneck layer in classifier (high dimensionality)\n",
        "        self._encoding = None\n",
        "        # Pooled and flattened encodings at the bottleneck layer (low dimensionality)\n",
        "        self._embedding = None\n",
        "        # Pre-activation conditional probabilities for classifier\n",
        "        self._probabilities = None\n",
        "        # Post-activation conditional probabilities for classifier\n",
        "        self._softmax = None\n",
        "\n",
        "        self._model = None\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self._model\n",
        "\n",
        "    @model.setter\n",
        "    def model(self, _model):\n",
        "        self._model = _model\n",
        "\n",
        "    @property\n",
        "    def encoding(self):\n",
        "        return self._encoding\n",
        "\n",
        "    @encoding.setter\n",
        "    def encoding(self, layer):\n",
        "        self._encoding = layer\n",
        "\n",
        "    @property\n",
        "    def embedding(self):\n",
        "        return self._embedding\n",
        "\n",
        "    @embedding.setter\n",
        "    def embedding(self, layer):\n",
        "        self._embedding = layer\n",
        "\n",
        "    @property\n",
        "    def probabilities(self):\n",
        "        return self._probabilities\n",
        "\n",
        "    @probabilities.setter\n",
        "    def probabilities(self, layer):\n",
        "        self._probabilities = layer\n",
        "\n",
        "    def prestem(self, inputs, **metaparameters):\n",
        "      \"\"\" Construct a Pre-stem for Stem Group\n",
        "          inputs : input to the pre-stem\n",
        "          norm   : include normalization layer\n",
        "      \"\"\"\n",
        "      x = inputs\n",
        "      if 'norm' in metaparameters:\n",
        "          norm = metaparameters['norm']\n",
        "          if norm:\n",
        "              x = self.Normalize(inputs)\n",
        "      return x\n",
        "\n",
        "    def stem(self, inputs, kernel_size=(7, 7), **metaparameters):\n",
        "      \"\"\" Construct the Stem Group\n",
        "          inputs     : input to the stem\n",
        "          kernel_size: kernel (filter) size\n",
        "          pooling    : pooling option\n",
        "      \"\"\"\n",
        "      if 'pooling' in metaparameters:\n",
        "          pooling = metaparameters['pooling']\n",
        "      else:\n",
        "          pooling = None\n",
        "\n",
        "      x = self.Conv2D(inputs, kernel_size, strides=(1, 1), padding='same')\n",
        "      x = self.BatchNormalization(x)\n",
        "      x = self.ReLU(x)\n",
        "\n",
        "      if pooling == 'max':\n",
        "          x = MaxPooling2D((2, 2), strides=2)(x)\n",
        "      elif pooling == 'feature':\n",
        "          # feature pooling\n",
        "          x = self.Conv2D(x, kernel_size, strides=(2, 2), padding='same')\n",
        "          x = self.BatchNormalization(x)\n",
        "          x = self.ReLU(x)\n",
        "      return x\n",
        "\n",
        "    def classifier(self, x, n_classes, **metaparameters):\n",
        "      \"\"\" Construct the Classifier Group \n",
        "          x         : input to the classifier\n",
        "          n_classes : number of output classes\n",
        "          pooling   : type of feature map pooling\n",
        "          dropout   : hidden dropout unit\n",
        "      \"\"\"\n",
        "      if 'pooling' in metaparameters:\n",
        "          pooling = metaparameters['pooling']\n",
        "      else:\n",
        "          pooling = GlobalAveragePooling2D\n",
        "      if 'dropout' in metaparameters:\n",
        "          dropout = metaparameters['dropout']\n",
        "      else:\n",
        "          dropout = None\n",
        "\n",
        "      if pooling is not None:\n",
        "          # Save the encoding layer (high dimensionality)\n",
        "          self.encoding = x\n",
        "\n",
        "          # Pooling at the end of all the convolutional groups\n",
        "          x = pooling()(x)\n",
        "\n",
        "          # Save the embedding layer (low dimensionality)\n",
        "          self.embedding = x\n",
        "\n",
        "      if dropout is not None:\n",
        "          x = Dropout(dropout)(x)\n",
        "\n",
        "      # Final Dense Outputting Layer for the outputs\n",
        "      x = self.Dense(x, n_classes, use_bias=True, **metaparameters)\n",
        "      \n",
        "      # Save the pre-activation probabilities layer\n",
        "      self.probabilities = x\n",
        "      outputs = Activation('softmax')(x)\n",
        "      # Save the post-activation probabilities layer\n",
        "      self.softmax = outputs\n",
        "      return outputs\n",
        "\n",
        "    def top(self, layer):\n",
        "        \"\"\" Add layer to the top of the neural network\n",
        "            layer : layer to add\n",
        "        \"\"\"\n",
        "        outputs = layer(self._model.outputs)\n",
        "        self._model = Model(self._model.inputs, outputs)\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\" Call underlying summary method\n",
        "        \"\"\"\n",
        "        self._model.summary()\n",
        "\n",
        "    def Dense(self, x, units, activation=None, use_bias=True, **hyperparameters):\n",
        "        \"\"\" Construct Dense Layer\n",
        "            x           : input to layer\n",
        "            activation  : activation function\n",
        "            use_bias    : whether to use bias\n",
        "            init_weights: kernel initializer\n",
        "            reg         : kernel regularizer\n",
        "        \"\"\"\n",
        "        if 'reg' in hyperparameters:\n",
        "            reg = hyperparameters['reg']\n",
        "        else:\n",
        "            reg = self.reg\n",
        "        if 'init_weights' in hyperparameters:\n",
        "            init_weights = hyperparameters['init_weights']\n",
        "        else:\n",
        "            init_weights = self.init_weights\n",
        "            \n",
        "        x = Dense(units, activation, use_bias=use_bias,\n",
        "                  kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
        "        return x\n",
        "\n",
        "    def Conv2D(self, x, n_filters, kernel_size, strides=(1, 1), padding='valid', activation=None, **hyperparameters):\n",
        "        \"\"\" Construct a Conv2D layer\n",
        "            x           : input to layer\n",
        "            n_filters   : number of filters\n",
        "            kernel_size : kernel (filter) size\n",
        "            strides     : strides\n",
        "            padding     : how to pad when filter overlaps the edge\n",
        "            activation  : activation function\n",
        "            use_bias    : whether to include the bias\n",
        "            init_weights: kernel initializer\n",
        "            reg         : kernel regularizer\n",
        "        \"\"\"\n",
        "        if 'reg' in hyperparameters:\n",
        "            reg = hyperparameters['reg']\n",
        "        else:\n",
        "            reg = self.reg\n",
        "        if 'init_weights' in hyperparameters:\n",
        "            init_weights = hyperparameters['init_weights']\n",
        "        else:\n",
        "            init_weights = self.init_weights\n",
        "        if 'bias' in hyperparameters:\n",
        "            bias = hyperparameters['bias']\n",
        "        else:\n",
        "            bias = self.bias\n",
        "\n",
        "        x = Conv2D(n_filters, kernel_size, strides=strides, padding=padding, activation=activation,\n",
        "                   use_bias=bias, kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
        "        return x\n",
        "\n",
        "    def Conv2DTranspose(self, x, n_filters, kernel_size, strides=(1, 1), padding='valid', activation=None, **hyperparameters):\n",
        "        \"\"\" Construct a Conv2DTranspose layer\n",
        "            x           : input to layer\n",
        "            n_filters   : number of filters\n",
        "            kernel_size : kernel (filter) size\n",
        "            strides     : strides\n",
        "            padding     : how to pad when filter overlaps the edge\n",
        "            activation  : activation function\n",
        "            use_bias    : whether to include the bias\n",
        "            init_weights: kernel initializer\n",
        "            reg         : kernel regularizer\n",
        "        \"\"\"\n",
        "        if 'reg' in hyperparameters:\n",
        "            reg = hyperparameters['reg']\n",
        "        else:\n",
        "            reg = self.reg\n",
        "        if 'init_weights' in hyperparameters:\n",
        "            init_weights = hyperparameters['init_weights']\n",
        "        else:\n",
        "            init_weights = self.init_weights \n",
        "        if 'bias' in hyperparameters:\n",
        "            bias = hyperparameters['bias']\n",
        "        else:\n",
        "            bias = self.bias\n",
        "\n",
        "        x = Conv2DTranspose(n_filters, kernel_size, strides=strides, padding=padding, activation=activation, \n",
        "\t\t\t    use_bias=bias, kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
        "        return x\n",
        "\n",
        "    def DepthwiseConv2D(self, x, kernel_size, strides=(1, 1), padding='valid', activation=None, **hyperparameters):\n",
        "        \"\"\" Construct a DepthwiseConv2D layer\n",
        "            x           : input to layer\n",
        "            kernel_size : kernel (filter) size\n",
        "            strides     : strides\n",
        "            padding     : how to pad when filter overlaps the edge\n",
        "            activation  : activation function\n",
        "            use_bias    : whether to include the bias\n",
        "            init_weights: kernel initializer\n",
        "            reg         : kernel regularizer\n",
        "        \"\"\"\n",
        "        if 'reg' in hyperparameters:\n",
        "            reg = hyperparameters['reg']\n",
        "        else:\n",
        "            reg = self.reg\n",
        "        if 'init_weights' in hyperparameters:\n",
        "            init_weights = hyperparameters['init_weights']\n",
        "        else:\n",
        "            init_weights = self.init_weights\n",
        "        if 'bias' in hyperparameters:\n",
        "            bias = hyperparameters['bias']\n",
        "        else:\n",
        "            bias = self.bias\n",
        "\n",
        "        x = DepthwiseConv2D(kernel_size, strides=strides, padding=padding, activation=activation, \n",
        "\t\t\t    use_bias=bias, kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
        "        return x\n",
        "\n",
        "    def SeparableConv2D(self, x, n_filters, kernel_size, strides=(1, 1), padding='valid', activation=None, **hyperparameters):\n",
        "        \"\"\" Construct a SeparableConv2D layer\n",
        "            x           : input to layer\n",
        "            n_filters   : number of filters\n",
        "            kernel_size : kernel (filter) size\n",
        "            strides     : strides\n",
        "            padding     : how to pad when filter overlaps the edge\n",
        "            activation  : activation function\n",
        "            use_bias    : whether to include the bias\n",
        "            init_weights: kernel initializer\n",
        "            reg         : kernel regularizer\n",
        "        \"\"\"\n",
        "        if 'reg' in hyperparameters:\n",
        "            reg = hyperparameters['reg']\n",
        "        else:\n",
        "            reg = self.reg\n",
        "        if 'init_weights' in hyperparameters:\n",
        "            init_weights = hyperparameters['init_weights']\n",
        "        else:\n",
        "            init_weights = self.init_weights\n",
        "        if 'bias' in hyperparameters:\n",
        "            bias = hyperparameters['bias']\n",
        "        else:\n",
        "            bias = self.bias\n",
        "\n",
        "        x = SeparableConv2D(n_filters, kernel_size, strides=strides, padding=padding, activation=activation,\n",
        "                            use_bias=bias, kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def ReLU(self, x):\n",
        "        \"\"\" Construct ReLU activation function\n",
        "            x  : input to activation function\n",
        "        \"\"\"\n",
        "        x = ReLU(self.relu)(x)\n",
        "        return x\n",
        "\t\n",
        "    def HS(self, x):\n",
        "        \"\"\" Construct Hard Swish activation function\n",
        "            x  : input to activation function\n",
        "        \"\"\"\n",
        "        return (x * K.relu(x + 3, max_value=6.0)) / 6.0\n",
        "\n",
        "    def BatchNormalization(self, x, **params):\n",
        "        \"\"\" Construct a Batch Normalization function\n",
        "            x : input to function\n",
        "        \"\"\"\n",
        "        x = BatchNormalization(epsilon=1.001e-5, **params)(x)\n",
        "        return x\n",
        "\n",
        "    ###\n",
        "    # Pre-stem Layers\n",
        "    ###\n",
        "\n",
        "    class Normalize(layers.Layer):\n",
        "        \"\"\" Custom Layer for Preprocessing Input - Normalization \"\"\"\n",
        "        def __init__(self, max=255.0, **parameters):\n",
        "            \"\"\" Constructor \"\"\"\n",
        "            super(Composable.Normalize, self).__init__(**parameters)\n",
        "            self.max = max\n",
        "    \n",
        "        def build(self, input_shape):\n",
        "            \"\"\" Handler for Build (Functional) or Compile (Sequential) operation \"\"\"\n",
        "            self.kernel = None # no learnable parameters\n",
        "    \n",
        "        @tf.function\n",
        "        def call(self, inputs):\n",
        "            \"\"\" Handler for run-time invocation of layer \"\"\"\n",
        "            inputs = inputs / self.max\n",
        "            return inputs\n",
        "\n",
        "    class Standarize(layers.Layer):\n",
        "        \"\"\" Custom Layer for Preprocessing Input - Standardization \"\"\"\n",
        "        def __init__(self, mean, std, **parameters):\n",
        "            \"\"\" Constructor \"\"\"\n",
        "            super(Composable.Standardize, self).__init__(**parameters)\n",
        "            self.mean = mean\n",
        "            self.std  = std\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            \"\"\" Handler for Build (Functional) or Compile (Sequential) operation \"\"\"\n",
        "            self.kernel = None # no learnable parameters\n",
        "\n",
        "        @tf.function\n",
        "        def call(self, inputs):\n",
        "            \"\"\" Handler for run-time invocation of layer \"\"\"\n",
        "            inputs = (inputs - self.mean) / self.std\n",
        "            return inputs\n",
        "\n",
        "    ###\n",
        "    # Preprocessing\n",
        "    ###\n",
        "\n",
        "    def normalization(self, x_train, x_test=None, centered=False):\n",
        "        \"\"\" Normalize the input\n",
        "            x_train : training images\n",
        "            y_train : test images\n",
        "        \"\"\"\n",
        "        if x_train.dtype == np.uint8:\n",
        "            if centered:\n",
        "                x_train = ((x_train - 1) / 127.5).astype(np.float32)\n",
        "                if x_test is not None:\n",
        "                    x_test  = ((x_test  - 1) / 127.5).astype(np.float32)\n",
        "            else:\n",
        "                x_train = (x_train / 255.0).astype(np.float32)\n",
        "                if x_test is not None:\n",
        "                    x_test  = (x_test  / 255.0).astype(np.float32)\n",
        "        return x_train, x_test\n",
        "\n",
        "    def standardization(self, x_train, x_test=None):\n",
        "        \"\"\" Standardize the input\n",
        "            x_train : training images\n",
        "            x_test  : test images\n",
        "        \"\"\"\n",
        "        self.mean = np.mean(x_train)\n",
        "        self.std  = np.std(x_train)\n",
        "        x_train = ((x_train - self.mean) / self.std).astype(np.float32)\n",
        "        if x_test is not None:\n",
        "            x_test  = ((x_test  - self.mean) / self.std).astype(np.float32)\n",
        "        return x_train, x_test\n",
        "\n",
        "    def label_smoothing(self, y_train, n_classes, factor=0.1):\n",
        "        \"\"\" Convert a matrix of one-hot row-vector labels into smoothed versions. \n",
        "            y_train  : training labels\n",
        "            n_classes: number of classes\n",
        "            factor   : smoothing factor (between 0 and 1)\n",
        "        \"\"\"\n",
        "        if 0 <= factor <= 1:\n",
        "            # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf\n",
        "            y_train *= 1 - factor\n",
        "            y_train += factor / n_classes\n",
        "        else:\n",
        "            raise Exception('Invalid label smoothing factor: ' + str(factor))\n",
        "        return y_train\n",
        "\n",
        "    ###\n",
        "    # Training\n",
        "    ###\n",
        "\n",
        "    def compile(self, loss='categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-5), metrics=['acc']):\n",
        "        \"\"\" Compile the model for training\n",
        "            loss     : the loss function\n",
        "            optimizer: the optimizer\n",
        "            metrics  : metrics to report\n",
        "        \"\"\"\n",
        "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    # training variables\n",
        "    hidden_dropout = None # hidden dropout in classifier\n",
        "    w_lr           = 0    # target warmup rate\n",
        "    w_epochs       = 0    # number of epochs in warmup\n",
        "    i_lr           = 0    # initial warmup rate during full training\n",
        "    e_decay        = 0    # weight decay rate during full training\n",
        "    e_steps        = 0    # number of steps (batches) in an epoch\n",
        "    t_steps        = 0    # total number of steps in training job\n",
        "\n",
        "    def init_draw(self, x_train, y_train, ndraws=5, epochs=3, steps=350, lr=1e-06, batch_size=32):\n",
        "        \"\"\" Use the lottery ticket principle to find the best weight initialization\n",
        "            x_train : training images\n",
        "            y_train : training labels\n",
        "            ndraws  : number of draws to find the winning lottery ticket\n",
        "            epochs  : number of trial epochs\n",
        "            steps   :\n",
        "            lr      :\n",
        "            batch_size:\n",
        "        \"\"\"\n",
        "        print(\"*** Initialize Draw\")\n",
        "        loss = sys.float_info.max\n",
        "        weights = None\n",
        "        for _ in range(ndraws):\n",
        "            self.model = tf.keras.models.clone_model(self.model)\n",
        "            self.compile(optimizer=Adam(lr))\n",
        "            w = self.model.get_weights()\n",
        "\n",
        "            # Create generator for training in steps\n",
        "            datagen = ImageDataGenerator()\n",
        "\n",
        "            print(\"*** Lottery\", _)\n",
        "            self.model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                                  epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "            d_loss = self.model.history.history['loss'][epochs-1]\n",
        "            if d_loss < loss:\n",
        "                loss = d_loss\n",
        "                w = self.model.get_weights()\n",
        "\n",
        "        # Set the best\n",
        "        self.model.set_weights(w)\n",
        "\n",
        "    def warmup_scheduler(self, epoch, lr):\n",
        "        \"\"\" learning rate schedular for warmup training\n",
        "            epoch : current epoch iteration\n",
        "            lr    : current learning rate\n",
        "        \"\"\"\n",
        "        if epoch == 0:\n",
        "           return lr\n",
        "        if epoch == 2:\n",
        "            # loss is diverging\n",
        "            if self.model.history.history['loss'][1] > self.model.history.history['loss'][0]:\n",
        "                print(\"*** Loss is divergining, Reducing Warmnup Rate\")\n",
        "                self.w_lr /= 10\n",
        "        return epoch * self.w_lr / self.w_epochs\n",
        "\n",
        "    def warmup(self, x_train, y_train, epochs=5, s_lr=1e-6, e_lr=0.001):\n",
        "        \"\"\" Warmup for numerical stability\n",
        "            x_train : training images\n",
        "            y_train : training labels\n",
        "            epochs  : number of epochs for warmup\n",
        "            s_lr    : start warmup learning rate\n",
        "            e_lr    : end warmup learning rate\n",
        "        \"\"\"\n",
        "        print(\"*** Warmup (for numerical stability)\")\n",
        "        # Setup learning rate scheduler\n",
        "        self.compile(optimizer=Adam(s_lr))\n",
        "        lrate = LearningRateScheduler(self.warmup_scheduler, verbose=1)\n",
        "        self.w_epochs = epochs\n",
        "        self.w_lr     = e_lr - s_lr\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=32, verbose=1,\n",
        "                       callbacks=[lrate])\n",
        "\n",
        "    def _tune(self, x_train, y_train, x_test, y_test, epochs, steps, lr, batch_size, weights):\n",
        "        \"\"\" Helper function for hyperparameter tuning\n",
        "            x_train   : training images\n",
        "            y_train   : training labels\n",
        "            x_test    : test images\n",
        "            y_test    : test labels\n",
        "            lr        : trial learning rate\n",
        "            batch_size: the batch size (constant)\n",
        "            epochs    : the number of epochs\n",
        "            steps     : steps per epoch\n",
        "            weights   : warmup weights\n",
        "        \"\"\"\n",
        "        # Compile the model for the new learning rate\n",
        "        self.compile(optimizer=Adam(lr))\n",
        "\n",
        "        # Create generator for training in steps\n",
        "        datagen = ImageDataGenerator()\n",
        "         \n",
        "        # Train the model\n",
        "        print(\"*** Learning Rate\", lr)\n",
        "        self.model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                 epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "        # Evaluate the model\n",
        "        result = self.evaluate(x_test, y_test)\n",
        "         \n",
        "        # Reset the weights\n",
        "        self.model.set_weights(weights)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def grid_search(self, x_train, y_train, x_test, y_test, epochs=3, steps=250,\n",
        "                          lr_range=[0.0001, 0.001, 0.01, 0.1], batch_range=[32, 128]):\n",
        "        \"\"\" Do a grid search for hyperparameters\n",
        "            x_train : training images\n",
        "            y_train : training labels\n",
        "            epochs  : number of epochs\n",
        "            steps   : number of steps per epoch\n",
        "            lr_range: range for searching learning rate\n",
        "            batch_range: range for searching batch size\n",
        "        \"\"\"\n",
        "        print(\"*** Hyperparameter Search\")\n",
        "\n",
        "        # Save the original weights\n",
        "        weights = self.model.get_weights()\n",
        "\n",
        "        # Search learning rate\n",
        "        v_loss = []\n",
        "        for lr in lr_range:\n",
        "            result = self._tune(x_train, y_train, x_test, y_test, epochs, steps, lr, batch_range[0], weights)\n",
        "            v_loss.append(result[0])\n",
        "            \n",
        "        # Find the best starting learning rate based on validation loss\n",
        "        best = sys.float_info.max\n",
        "        for _ in range(len(lr_range)):\n",
        "            if v_loss[_] < best:\n",
        "                best = v_loss[_]\n",
        "                lr = lr_range[_]\n",
        "\n",
        "        # Best was smallest learning rate\n",
        "        if lr == lr_range[0]:\n",
        "            # try 1/2 the lowest learning rate\n",
        "            result = self._tune(x_train, y_train, x_test, y_test, epochs, steps, (lr / 2.0), batch_range[0], weights)\n",
        "\n",
        "            # 1/2 of lr is even better\n",
        "            if result[0] < best:\n",
        "                lr = lr / 2.0\n",
        "            # try halfway between the first and second value\n",
        "            else:\n",
        "                n_lr = (lr_range[0] + lr_range[1]) / 2.0\n",
        "                result = self._tune(x_train, y_train, x_test, y_test, epochs, steps, n_lr, batch_range[0], weights)\n",
        "\n",
        "                # 1/2 of lr is even better\n",
        "                if result[0] < best:\n",
        "                    lr = lr / 2.0\n",
        "                \n",
        "        elif lr == lr_range[len(lr_range)-1]:\n",
        "            # try 2X the largest learning rate\n",
        "            result = self._tune(x_train, y_train, x_test, y_test, epochs, steps, (lr * 2.0), batch_range[0], weights)\n",
        "\n",
        "            # 2X of lr is even better\n",
        "            if result[0] < best:\n",
        "                lr = lr * 2.0\n",
        "\t\t\n",
        "        print(\"*** Selected best learning rate:\", lr)\n",
        "\n",
        "        # Compile the model for the new learning rate\n",
        "        self.compile(optimizer=Adam(lr))\n",
        "        \n",
        "        v_loss = []\n",
        "        # skip the first batch size - since we used it in searching learning rate\n",
        "        datagen = ImageDataGenerator()\n",
        "        for bs in batch_range[1:]:\n",
        "            print(\"*** Batch Size\", bs)\n",
        "\n",
        "            # equalize the number of examples per epoch\n",
        "            steps = int(batch_range[0] * steps / bs)\n",
        "\n",
        "            self.model.fit(datagen.flow(x_train, y_train, batch_size=bs),\n",
        "                                     epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "            # Evaluate the model\n",
        "            result = self.evaluate(x_test, y_test)\n",
        "            v_loss.append(result[0])\n",
        "            \n",
        "            # Reset the weights\n",
        "            self.model.set_weights(weights)\n",
        "\n",
        "        # Find the best batch size based on validation loss\n",
        "        best = sys.float_info.max\n",
        "        bs = batch_range[0]\n",
        "        for _ in range(len(batch_range)-1):\n",
        "            if v_loss[_] < best:\n",
        "                best = v_loss[_]\n",
        "                bs = batch_range[_]\n",
        "\n",
        "        print(\"*** Selected best batch size:\", bs)\n",
        "\n",
        "        # return the best learning rate and batch size\n",
        "        return lr, bs\n",
        "\n",
        "    def time_decay(self, epoch, lr):\n",
        "        \"\"\" Time-based Decay\n",
        "        \"\"\"\n",
        "        return lr * (1. / (1. + self.e_decay[1] * epoch))\n",
        "\n",
        "    def step_decay(self, epoch, lr):\n",
        "        \"\"\" Step-based decay\n",
        "        \"\"\"\n",
        "        return self.i_lr * self.e_decay[1]**(epoch)\n",
        "\n",
        "    def exp_decay(self, epoch, lr):\n",
        "        \"\"\" Exponential Decay\n",
        "        \"\"\"\n",
        "        return self.i_lr * math.exp(-self.e_decay[1] * epoch)\n",
        "\n",
        "    def cosine_decay(self, epoch, lr, alpha=0.0):\n",
        "        \"\"\" Cosine Decay\n",
        "        \"\"\"\n",
        "        cosine_decay = 0.5 * (1 + np.cos(np.pi * (self.e_steps * epoch) / self.t_steps))\n",
        "        decayed = (1 - alpha) * cosine_decay + alpha\n",
        "        return lr * decayed\n",
        "\n",
        "    def training_scheduler(self, epoch, lr):\n",
        "        \"\"\" Learning Rate scheduler for full-training\n",
        "            epoch : epoch number\n",
        "            lr    : current learning rate\n",
        "        \"\"\"\n",
        "        # First epoch (not started) - do nothing\n",
        "        if epoch == 0:\n",
        "            return lr\n",
        "\n",
        "        # Hidden dropout unit in classifier\n",
        "        if self.hidden_dropout is not None:\n",
        "            # If training accuracy and validation accuracy more than 3% apart\n",
        "            if self.model.history.history['acc'][epoch-1] > self.model.history.history['val_acc'][epoch-1] + 0.03:\n",
        "                if self.hidden_dropout.rate == 0.0:\n",
        "                    self.hidden_dropout.rate = 0.5\n",
        "                elif self.hidden_dropout.rate < 0.75:\n",
        "                    self.hidden_dropout.rate *= 1.1\n",
        "                print(\"*** Overfitting, set dropout to\", self.hidden_dropout.rate)\n",
        "            else:\n",
        "                if self.hidden_dropout.rate != 0.0:\n",
        "                    print(\"*** Turning off dropout\")\n",
        "                    self.hidden_dropout.rate = 0.0\n",
        "\n",
        "        if self.e_decay[0] is None:\n",
        "            return lr\n",
        "\n",
        "        # Decay the learning rate\n",
        "        if self.e_decay[0] == 'time':\n",
        "            lr = self.time_decay(epoch, lr)\n",
        "        elif self.e_decay[0] == 'step':\n",
        "            lr = self.step_decay(epoch, lr)\n",
        "        elif self.e_decay[0] == 'exp':\n",
        "            lr = self.exp_decay(epoch, lr)\n",
        "        else:\n",
        "            lr = self.cosine_decay(epoch, lr)\n",
        "        return lr\n",
        "\n",
        "    def training(self, x_train, y_train, epochs=10, batch_size=32, lr=0.001, decay=(None, 0)):\n",
        "        \"\"\" Full Training of the Model\n",
        "            x_train    : training images\n",
        "            y_train    : training labels\n",
        "            epochs     : number of epochs\n",
        "            batch_size : size of batch\n",
        "            lr         : learning rate\n",
        "            decay      : step-wise learning rate decay\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"*** Full Training\")\n",
        "\n",
        "        # Check for hidden dropout layer in classifier\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                self.hidden_dropout = layer\n",
        "                break    \n",
        "\n",
        "        if decay is None or 0:\n",
        "            decay = (None, 0)\n",
        "        elif isinstance(decay, float):\n",
        "            decay = ('time', decay)\n",
        "        elif not isinstance(decay, tuple):\n",
        "            raise Exception(\"Training: decay must be (time, value)\")\n",
        "        elif decay[0] not in [None, 'time', 'step', 'exp', 'cosine']:\n",
        "            raise Exception(\"Training: invalid method for decay\")\n",
        "\n",
        "        self.i_lr    = lr\n",
        "        self.e_decay = decay\n",
        "        self.e_steps = x_train.shape[0] // batch_size\n",
        "        self.t_steps = self.e_steps * epochs\n",
        "        self.compile(optimizer=Adam(lr=lr, decay=decay[1]))\n",
        "\n",
        "        lrate = LearningRateScheduler(self.training_scheduler, verbose=1)\n",
        "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=1,\n",
        "                       callbacks=[lrate])\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\" Call underlying evaluate() method\n",
        "        \"\"\"\n",
        "        return self._model.evaluate(x_test, y_test)\n",
        "\n",
        "    def cifar10(self, epochs=10, decay=('cosine', 0)):\n",
        "        \"\"\" Train on CIFAR-10\n",
        "            epochs : number of epochs for full training\n",
        "        \"\"\"\n",
        "        from tensorflow.keras.datasets import cifar10\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train, x_test = self.standardization(x_train, x_test)\n",
        "        y_train = to_categorical(y_train, 10)\n",
        "        y_test  = to_categorical(y_test, 10)\n",
        "        y_train = self.label_smoothing(y_train, 10, 0.1)\n",
        "\n",
        "        # compile the model\n",
        "        self.compile(loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "        self.warmup(x_train, y_train)\n",
        "\n",
        "        lr, batch_size = self.grid_search(x_train, y_train, x_test, y_test)\n",
        "\n",
        "        self.training(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                      lr=lr, decay=decay)\n",
        "        self.evaluate(x_test, y_test)\n",
        "\n",
        "    def cifar100(self, epochs=20, decay=('cosine', 0)):\n",
        "        \"\"\" Train on CIFAR-100\n",
        "            epochs : number of epochs for full training\n",
        "        \"\"\"\n",
        "        from tensorflow.keras.datasets import cifar100\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        x_train, x_test = self.normalization(x_train, x_test)\n",
        "        y_train = to_categorical(y_train, 100)\n",
        "        y_test  = to_categorical(y_test, 100)\n",
        "        y_train = self.label_smoothing(y_train, 10, 0.1)\n",
        "        self.compile(loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "        self.warmup(x_train, y_train)\n",
        "\n",
        "        lr, batch_size = self.grid_search(x_train, y_train, x_test, y_test)\n",
        "\n",
        "        self.training(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                      lr=lr, decay=decay)\n",
        "        self.evaluate(x_test, y_test)\n",
        "\n",
        "    def coil100(self, epochs=20, decay=('cosine', 0)):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # Get TF.dataset generator for COIL100\n",
        "        train, info = tfds.load('coil100', split='train', shuffle_files=True, with_info=True, as_supervised=True)\n",
        "        n_classes = info.features['label'].num_classes\n",
        "        n_images = info.splits['train'].num_examples\n",
        "        input_shape = info.features['image'].shape\n",
        "\n",
        "        # Get the dataset into memory\n",
        "        train = train.shuffle(n_images).batch(n_images)\n",
        "        for images, labels in train.take(1):\n",
        "            pass\n",
        "    \n",
        "        images = np.asarray(images)\n",
        "        images, _ = self.standardization(images, None)\n",
        "        labels = to_categorical(np.asarray(labels), n_classes)\n",
        "\n",
        "        # split the dataset into train/test\n",
        "        x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
        "\n",
        "        self.compile(loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "        self.warmup(x_train, y_train)\n",
        "\n",
        "        lr, batch_size = self.grid_search(x_train, y_train, x_test, y_test)\n",
        "\n",
        "        self.training(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                      lr=lr, decay=decay)\n",
        "        self.evaluate(x_test, y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sibr1VKK0K-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "class SiameseTwin(Composable):\n",
        "    \"\"\" Construct a Siamese Twin network \"\"\"\n",
        "    global conv_weights, dense_weights, biases\n",
        "   \n",
        "    # The weights for the convolutional layers are initialized from a normal distribution\n",
        "    # with a zero_mean and standard deviation of 10e-2\n",
        "    conv_weights = RandomNormal(mean=0.0, stddev=10e-2)\n",
        "\n",
        "    # The weights for the dense layers are initialized from a normal distribution\n",
        "    # with a mean of 0 and standard deviation of 2 * 10e-1\n",
        "    dense_weights = RandomNormal(mean=0.0, stddev=(2 * 10e-1))\n",
        "\n",
        "    # The biases for all layers are initialized from a normal distribution\n",
        "    # with a mean of 0.5 and standard deviation of 10e-2\n",
        "    biases = RandomNormal(mean=0.5, stddev=10e-2)\n",
        "\n",
        "\n",
        "    def __init__(self, input_shape=(105, 105, 3),\n",
        "                       init_weights='glorot_uniform', reg=None, relu=None, bias=True):\n",
        "        \"\"\" Construct a Siamese Twin Neural Network \n",
        "            input_shape : input shape\n",
        "            reg         : kernel regularizer\n",
        "            relu        : max value for ReLU\n",
        "            init_weights: kernel initializer\n",
        "            bias        : whether to use bias in conjunction with batch norm\n",
        "        \"\"\"\n",
        "        # Configure the base (super) class\n",
        "        super().__init__(reg=reg, relu=relu, init_weights=init_weights, bias=bias)\n",
        "    \n",
        "        # Build the twin model\n",
        "        twin = self.twin(input_shape)\n",
        "\n",
        "        # Create input tensors for the left and right side (twins) of the network.\n",
        "        left_input  = Input(input_shape)\n",
        "        right_input = Input(input_shape)\n",
        "\n",
        "        # Create the encoders for the left and right side (twins)\n",
        "        left  = twin( left_input )\n",
        "        right = twin( right_input )\n",
        "\n",
        "        # Use Lambda method to create a custom layer for implementing a L1 distance layer.\n",
        "        L1Distance = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "\n",
        "        # Connect the left and right twins (via encoders) to the layer that calculates the\n",
        "        # distance between the encodings.\n",
        "        connected = L1Distance([left, right])\n",
        "\n",
        "        # Create the output layer for predicting the similarity from the distance layer\n",
        "        outputs = self.Dense(connected, 1,activation='sigmoid', kernel_initializer=dense_weights, bias_initializer=biases)\n",
        "    \n",
        "\t# Create the Siamese Network model\n",
        "\t# Connect the left and right inputs to the outputs\n",
        "        self._model = Model(inputs=[left_input,right_input],outputs=outputs)\n",
        "\n",
        "    def twin(self, input_shape):\n",
        "        ''' Construct the model for both twins of the Siamese (connected) Network\n",
        "            input_shape : input shape for input vector\n",
        "        '''\n",
        "    \n",
        "        def stem(inputs):\n",
        "            ''' Construct the Stem Group\n",
        "                inputs: the input tensor\n",
        "            '''\n",
        "\n",
        "            # entry convolutional layer and reduce feature maps by 75% (max pooling)\n",
        "            x = self.Conv2D(inputs, 64, (10, 10), activation='relu', kernel_initializer=conv_weights, bias_initializer=biases)\n",
        "            x = MaxPooling2D((2, 2), strides=2)(x)\n",
        "            return x\n",
        "        \n",
        "        def learner(x):\n",
        "            ''' Construct the learner \n",
        "                x   : input to the learner\n",
        "            '''\n",
        "    \n",
        "            # 2nd convolutional layer doubling the number of filters, and reduce feature maps by 75% (max pooling)\n",
        "            x = self.Conv2D(x, 128, (7, 7), activation='relu', kernel_initializer=conv_weights, bias_initializer=biases)\n",
        "            x = MaxPooling2D((2, 2), strides=2)(x)\n",
        "    \n",
        "            # 3rd convolutional layer and reduce feature maps by 75% (max pooling)\n",
        "            x = self.Conv2D(x, 128, (4, 4), activation='relu', kernel_initializer=conv_weights, bias_initializer=biases)\n",
        "            x = MaxPooling2D((2, 2), strides=2)(x)\n",
        "        \n",
        "            # 4th convolutional layer doubling the number of filters with no feature map downsampling\n",
        "            x = self.Conv2D(x, 256, (4, 4), activation='relu', kernel_initializer=conv_weights, bias_initializer=biases)\n",
        "\n",
        "            # for a 105x105 input, the feature map size will be 6x6\n",
        "            return x\n",
        "        \n",
        "        def classifier(x):\n",
        "            ''' Construct the classifier (Encoding block) \n",
        "                x  : input to the classifier\n",
        "            '''\n",
        "\n",
        "            # flatten the maps into a 1D vector\n",
        "            x = Flatten()(x)\n",
        "    \n",
        "            # use dense layer to produce a 4096 encoding of the flattened feature maps\n",
        "            x = self.Dense(x, 4096, activation='sigmoid', kernel_initializer=dense_weights, bias_initializer=biases)\n",
        "            return x\n",
        "\n",
        "        inputs = Input(shape=input_shape)\n",
        "        x = stem(inputs)\n",
        "        x = learner(x)\n",
        "        outputs = classifier(x)\n",
        "\n",
        "        return Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw1Vf23k0UQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siam = SiameseTwin()\n",
        "\n",
        "# getter for the tf.keras model\n",
        "model = siam.model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}